--using splitter

#using RecursiveCharacterTextSplitter from LangChain, which is commonly used to split large texts into 
#manageable chunks for processing (like for embedding or indexing):

"""The TextSplitter like RecursiveCharacterTextSplitter is super useful when working with LLMs because many LLMs have token 
limits (e.g., GPT-4 may cap at 8k or 32k tokens), and large texts need to be broken into smaller, overlapping chunks for:

Context-aware responses
Document search or retrieval
Embedding & vector store creation 
"""

<code>
#Testing splitter
from langchain.text_splitter import RecursiveCharacterTextSplitter

text = """langchain is a framework for developing llm appls powered by llms
          It connects different components such as models, prompts, configuration settings """

text_splitter = RecursiveCharacterTextSplitter(
                chunk_size= 50,
                chunk_overlap= 10,  
)

chunks = text_splitter.split_text(text)

for i in chunks:
  print(i)

for i,chunk in enumerate(chunks):
  print(f"Chunk {i+1}: {chunk}")

--------------------------------------
--create generator and test it 
from transformers import pipeline
generator = pipeline("text2text-generation", model="google/flan-t5-base")

def get_completion(prompt):
    response = generator(prompt,max_new_tokens=100, do_sample=False)
    return (response[0]["generated_text"].strip())

print(get_completion("What is a DEFI in context of crypto world"))

prompt = "what is benefit of using langchain"
# Generate the response
response = generator(prompt, max_new_tokens=150, do_sample=False)

# Print the output
print(response[0]["generated_text"].strip())

------------------------------------------
--Now aim is to use create embeddings , push them in a vector store such as FAISS and using them when using model

#Creating embeddings
#for using openAI
#from langchain.embeddings import OpenAIEmbeddings
#api_token = ""
#embeddings = OpenAIEmbeddings(openai_api_key=api_token,model="text-embedding-ada-002")

#for using HuggingFace
!pip install langchain-community
from langchain_community.embeddings import HuggingFaceEmbeddings
model = "sentence-transformers/all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=model)
text = "tell me something about langchain"
test_embedding = embeddings.embed_documents(text)
print(test_embedding)

#again
text2 = ["tell me something about langchain","how langchain helps"]
test_embedding2 = embeddings.embed_documents(text2)
for i in test_embedding2:
  print(i)

#Getting faiss
!pip install faiss-cpu

#Note**
FAISS.from_documents(documents, embeddings) is the constructor that:
Takes in a list of Document objects (which are essentially text chunks + metadata),
Uses the provided embedding model to convert each chunk into a vector,
Builds a FAISS index from those vectors,
Wraps it all into a FAISS vector store object you can search.

#import the FAISS class
from langchain.vectorstores.faiss import FAISS
from langchain.docstore.document import Document
chunks2 = text_splitter.create_documents([text]) 
vectorstore = FAISS.from_documents(chunks2, embeddings)


# (Optional) 
Save the vectorstore
vectorstore.save_local("faiss_store")
# (Optional) 
Load it again later
# vectorstore = FAISS.load_local("faiss_store", embeddings)

# Perform similarity search
query = "What does LangChain do?"
results = vectorstore.similarity_search(query, k=2)
for i, doc in enumerate(results, 1):
    print(f"\nResult {i}:")
    print(doc.page_content)

#or
query = "What is the document about?"
retrieved_docs = vectorstore.similarity_search(query, k=3)  
# k = number of results

for i, doc in enumerate(retrieved_docs):
    print(f"Result {i+1}:\n{doc.page_content}\n")

Note**
Use k=5 or more if your results are too narrow.
You can also use similarity_search_with_score if you want distance scores.

#now testing the vector store usage within a LLM appl or with our generator 

#Adding more documents to vectorstore can be done (not required now)
new_docs = text_splitter.create_documents([new_text])
vectorstore.add_documents(new_docs)

#Note**
the documents are stored internally in
vectorstore.docstore._dict

--to access all data from vectorstore
all_docs = list(vectorstore.docstore._dict.values())

for i, doc in enumerate(all_docs):
    print(f"Document {i+1}:\n{doc.page_content}\n")

#Use a retriever to fetch relevant chunks
retriever = vectorstore.as_retriever()
#retriever = vectorstore.as_retriever(search_type="similarity", k=4)

#using retriver in appl (for Retrieval-Augmented Generation (RAG))

def rag_qa(query, retriever, gen_pipeline, top_k=3):
    # 1. Retrieve relevant documents
    docs = retriever.get_relevant_documents(query)
    context = "\n".join([doc.page_content for doc in docs[:top_k]])

    # 2. Format the prompt (FLAN works well with Q&A style prompts)
    prompt = f"Context: {context}\n\nQuestion: {query}\nAnswer:"

    # 3. Run generation
    result = gen_pipeline(prompt, max_new_tokens=100, do_sample=False)
    return result[0]["generated_text"]

# Example
query = "What is LangChain?"
answer = rag_qa(query, retriever, pipeline)
print("Answer:", answer)

Note** if error, such as
[KeyError: "Unknown task Context: tell me something about langchain\n\nQuestion: What is LangChain?\nAnswer:, 
available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 
'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 
'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 
'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 
'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 
'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
]

then..

#using AutoTokenizer, AutoModelForSeq2SeqLM instead of pipeline()
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load FLAN-T5
model_id = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

def generate_answer(prompt, max_tokens=100):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True)
    outputs = model.generate(**inputs, max_new_tokens=max_tokens)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def rag_qa(query, retriever, top_k=3):
    # Ensure top_k is an integer
    top_k = int(top_k)
    # 1. Retrieve context
    docs = retriever.get_relevant_documents(query)
    context = "\n".join([doc.page_content for doc in docs[:top_k]])

    # 2. Prompt format
    prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"

    # 3. Generate
    return generate_answer(prompt)

# Example
query = "What is LangChain?"
answer = rag_qa(query, retriever, top_k=3)
print("Answer:", answer)

#Export all chunks
texts = [doc.page_content for doc in vectorstore.docstore._dict.values()]
print(texts)
--------------------------------




