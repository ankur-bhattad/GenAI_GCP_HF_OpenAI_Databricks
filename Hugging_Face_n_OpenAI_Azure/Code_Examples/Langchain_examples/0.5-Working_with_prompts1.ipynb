{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VKcw2iCvHrTvGgwTG83zHiy4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKcw2iCvHrTvGgwTG83zHiy4",
        "outputId": "502aa742-ab3c-40c1-d742-b0c4fd0e69f3",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FjyUcVbgy-Rk",
      "metadata": {
        "id": "FjyUcVbgy-Rk"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# Initialize client once\n",
        "client = AzureOpenAI(\n",
        "    api_key=\"MyKey1\",\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    azure_endpoint=\"MyEndpoint on Azure\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WiV_xPDgJJ5_",
      "metadata": {
        "id": "WiV_xPDgJJ5_"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, deployment_name=\"gpt-4.1-nano\"):\n",
        "    \"\"\"\n",
        "    Get a chat completion from Azure OpenAI.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): User input prompt.\n",
        "        deployment_name (str): The deployment name you gave your model in Azure portal.\n",
        "\n",
        "    Returns:\n",
        "        dict: Full response object, or error dict.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=deployment_name,    # <-- This is the \"deployment name\" not the raw model name\n",
        "            messages=messages,\n",
        "            temperature=0.1,\n",
        "            top_p=0.8,\n",
        "            max_tokens=512\n",
        "        )\n",
        "\n",
        "        return response.model_dump()  # Return the full response as dict\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZlY6258sVC5p",
      "metadata": {
        "id": "ZlY6258sVC5p"
      },
      "outputs": [],
      "source": [
        "#Define Your Prompts__\n",
        "#Provide a series of prompts that guide the model through a chain of thought.\n",
        "#Call the __get_completion__ to get a response from the AI model.\n",
        "#Print both the prompt and the AI-generated response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nl0JIGN8JhZt",
      "metadata": {
        "id": "nl0JIGN8JhZt"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Imagine you are a detective trying to solve a mystery.\",\n",
        "    \"You arrive at the crime scene and start looking for clues.\",\n",
        "    \"You find a strange object at the crime scene. What is it?\",\n",
        "    \"How does this object relate to the crime?\",\n",
        "    \"Who do you think is the suspect and why?\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pb9YT-O4JwXE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb9YT-O4JwXE",
        "outputId": "5ea4f7ba-8887-48dc-b43a-7d2c980ad785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Imagine you are a detective trying to solve a mystery.\n",
            "Response: {'id': 'chatcmpl-CmjFTrqqfwywOyQh3h8ZWx382SZK1', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Absolutely! Let's dive into this mystery together. \\n\\n**Scenario:**  \\nA valuable diamond necklace has gone missing from the city museum during a high-profile exhibition. The security footage shows no signs of forced entry, and only a handful of people had access to the display case. \\n\\n**Clues so far:**  \\n- The necklace was last seen intact at 8:00 PM.  \\n- The security guard on duty reported no unusual activity.  \\n- A faint scent of lavender was detected near the display case.  \\n- A torn piece of fabric was found on the floor nearby.  \\n- The museum's staff list includes five individuals who had access after hours.\\n\\n**Your task:**  \\nAs the detective, who do you suspect and what steps will you take to solve this case?\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765729715, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 157, 'prompt_tokens': 18, 'total_tokens': 175, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n",
            "\n",
            "Prompt: You arrive at the crime scene and start looking for clues.\n",
            "Response: {'id': 'chatcmpl-CmjFUtewkcayzqrj9M4anRGnuhsn0', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'As I arrive at the crime scene, I carefully observe the surroundings, noting any immediate signs of disturbance or unusual activity. I begin by securing the perimeter to prevent contamination, then systematically search for physical evidence such as fingerprints, footprints, bloodstains, or any objects out of place. I document everything with photographs and detailed notes, paying close attention to the position of items and any potential clues that could lead to identifying the perpetrator or understanding the sequence of events. I also look for witness statements or surveillance footage in the vicinity that might provide additional context.', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'low'}}}], 'created': 1765729716, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 112, 'prompt_tokens': 19, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'low'}}}]}\n",
            "\n",
            "Prompt: You find a strange object at the crime scene. What is it?\n",
            "Response: {'id': 'chatcmpl-CmjFVIjpzKJFxzvPIsmfQaqbuiFf3', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Could you please provide more context or details about the crime scene? That way, I can help identify what the strange object might be.', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'low'}}}], 'created': 1765729717, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 28, 'prompt_tokens': 21, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'low'}}}]}\n",
            "\n",
            "Prompt: How does this object relate to the crime?\n",
            "Response: {'id': 'chatcmpl-CmjFVqzv64SDTECtdSPz5cBOYB8mn', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Could you please provide more details or specify which object you're referring to? That way, I can better assist you in understanding its relation to the crime.\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765729717, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 31, 'prompt_tokens': 16, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n",
            "\n",
            "Prompt: Who do you think is the suspect and why?\n",
            "Response: {'id': 'chatcmpl-CmjFWmU39AqiRXy0qj4m7ftagbdDd', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Could you please provide more context or details about the situation or case you're referring to? That way, I can better assist you in analyzing the suspect.\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765729718, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 31, 'prompt_tokens': 17, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for prompt in prompts:\n",
        "    response = get_completion(prompt)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1FC--falKi_7",
      "metadata": {
        "id": "1FC--falKi_7"
      },
      "outputs": [],
      "source": [
        "#Another example\n",
        "prompt = \"\"\"\n",
        "\n",
        "let's analyze the sentiment of the review step by step\n",
        "\n",
        "1. Identify the Positive aspect of the review and give a score from 10\n",
        "2. Identify the Negative aspect of the review and give a score from 10\n",
        "3. Weight the positive and negative aspect to determine the overall sentiment.\n",
        "4. provide the final sentiment classification with justification for scores used in above steps.\n",
        "\n",
        "Review: \"The product is very well designed product\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nLdOToLGLnm4",
      "metadata": {
        "id": "nLdOToLGLnm4"
      },
      "outputs": [],
      "source": [
        "response = get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bmkmNpZiLqry",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmkmNpZiLqry",
        "outputId": "96ba6944-9e0a-4130-ec26-7812d62c7dbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'chatcmpl-CmL0O8sUJSd8zMf8WieTlKecaky5A', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Let\\'s analyze the review step by step:\\n\\n**1. Identify the Positive aspect of the review and give a score from 10**\\n\\n- The review states: \"The product is very well designed product.\"\\n- The positive aspect is the design quality, which is described as \"very well designed.\"\\n- This indicates a strong positive sentiment toward the design.\\n\\n**Score:** 8/10\\n\\n**2. Identify the Negative aspect of the review and give a score from 10**\\n\\n- The review does not mention any negative aspects.\\n- There are no complaints or criticisms present.\\n\\n**Score:** 0/10\\n\\n**3. Weight the positive and negative aspects to determine overall sentiment**\\n\\n- Since the positive aspect is strong (8/10) and negative is absent (0/10), the overall sentiment leans positive.\\n- The absence of negatives suggests the review is primarily positive.\\n\\n**4. Final sentiment classification with justification**\\n\\n- **Sentiment:** Positive\\n- **Justification:** The review highlights a strong positive aspect (design quality) with a high score, and no negative points are mentioned. Therefore, the overall sentiment is clearly positive, supported by the high positive score and zero negative score.\\n\\n**Final conclusion:** The review is predominantly positive, emphasizing the well-designed nature of the product.', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765636524, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 260, 'prompt_tokens': 95, 'total_tokens': 355, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fBui_PtqLtBZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBui_PtqLtBZ",
        "outputId": "085aa27a-e830-409b-f3f8-7e73c3c6b3a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-CmO4ulUw8qSScxc2Ow88RqNG7P7sL',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': 'Let\\'s analyze the review step by step:\\n\\n**1. Identify the Positive aspect of the review and give a score from 10**  \\n- The review states: \"The product is very well designed product.\"  \\n- The positive aspect is the design quality.  \\n- Since the statement is explicitly positive about the design, I would assign a high positive score.  \\n**Score: 8/10**\\n\\n**2. Identify the Negative aspect of the review and give a score from 10**  \\n- There is no mention of any negative aspect in the review.  \\n- Therefore, negative sentiment is minimal or absent.  \\n**Score: 1/10** (indicating very low negativity)\\n\\n**3. Weight the positive and negative aspects to determine overall sentiment**  \\n- Positive score: 8  \\n- Negative score: 1  \\n- Since the positive aspect is significantly stronger, the overall sentiment leans positive.\\n\\n**4. Final sentiment classification with justification**  \\n- The review is predominantly positive, emphasizing the well-designed nature of the product.  \\n- The absence of negative comments supports a positive overall sentiment.  \\n- **Final sentiment: Positive**  \\n- **Justification:** The high positive score (8/10) and minimal negative score (1/10) indicate a favorable opinion, leading to an overall positive sentiment.',\n",
              "    'refusal': None,\n",
              "    'role': 'assistant',\n",
              "    'annotations': [],\n",
              "    'audio': None,\n",
              "    'function_call': None,\n",
              "    'tool_calls': None},\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'protected_material_code': {'filtered': False, 'detected': False},\n",
              "    'protected_material_text': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}],\n",
              " 'created': 1765648336,\n",
              " 'model': 'gpt-4.1-nano-2025-04-14',\n",
              " 'object': 'chat.completion',\n",
              " 'service_tier': None,\n",
              " 'system_fingerprint': 'fp_03e44fcc34',\n",
              " 'usage': {'completion_tokens': 268,\n",
              "  'prompt_tokens': 95,\n",
              "  'total_tokens': 363,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
              " 'prompt_filter_results': [{'prompt_index': 0,\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'jailbreak': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bCHlhhSUL4nF",
      "metadata": {
        "id": "bCHlhhSUL4nF"
      },
      "outputs": [],
      "source": [
        "#Another example\n",
        "prompt = \"\"\"\n",
        "\n",
        "let's sort the values of the list step by step\n",
        "\n",
        "1. Start with the unsorted list.\n",
        "2. Compare each elements and find the smallest value.\n",
        "3. Place the samllest value in the first position.\n",
        "4. Repeat the process for all the remaming elements.\n",
        "5. provide the sorted list.\n",
        "\n",
        "Sort the list : [3,1,4,6,5,9,2]\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rwY12Ul0MMhs",
      "metadata": {
        "id": "rwY12Ul0MMhs"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "\n",
        "\"You have 12 identical-looking balls, but one is either heavier or lighter.\n",
        "You have a balance scale and can only use it three times.\n",
        "Explain step-by-step how you can find the odd ball and determine whether it is heavier or lighter.\n",
        "Think through the problem carefully and explain your reasoning in detail before giving the final answer.\"\n",
        "\n",
        "\"\"\"\n",
        "response = get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IGFmVJiWM8-C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGFmVJiWM8-C",
        "outputId": "df35bf4e-408b-4c1e-a2f4-a337db60fae2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-C8dhKgYZQK5tMQncPza4Z92uUhSSs',\n",
              " 'choices': [{'finish_reason': 'length',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': \"To solve the problem of identifying the odd ball among 12 identical-looking balls, where one ball is either heavier or lighter, we can use a systematic approach with a balance scale. The key is to divide the balls into groups and use the results of the weighings to narrow down the possibilities. Here’s a step-by-step breakdown of the solution:\\n\\n### Step 1: Initial Grouping\\n1. **Divide the 12 balls into three groups of 4 balls each**: Let's label the balls as A1, A2, A3, A4 (Group A), B1, B2, B3, B4 (Group B), and C1, C2, C3, C4 (Group C).\\n\\n### Step 2: First Weighing\\n2. **Weigh Group A against Group B**:\\n   - **Case 1**: If the scales balance, then the odd ball is in Group C (C1, C2, C3, C4).\\n   - **Case 2**: If Group A is heavier, then the odd ball is either in Group A (and heavier) or in Group B (and lighter).\\n   - **Case 3**: If Group B is heavier, then the odd ball is either in Group B (and heavier) or in Group A (and lighter).\\n\\n### Step 3: Second Weighing\\nNow, we will analyze each case separately.\\n\\n#### Case 1: A = B (Odd ball is in Group C)\\n3. **Weigh C1, C2 against C3, C4**:\\n   - If they balance, then the odd ball is not among C1, C2, C3, or C4, which is impossible since we know one is odd. So, this case will not occur.\\n   - If C1, C2 is heavier, then one of C1 or C2 is heavier, or one of C3 or C4 is lighter.\\n   - If C3, C4 is heavier, then one of C3 or C4 is heavier, or one of C1 or C2 is lighter.\\n\\n#### Case 2: A > B (Odd ball is in A or B)\\n4. **Weigh A1, A2, B1 against A3, A4, B2**:\\n   - If they balance, then the odd ball is either B3 or B4 (and lighter) or A1, A2 (and heavier).\\n   - If\",\n",
              "    'refusal': None,\n",
              "    'role': 'assistant',\n",
              "    'annotations': [],\n",
              "    'audio': None,\n",
              "    'function_call': None,\n",
              "    'tool_calls': None},\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'protected_material_code': {'filtered': False, 'detected': False},\n",
              "    'protected_material_text': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}],\n",
              " 'created': 1756175258,\n",
              " 'model': 'gpt-4o-mini-2024-07-18',\n",
              " 'object': 'chat.completion',\n",
              " 'service_tier': None,\n",
              " 'system_fingerprint': 'fp_efad92c60b',\n",
              " 'usage': {'completion_tokens': 512,\n",
              "  'prompt_tokens': 74,\n",
              "  'total_tokens': 586,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
              " 'prompt_filter_results': [{'prompt_index': 0,\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'jailbreak': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "__o766f5NCpZ",
      "metadata": {
        "id": "__o766f5NCpZ"
      },
      "outputs": [],
      "source": [
        "#Another example\n",
        "prompt = \"\"\"\n",
        "Let's consider which is heavier: 1000 feathers or a 30-pound weight.\n",
        "I'll think through this in a few different ways and then decide which answer seems most consistent.\n",
        "\n",
        "1. First line of reasoning: A single feather is very light, almost weightless.\n",
        "So, 1000 feathers might still be quite light, possibly lighter than a 30-pound weight.\n",
        "\n",
        "2. Second line of reasoning: 1000 is a large number, and when you add up the weight of so many feathers,\n",
        "it could be quite heavy. Maybe it's heavier than a 30-pound weight.\n",
        "\n",
        "3. Third line of reasoning: The average weight of a feather is very small. Even 1000 feathers would not add up to 30 pounds.\n",
        "\n",
        "Considering these reasonings, the most consistent answer is: & the reason to choose the answer is :\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a0I6UQ_NvQI",
      "metadata": {
        "id": "6a0I6UQ_NvQI"
      },
      "outputs": [],
      "source": [
        "response = get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lq4_40kdNz9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq4_40kdNz9f",
        "outputId": "61519a36-87c3-402e-85cf-f8a38098c1f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-CmOAEs365wJaM1Dr9Jlpdmqlu9BOh',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': \"Considering all three lines of reasoning:\\n\\n1. The first suggests that a single feather is extremely light, so 1000 feathers might still be quite light—likely less than 30 pounds.\\n2. The second emphasizes that 1000 feathers could accumulate to a significant weight, possibly exceeding 30 pounds.\\n3. The third indicates that even 1000 feathers probably don't reach 30 pounds, given their tiny individual weights.\\n\\nMost evidence points toward the idea that 1000 feathers are still relatively light and unlikely to weigh more than a 30-pound weight. Therefore, the most consistent answer is:\\n\\n**1000 feathers are lighter than a 30-pound weight.**\\n\\nThe reason to choose this answer is because, based on the typical weight of a feather, even a thousand of them would not come close to reaching 30 pounds.\",\n",
              "    'refusal': None,\n",
              "    'role': 'assistant',\n",
              "    'annotations': [],\n",
              "    'audio': None,\n",
              "    'function_call': None,\n",
              "    'tool_calls': None},\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'protected_material_code': {'filtered': False, 'detected': False},\n",
              "    'protected_material_text': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}],\n",
              " 'created': 1765648666,\n",
              " 'model': 'gpt-4.1-nano-2025-04-14',\n",
              " 'object': 'chat.completion',\n",
              " 'service_tier': None,\n",
              " 'system_fingerprint': 'fp_03e44fcc34',\n",
              " 'usage': {'completion_tokens': 168,\n",
              "  'prompt_tokens': 176,\n",
              "  'total_tokens': 344,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
              " 'prompt_filter_results': [{'prompt_index': 0,\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'jailbreak': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}]}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1M365W83qas",
      "metadata": {
        "id": "d1M365W83qas"
      },
      "outputs": [],
      "source": [
        "#To work with Langchain, Install langchain related dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lwHLLhIDN1Rp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwHLLhIDN1Rp",
        "outputId": "d9b9393f-0d95-4740-b1c1-fab865c0b8c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.1.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (0.3.79)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-1.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.4.42)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (2.11.10)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
            "Collecting uuid-utils<1.0,>=0.12.0 (from langchain_core)\n",
            "  Downloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain_community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain_community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain_community)\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-1.1.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.0-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.9/475.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.7/343.7 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: uuid-utils, requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_core, langchain-text-splitters, langchain_openai, langchain-classic, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.2.0 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.1.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.0 langchain-text-splitters-1.1.0 langchain_community-0.4.1 langchain_core-1.2.0 langchain_openai-1.1.3 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0 uuid-utils-0.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai langchain_core langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oH-Er3wK4ENB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH-Er3wK4ENB",
        "outputId": "62ea418f-31d5-4c68-d7c5-e9797b6b1d88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain>=0.3.29\n",
            "  Downloading langchain-1.1.3-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: langchain-core>=1.0.0 in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.3)\n",
            "Collecting langgraph<1.1.0,>=1.0.2 (from langchain>=0.3.29)\n",
            "  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.3.29) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (0.4.42)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (0.12.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=1.0.0) (3.0.0)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain>=0.3.29)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain>=0.3.29)\n",
            "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph<1.1.0,>=1.0.2->langchain>=0.3.29)\n",
            "  Downloading langgraph_sdk-0.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.3.29) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3.29) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3.29) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3.29) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (0.16.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain>=0.3.29)\n",
            "  Downloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (2.5.0)\n",
            "Downloading langchain-1.1.3-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-1.0.5-py3-none-any.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.1/157.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
            "Downloading langgraph_sdk-0.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "Successfully installed langchain-1.1.3 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.0 ormsgpack-1.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade \"langchain>=0.3.29\" \"langchain-core>=1.0.0\" langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qMcbe_eQ4LfW",
      "metadata": {
        "id": "qMcbe_eQ4LfW"
      },
      "outputs": [],
      "source": [
        "#from langchain_openai import AzureOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pjtgqh4r4hU_",
      "metadata": {
        "id": "pjtgqh4r4hU_"
      },
      "outputs": [],
      "source": [
        "# Initialize client once\n",
        "#client_lc = AzureOpenAI(\n",
        "#    api_key=\"MyKey1\",\n",
        "#    api_version=\"2024-12-01-preview\",\n",
        "#    azure_endpoint=\"MyEndpoint on Azure\",\n",
        "#    deployment_name=\"gpt-4o-mini\", # chat completions would work with this model\n",
        "#    temperature=0.5,\n",
        "#    top_p=0.8,\n",
        "#    max_tokens=512\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MbWwtnel4scU",
      "metadata": {
        "id": "MbWwtnel4scU"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "# Initialize client once\n",
        "client_lc = AzureChatOpenAI(\n",
        "    api_key=\"MyKey1\",\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    azure_endpoint=\"MyEndpoint on Azure\",\n",
        "    deployment_name=\"gpt-5-chat\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=512\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a_Ip0ZzV47rU",
      "metadata": {
        "id": "a_Ip0ZzV47rU"
      },
      "outputs": [],
      "source": [
        "def get_completion_lc(prompt):\n",
        "    try:\n",
        "        response = client_lc.invoke(prompt)\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oSxnw6wi5RIp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSxnw6wi5RIp",
        "outputId": "63ba0185-197d-4636-97d0-0c92def54dfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When we talk about *human messages* in the context of *AI-generated responses* by large language models (LLMs), we’re really discussing how human input (prompts, instructions, or conversational turns) interacts with the model’s internal mechanisms to produce meaningful, context-aware output. Let’s break this down:\n",
            "\n",
            "---\n",
            "\n",
            "### 1. **Human Messages as Input**\n",
            "Human messages are the text inputs or prompts that users provide to an LLM. They carry:\n",
            "- **Explicit content** – the literal words, questions, or commands.\n",
            "- **Implicit intent** – the underlying purpose, tone, or context (e.g., seeking information, requesting creativity, expressing emotion).\n",
            "\n",
            "LLMs don’t “understand” these messages in a human sense but *model* the statistical and semantic relationships between tokens (words, phrases) based on patterns learned from large datasets.\n",
            "\n",
            "---\n",
            "\n",
            "### 2. **Context and Conversation History**\n",
            "LLMs use *context windows* to process not just the latest message but also preceding ones in a conversation.  \n",
            "This allows them to:\n",
            "- Maintain continuity (refer back to previous topics or user preferences).\n",
            "- Interpret ambiguous messages based on prior exchanges.\n",
            "- Generate responses that feel coherent and contextually relevant.\n",
            "\n",
            "For example, if a user says “Explain that further,” the model must look back at what “that” refers to in the prior message.\n",
            "\n",
            "---\n",
            "\n",
            "### 3. **Message Interpretation and Representation**\n",
            "Internally, the model converts human language into *vector representations*—numerical encodings that capture semantic relationships.  \n",
            "When generating a response, the model predicts the next token (word or subword) that best fits the context, given the probability distribution learned during training.\n",
            "\n",
            "Thus, the “meaning” of a human message is represented as patterns of relationships among tokens, not as conscious understanding.\n",
            "\n",
            "---\n",
            "\n",
            "### 4. **Human-AI Interaction Loop**\n",
            "Each human message influences the model’s next output, creating a feedback loop:\n",
            "1. **Human sends message** → provides linguistic and contextual cues.\n",
            "2. **Model interprets context** → transforms text into internal representations.\n",
            "3. **Model generates response** → predicts text that aligns with the prompt and context.\n",
            "4. **Human reacts** → sends another message, adjusting the conversation.\n",
            "\n",
            "This loop mirrors dialogue dynamics, though the model’s “understanding” is statistical rather than experiential.\n",
            "\n",
            "---\n",
            "\n",
            "### 5. **Human Message Framing and Control**\n",
            "The *way* humans phrase messages strongly affects the model’s behavior:\n",
            "- Clear, specific prompts yield more accurate responses.\n",
            "- Amb\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Explain human messages in context of AI generated responses by LLM\"\n",
        "response = get_completion_lc(prompt)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HimcVeX8wiBr",
      "metadata": {
        "id": "HimcVeX8wiBr"
      },
      "outputs": [],
      "source": [
        "#Generate multiple lines of reasoning to answer the question.\n",
        "#Ask the AI to evaluate these reasonings and determine the most consistent answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I_kNat2gWZAN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_kNat2gWZAN",
        "outputId": "5d92cfa4-25f0-4b09-9866-33608e1b186f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'chatcmpl-CmQImEgUoOXMoytlZrgoXX68r4Su3', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Given the reasoning you've outlined:\\n\\n- The first reasoning suggests that a single feather is very light, so 1000 feathers might still be lighter than 30 pounds.\\n- The second reasoning considers that 1000 feathers could add up to a significant weight, potentially exceeding 30 pounds.\\n- The third reasoning emphasizes that individual feathers are very light, so even 1000 of them probably wouldn't reach 30 pounds.\\n\\nIn reality, the weight of 1000 feathers depends on the size and type of the feathers, but generally, feathers are quite light. Typically, a single feather weighs only a few grams at most. For example, if one feather weighs about 0.5 grams, then 1000 feathers weigh approximately 500 grams, which is about 1.1 pounds—much less than 30 pounds.\\n\\n**Therefore, the most consistent answer is:**  \\n**1000 feathers are lighter than a 30-pound weight.**\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765656884, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 190, 'prompt_tokens': 167, 'total_tokens': 357, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Let's consider which is heavier: 1000 feathers or a 30-pound weight.\n",
        "I'll think through this in a few different ways and then decide which answer seems most consistent.\n",
        "\n",
        "1. First line of reasoning: A single feather is very light, almost weightless.\n",
        "So, 1000 feathers might still be quite light, possibly lighter than a 30-pound weight.\n",
        "\n",
        "2. Second line of reasoning: 1000 is a large number, and when you add up the weight of so many feathers,\n",
        "it could be quite heavy. Maybe it's heavier than a 30-pound weight.\n",
        "\n",
        "3. Third line of reasoning: The average weight of a feather is very small. Even 1000 feathers would not add up to 30 pounds.\n",
        "\n",
        "Considering these reasonings, the most consistent answer is:\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hWKH3oh9WfDy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWKH3oh9WfDy",
        "outputId": "9b389b25-3635-40c9-972f-5d041e1d5b84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'chatcmpl-CmQJAq9hGoRT47kJv4CIUVbHk6gNn', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Let\\'s analyze the reasoning carefully:\\n\\n- The phrase \"all but 9 run away\" means that 9 sheep did **not** run away.\\n- Therefore, the number of sheep remaining is **9**.\\n\\nThe other interpretations:\\n\\n- \"All but 9\" does **not** mean subtracting 9 from 17; it simply indicates that 9 sheep are left.\\n- The statement \"17 - 9 = 8\" is incorrect in this context because it misinterprets the phrase.\\n\\n**Most consistent answer:**\\n\\n**9 sheep are left.**', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765656908, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 115, 'prompt_tokens': 87, 'total_tokens': 202, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "\n",
        "A farmer has 17 sheeps, all but 9 run away. How many are left?\n",
        "\n",
        "1. All but 9 ran away --> 9 are left\n",
        "\n",
        "2. \"All but 9\" means 9 stayed --> 9 are left\n",
        "\n",
        "3. Subtracting 17 - 9 --> 8 are left\n",
        "\n",
        "Considering these reasonings, the most consistent answer is:\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7AbQyfG5Wku-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AbQyfG5Wku-",
        "outputId": "78587bc5-c8ef-410c-840d-23fec1e0313e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'chatcmpl-CmQJPPVcjaVNTRx79zfqvCIYf0dbz', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Let\\'s analyze each approach carefully:\\n\\n1. **First approach:**  \\n\"15 apples - 4 apples = 11 apples\"  \\nThis calculation correctly finds the number of apples remaining **after** taking away 4.  \\n**However, the question asks:** *\"how many apples do you have?\"*  \\nIf you took away 4 apples, **you now have those 4 apples** with you.  \\n**So, this approach is incorrect** in answering the question, because it focuses on what\\'s left, not what you have.\\n\\n2. **Second approach:**  \\n\"If I take away 4 apples, then I have 4 apples with me.\"  \\n**This is correct** because when you take away 4 apples, **you now possess those 4 apples**.\\n\\n3. **Third approach:**  \\n\"Taking away 4 apples means I have 4 apples.\"  \\n**This is also correct** for the same reason as the second approach.\\n\\n4. **Fourth approach:**  \\n\"Subtracting 4 from 15 will give me 11 apples,\" which is **incorrect** in context.  \\n**Again, this is about what\\'s left, not what you have.**  \\nSo, this approach is incorrect for answering the question.\\n\\n---\\n\\n### **Summary:**\\n\\n- The **correct answer** to \"how many apples do you have?\" is **4**, because you took away 4 apples and now possess them.\\n- Approaches **2 and 3** correctly identify that you have 4 apples.\\n- Approaches **1 and 4** focus on the remaining apples (11), which **do not answer** the question asked.\\n\\n**Conclusion:**  \\nThe most consistent and correct approaches are **2 and 3**, both indicating you have **4 apples**.', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765656923, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 360, 'prompt_tokens': 154, 'total_tokens': 514, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "I will solve the following math problem in several different ways and check if I arrive at the same answer each time.\n",
        "\n",
        "Problem: There were 15 apples and you took away 4, how many apples do you have?\n",
        "\n",
        "1. First approach: 15 apples - 4 apples = 11 apples, which is incorrect\n",
        "\n",
        "2. Second approach: If i take away 4 apples, then i have 4 apples with me, which is correct\n",
        "\n",
        "3. Third approach: Taking away 4 apples means i have 4 apples, which is correct\n",
        "\n",
        "4. Fourth approach : Subtracting 4 from 15 will give me 11 apples, which is incorrect.\n",
        "\n",
        "Let's see which approach gives the most consistent result.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EAtrIkhEWod4",
      "metadata": {
        "id": "EAtrIkhEWod4"
      },
      "outputs": [],
      "source": [
        "#Provide a series of prompts that guide the model through a tree of thought.\n",
        "#Call __get_completion__ to get a response from the AI model.\n",
        "#Print both the prompt and the AI-generated response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vWU5Oqs4W3SX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWU5Oqs4W3SX",
        "outputId": "692a4270-56b1-4ece-a01c-0416c835158e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI Response:\n",
            "{'id': 'chatcmpl-CmQKghqtZxtskqzSJ5lKMvS90yekL', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Let's analyze the problem systematically.\\n\\nGiven:\\n- Total fencing (perimeter) \\\\( P = 100 \\\\) meters\\n- The field is rectangular with length \\\\( L \\\\) and width \\\\( W \\\\)\\n- Perimeter constraint: \\\\( 2L + 2W = 100 \\\\) or simplified as \\\\( L + W = 50 \\\\)\\n\\n**Objective:**\\nMaximize the area \\\\( A = L \\\\times W \\\\)\\n\\n---\\n\\n### Step 1: Express area in terms of one variable\\n\\nFrom the perimeter constraint:\\n\\\\[\\nW = 50 - L\\n\\\\]\\n\\nSo, the area:\\n\\\\[\\nA(L) = L \\\\times (50 - L) = 50L - L^2\\n\\\\]\\n\\n---\\n\\n### Step 2: Find the value of \\\\( L \\\\) that maximizes \\\\( A(L) \\\\)\\n\\nThis is a quadratic function:\\n\\\\[\\nA(L) = -L^2 + 50L\\n\\\\]\\n\\nThe parabola opens downward (since coefficient of \\\\( L^2 \\\\) is negative), so the maximum occurs at the vertex:\\n\\n\\\\[\\nL_{max} = -\\\\frac{b}{2a} = -\\\\frac{50}{2 \\\\times (-1)} = \\\\frac{50}{2} = 25\\n\\\\]\\n\\nCorrespondingly:\\n\\\\[\\nW = 50 - L = 50 - 25 = 25\\n\\\\]\\n\\n---\\n\\n### **Conclusion:**\\n\\nThe maximum area is achieved when the rectangle is a **square** with sides of **25 meters**.\\n\\n**Maximum area:**\\n\\\\[\\nA_{max} = 25 \\\\times 25 = 625 \\\\text{ square meters}\\n\\\\]\\n\\n---\\n\\n### **Final answer:**\\n\\n**The farmer should build a square with sides of 25 meters to enclose the maximum area of 625 square meters.**\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765657002, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 376, 'prompt_tokens': 175, 'total_tokens': 551, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Solve the problem: A farmer has 100 meters of fencing and wants to enclose the maximum area for his rectangular field. What should the dimensions be?\n",
        "\n",
        "Let's think about this in a few ways:\n",
        "\n",
        "1. If the field is a square, each side would be 100 / 4 = 25 meters. The area would be 25 * 25 = 625 square meters.\n",
        "\n",
        "2. What if the field is not a square? Let's try a 4:1 ratio. The lengths would be 40 and 10 meters. The area would be 40 * 10 = 400 square meters.\n",
        "\n",
        "3. Are there any other ratios that might give a larger area than a square or a 4:1 rectangle?\n",
        "\n",
        "Considering these options and reason out on own and then output the best dimensions for the maximum area :\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"AI Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rGSuVsyHW7u9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGSuVsyHW7u9",
        "outputId": "a90e1a69-3133-4416-c7bd-89ea53b1ad75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'chatcmpl-CmQL7u732UhOdTTkvqaE2f3YP8QAw', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"To analyze the most likely outcome of the current case, let's compare its facts with the provided precedents and their branches.\\n\\n**Step 1: Identify key facts of the current case**  \\n(Assuming typical factors such as evidence strength, negligence, breach of duty, contributory negligence, or shared liability—please specify if you have particular facts.)\\n\\n**Step 2: Match with relevant precedents**\\n\\n- **If the current case closely resembles Precedent 1 (plaintiff wins):**  \\n  - *Branch A (defendant negligent)*  \\n    - *Sub-branch A1:* Clear evidence of negligence → Likely damages awarded.  \\n    - *Sub-branch A2:* Breach of duty established → Likely in favor of plaintiff.\\n\\n- **If it resembles Precedent 2 (defendant wins):**  \\n  - *Branch B (no negligence)*  \\n    - *Sub-branch B1:* Insufficient evidence → Court rules in favor of defendant.  \\n    - *Sub-branch B2:* Plaintiff assumed risk → Defense prevails.\\n\\n- **If it resembles Precedent 3 (mixed outcome):**  \\n  - *Branch C (both parties at fault)*  \\n    - *Sub-branch C1:* Damages reduced due to contributory negligence.  \\n    - *Sub-branch C2:* Shared liability → Split decision.\\n\\n**Step 3: Consider the most probable outcome based on facts**  \\n- If the evidence suggests clear negligence and breach of duty, the most likely outcome aligns with **Precedent 1**, favoring the plaintiff.  \\n- If evidence is weak or the defendant can demonstrate lack of negligence or assumption of risk, the outcome may favor the defendant, similar to **Precedent 2**.  \\n- If both parties are partially at fault, the case may resemble **Precedent 3**, leading to damages being reduced or shared liability.\\n\\n**Conclusion:**  \\nWithout specific facts, the most probable outcome depends on the strength of evidence regarding negligence and liability. If the current case has strong evidence of defendant negligence, the most likely outcome is that the court finds in favor of the plaintiff, awarding damages. Conversely, if evidence is weak or defenses are strong, the defendant may prevail. If fault is shared, expect a reduced damages award or a split liability decision.\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765657029, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 478, 'prompt_tokens': 253, 'total_tokens': 731, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Let's analyze the legal case by considering multiple precedents and possible outcomes.\n",
        "\n",
        "1. Precedent 1: A similar case where the plaintiff won.\n",
        "    - Branch A: The court found that the defendant was negligent.\n",
        "        - Sub-branch A1: The plaintiff was awarded damages due to clear evidence of negligence.\n",
        "        - Sub-branch A2: The court ruled in favor of the plaintiff due to the defendant's breach of duty.\n",
        "\n",
        "2. Precedent 2: A similar case where the defendant won.\n",
        "    - Branch B: The court found no negligence on the defendant's part.\n",
        "        - Sub-branch B1: The plaintiff failed to provide sufficient evidence.\n",
        "        - Sub-branch B2: The court ruled that the plaintiff assumed the risk.\n",
        "\n",
        "3. Precedent 3: A case with a mixed outcome.\n",
        "    - Branch C: The court found both parties partially at fault.\n",
        "        - Sub-branch C1: Damages were reduced based on the plaintiff's contributory negligence.\n",
        "        - Sub-branch C2: The court ruled that both parties shared liability, resulting in a split decision.\n",
        "\n",
        "4. Based on the facts of the current case, consider the most likely outcome.\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fyV0mns1XCfN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyV0mns1XCfN",
        "outputId": "4ab18730-2b26-4884-dd23-54bc88431f7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI Response:\n",
            "{'id': 'chatcmpl-CmQLQac8lNUQ7hJaTKN3etDi5E8uw', 'choices': [{'finish_reason': 'length', 'index': 0, 'logprobs': None, 'message': {'content': \"Let's carefully analyze the problem:\\n\\n**Problem Restatement:**\\n- 12 identical-looking balls.\\n- Exactly one ball is either heavier or lighter than the rest.\\n- You have a balance scale.\\n- You can only use the scale **3 times**.\\n- Goal: Identify the **odd ball** and determine whether it is **heavier or lighter**.\\n\\n---\\n\\n### Step 1: Understand the constraints and possibilities\\n\\n- Total possibilities:\\n  - 12 balls.\\n  - Each could be heavier or lighter.\\n  - Total scenarios: 12 balls × 2 states (heavier or lighter) = **24 possibilities**.\\n\\n- Each weighing can have three outcomes:\\n  - Left side heavier.\\n  - Right side heavier.\\n  - Balance (equal).\\n\\n- With 3 weighings, the maximum number of distinguishable outcomes:\\n  - \\\\(3^3 = 27\\\\).\\n\\n- Since 27 > 24, it is theoretically possible to identify the odd ball and whether it is heavier or lighter within 3 weighings.\\n\\n---\\n\\n### Step 2: Strategy overview\\n\\nThe general approach:\\n- Divide the balls into groups to maximize information gain.\\n- Use the first weighing to narrow down the possibilities.\\n- Use subsequent weighings to pinpoint the odd ball and its nature.\\n\\n---\\n\\n### Step 3: Step-by-step reasoning\\n\\n#### **First Weighing:**\\n\\n**Option A:** Divide into three groups of 4 balls each:\\n- Group 1: Balls 1-4\\n- Group 2: Balls 5-8\\n- Group 3: Balls 9-12\\n\\n**Weigh:** Group 1 vs. Group 2\\n\\n**Possible outcomes:**\\n\\n1. **Balance:** The odd ball is in Group 3 (balls 9-12).  \\n2. **Left side heavier:** The odd ball is in Group 1, and it is heavier, or in Group 2, and it is lighter.  \\n3. **Right side heavier:** The odd ball is in Group 1, and it is lighter, or in Group 2, and it is heavier.\\n\\n---\\n\\n#### **Second Weighing:**\\n\\nBased on the first outcome:\\n\\n---\\n\\n### Scenario 1: First weighing balanced (Group 3 contains the odd ball)\\n\\n- Now, focus on balls 9-12.\\n- We know the odd ball is among these 4.\\n- To determine which one and whether heavier or lighter, weigh 1 vs. 2 (from balls 9-12).\\n\\n**We\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765657048, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 512, 'prompt_tokens': 83, 'total_tokens': 595, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "\n",
        "\"You have 12 identical-looking balls, but one is either heavier or lighter.\n",
        "You have a balance scale and can only use it three times.\n",
        "Think step-by-step, and at each step, explore multiple possible ways to proceed.\n",
        "Evaluate which options are promising and continue expanding those paths.\n",
        "Discard any unhelpful paths. After exploring the reasoning tree, provide the best solution.\"\n",
        "\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"AI Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BUx1V3UcXHGH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUx1V3UcXHGH",
        "outputId": "fc03ddc0-5105-400d-db5f-2eacf31c463d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'chatcmpl-CmQLhTFkwV3yIO99u9Qy1xBLoeegM', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Planning a trip to Goa in August involves considering the weather conditions, as August is typically the monsoon season with heavy rains and high humidity. Here's a breakdown based on the options you provided:\\n\\n**Option 1: Mountains**  \\n- Not applicable, since Goa is a coastal region and doesn't have mountainous terrain suitable for this option.\\n\\n**Option 2: Beach**  \\n- **Weather in August:** Usually rainy, cloudy, and humid. Sunny days are less frequent, and outdoor activities like swimming or sunbathing may be limited.\\n\\n**Recommended Choice:**  \\n- **Indoor activities and exploring local culture** are better suited during the monsoon season.  \\n- You can visit indoor attractions such as museums, art galleries, or spice plantations.  \\n- Enjoy local cuisine at cozy cafes or restaurants.  \\n- Consider shopping at indoor markets or exploring historical sites that are less affected by rain.\\n\\n**Additional Tips:**  \\n- Check the weather forecast before planning outdoor activities.  \\n- Embrace the monsoon ambiance—lush greenery, fewer crowds, and scenic waterfalls.  \\n- Be prepared with rain gear and waterproof clothing.\\n\\n**Conclusion:**  \\nGiven the typical weather in August, the best option for your Goa trip is to focus on indoor attractions, cultural experiences, and enjoying the natural beauty that the monsoon season enhances. Outdoor beach activities might be limited, but the overall experience can still be memorable with the right planning.\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765657065, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 283, 'prompt_tokens': 254, 'total_tokens': 537, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Let's plan a weekend trip by considering multiple options.\n",
        "\n",
        "1. Option 1: Go to the mountains.\n",
        "    - Branch A: If the weather is good in the mountains.\n",
        "        - Sub-branch A1: You can go hiking.\n",
        "        - Sub-branch A2: You can visit a nearby lake.\n",
        "    - Branch B: If the weather is bad in the mountains.\n",
        "        - Sub-branch B1: You will stay in a cabin and relax.\n",
        "        - Sub-branch B2: You can explore local museums.\n",
        "\n",
        "2. Option 2: Go to the beach.\n",
        "    - Branch C: If the weather is sunny at the beach.\n",
        "        - Sub-branch C1: You can swim in the ocean.\n",
        "        - Sub-branch C2: You can sunbathe and play beach volleyball.\n",
        "    - Branch D: If the weather is cloudy or rainy at the beach.\n",
        "        - Sub-branch D1: You will visit indoor attractions like an aquarium.\n",
        "        - Sub-branch D2: You can go shopping in beachside stores.\n",
        "\n",
        "3. Considering all these factors, decide the best option for your weekend trip.\n",
        "\n",
        "What should i do for my trip to Goa in month of august\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y7TCpFKwL7ZY",
      "metadata": {
        "id": "Y7TCpFKwL7ZY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "0.5-Working_with_prompts.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
