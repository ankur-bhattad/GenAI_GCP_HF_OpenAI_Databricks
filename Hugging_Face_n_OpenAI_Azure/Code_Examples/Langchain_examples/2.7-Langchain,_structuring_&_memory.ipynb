{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E2hbfXMc7xTtPdkE45JzoOcn",
      "metadata": {
        "id": "E2hbfXMc7xTtPdkE45JzoOcn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Note** If using gpt model and AzureOpenAI or AzureChatOpenAI (refer: 'Working_with_AzureOpenAI' folder)\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/content/.env\")\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.getenv(\"API_KEY\"),\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_version=\"2024-12-01-preview\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mNGG_WLjaaWY",
      "metadata": {
        "id": "mNGG_WLjaaWY"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, deployment=\"gpt-4.1\"):\n",
        "    response = client.chat.completions.create(\n",
        "        model=deployment,   #Azure uses DEPLOYMENT name here\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qLQfR4Dibcjz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLQfR4Dibcjz",
        "outputId": "432e98c7-0a2f-40b8-ba1e-006570f6262b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure! Here’s a simple explanation of **transformers**:\n",
            "\n",
            "---\n",
            "\n",
            "**Transformers** are a type of computer model that helps machines understand and generate language (like answering questions or writing stories).\n",
            "\n",
            "**How do they work?**\n",
            "\n",
            "- Imagine you’re reading a sentence. To understand it, you pay attention to all the words, not just one at a time.\n",
            "- Transformers do the same! They look at all the words in a sentence at once and figure out which words are important to each other.\n",
            "- They use something called **attention** to focus on the most relevant words when making sense of the sentence.\n",
            "\n",
            "**Why are they special?**\n",
            "\n",
            "- Before transformers, computers read sentences one word at a time, which was slow and less accurate.\n",
            "- Transformers can read and understand whole sentences (or even paragraphs) at once, making them much better at tasks like translation, answering questions, and writing.\n",
            "\n",
            "**In short:**  \n",
            "Transformers are smart computer models that understand language by paying attention to all the words at once, helping machines read, write, and talk more like humans.\n",
            "\n",
            "---\n",
            "\n",
            "Let me know if you want a bit more detail or an example!\n"
          ]
        }
      ],
      "source": [
        "print(get_completion(\"Explain transformers in simple terms\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y2QCehqPbgjV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Y2QCehqPbgjV",
        "outputId": "c621408b-ec33-4322-d09c-cc9d2a393a56"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Transformers are AI models that understand language by focusing on important words in context, using attention mechanisms to process and generate text efficiently and accurately.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Using Langchain Equivalent\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "#from dotenv import load_dotenv\n",
        "#load_dotenv(\"/content/.env\")\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"API_KEY\"),\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    deployment_name=\"gpt-4.1\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "llm.invoke(\"Explain transformers in simple terms in 25 words\").content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s9fA5V0QbvpF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9fA5V0QbvpF",
        "outputId": "62c47ca2-4538-42ae-cf0a-c95e2327cf05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 + 1 = **2**\n"
          ]
        }
      ],
      "source": [
        "print(get_completion(\"What is 1+1?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F_KN2Q0Vb-E3",
      "metadata": {
        "id": "F_KN2Q0Vb-E3"
      },
      "outputs": [],
      "source": [
        "## Start by creating an instance of the AzureChatOpenAI class.\n",
        "chat = AzureChatOpenAI(\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"API_KEY\"),\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    deployment_name=\"gpt-4.1\",\n",
        "    temperature=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y_NojHalcpBc",
      "metadata": {
        "id": "y_NojHalcpBc"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template_string = \"\"\"Translate the text that is delimited by triple backticks\n",
        "into a style that is {style}.\n",
        "text: ```{text}```\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BjEtJYJecodw",
      "metadata": {
        "id": "BjEtJYJecodw"
      },
      "outputs": [],
      "source": [
        "customer_style = \"American English in a casual tone\"\n",
        "\n",
        "customer_email = \"\"\"\n",
        "I'm super excited about the new gaming console I bought!\n",
        "It arrived in just 2 days and I've been playing non-stop.\n",
        "Totally worth the price!\n",
        "\"\"\"\n",
        "\n",
        "customer_messages = prompt_template.format_messages(\n",
        "    style=customer_style,\n",
        "    text=customer_email\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KHaJIY9Hc5h2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHaJIY9Hc5h2",
        "outputId": "629d790d-5619-4cdd-c120-6add45395d34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm so pumped about the new gaming console I got! It showed up in just two days, and I've been playing nonstop ever since. Definitely worth every penny!\n"
          ]
        }
      ],
      "source": [
        "response = chat.invoke(customer_messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wyTMidXsc-Ux",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyTMidXsc-Ux",
        "outputId": "dfa9e6aa-03bf-4c0b-e7b9-91accc2a1bec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate the text that is delimited by triple backticks\n",
            "into a style that is a cheerful tone that speaks in English Pirate.\n",
            "text: ```Hey there, we're glad you're enjoying your new gaming console. Game on!```\n"
          ]
        }
      ],
      "source": [
        "# Format a new message using a different style.\n",
        "service_reply = \"Hey there, we're glad you're enjoying your new gaming console. Game on!\"\n",
        "service_style_pirate = \"a cheerful tone that speaks in English Pirate\"\n",
        "service_messages = prompt_template.format_messages(\n",
        "    style=service_style_pirate,\n",
        "    text=service_reply\n",
        ")\n",
        "print(service_messages[0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ypDDcpydNiL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ypDDcpydNiL",
        "outputId": "8d0f72f4-2c05-408b-9f45-e983df99443b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ahoy matey! We be thrilled ye be enjoyin’ yer new gamin’ contraption. Hoist the sails and game on, yarrr!\n"
          ]
        }
      ],
      "source": [
        "# Generate a response in our new style using the ChatOpenAI instance.\n",
        "service_response = chat.invoke(service_messages)\n",
        "print(service_response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l1n0G2bVdTTT",
      "metadata": {
        "id": "l1n0G2bVdTTT"
      },
      "outputs": [],
      "source": [
        "# Define how we would like the output from the language model to look like.\n",
        "desired_output = {\n",
        "    \"gift\": False,\n",
        "    \"delivery_days\": 2,\n",
        "    \"price_value\": \"Totally worth the price!\"\n",
        "}\n",
        "\n",
        "# Create a customer review and a template for extracting information from the review.\n",
        "customer_review = \"\"\"\\\n",
        "I'm super excited about the new gaming console I bought! It arrived in just 2 days and I've been playing non-stop. Totally worth the price!\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "text: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KmtSMLHteCbE",
      "metadata": {
        "id": "KmtSMLHteCbE"
      },
      "outputs": [],
      "source": [
        "import langchain_core\n",
        "import langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gy4090xYf2UZ",
      "metadata": {
        "id": "gy4090xYf2UZ"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "import inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Osf7bQKevvo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Osf7bQKevvo",
        "outputId": "73debed2-d3ba-4c43-b8fb-0f1d3f238141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.2.3\n",
            "/usr/local/lib/python3.12/dist-packages/langchain/__init__.py\n"
          ]
        }
      ],
      "source": [
        "print(langchain.__version__)\n",
        "print(inspect.getfile(langchain))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QiJ6LdV4flJU",
      "metadata": {
        "id": "QiJ6LdV4flJU"
      },
      "outputs": [],
      "source": [
        "#check if model exists\n",
        "#import langchain.output_parsers as op\n",
        "#print(dir(op))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hOoCog9Ug_LP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOoCog9Ug_LP",
        "outputId": "672e8fbe-d0f9-47c7-fbbb-d0c67b20caa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eBCgWE-VhYe2",
      "metadata": {
        "id": "eBCgWE-VhYe2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pydantic import BaseModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Kb72b5WNh-JE",
      "metadata": {
        "id": "Kb72b5WNh-JE"
      },
      "outputs": [],
      "source": [
        "# Step 3: Define Pydantic model for structured output\n",
        "class ReviewAnalysis(BaseModel):\n",
        "    gift: bool\n",
        "    delivery_days: int\n",
        "    price_value: list[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85uGMLA1iA3w",
      "metadata": {
        "id": "85uGMLA1iA3w"
      },
      "outputs": [],
      "source": [
        "## Start by creating an instance of the AzureChatOpenAI class.\n",
        "chat = AzureChatOpenAI(\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"API_KEY\"),\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    deployment_name=\"gpt-4.1\",\n",
        "    temperature=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EtrhnGfJiIOS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtrhnGfJiIOS",
        "outputId": "a472c744-4aa2-4541-f471-320a9adfe599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw dict output:\n",
            "{'gift': True, 'delivery_days': 2, 'price_value': ['a bit high', 'around $500', 'totally worth it']}\n",
            "\n",
            "Pydantic model output:\n",
            "gift=True delivery_days=2 price_value=['a bit high', 'around $500', 'totally worth it']\n",
            "Gift: True\n",
            "Delivery days: 2\n",
            "Price sentences: ['a bit high', 'around $500', 'totally worth it']\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Prepare prompt template\n",
        "review_template = \"\"\"Analyze the following customer review and return a JSON object\n",
        "matching this structure:\n",
        "\n",
        "{{\n",
        "  \"gift\": true/false,\n",
        "  \"delivery_days\": int,\n",
        "  \"price_value\": [ ... ]\n",
        "}}\n",
        "\n",
        "Review:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(review_template)\n",
        "\n",
        "# Step 6: Example review text\n",
        "customer_review = \"\"\"\n",
        "I bought this gaming console as a present for my brother.\n",
        "It arrived in just 2 days, which was amazing!\n",
        "The price was a bit high, around $500, but totally worth it.\n",
        "\"\"\"\n",
        "\n",
        "# Step 7: Format messages\n",
        "messages = prompt.format_messages(text=customer_review)\n",
        "\n",
        "# Step 8: Invoke AzureChatOpenAI\n",
        "response = chat.invoke(messages)\n",
        "\n",
        "# Step 9: Parse JSON into dictionary\n",
        "try:\n",
        "    output_dict = json.loads(response.content)\n",
        "except json.JSONDecodeError:\n",
        "    print(\"Failed to parse JSON. Raw response:\")\n",
        "    print(response.content)\n",
        "    output_dict = {}\n",
        "# Step 10: Inspect results\n",
        "print(\"Raw dict output:\")\n",
        "print(output_dict)\n",
        "\n",
        "# Optional: convert to Pydantic model for type safety\n",
        "try:\n",
        "    analysis = ReviewAnalysis(**output_dict)\n",
        "except Exception as e:\n",
        "    print(\"Failed to convert to Pydantic model:\", e)\n",
        "    analysis = None\n",
        "\n",
        "if analysis:\n",
        "    print(\"\\nPydantic model output:\")\n",
        "    print(analysis)\n",
        "    print(\"Gift:\", analysis.gift)\n",
        "    print(\"Delivery days:\", analysis.delivery_days)\n",
        "    print(\"Price sentences:\", analysis.price_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U8y9W42EidU7",
      "metadata": {
        "id": "U8y9W42EidU7"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableSequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PVqs-VXRi_Dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVqs-VXRi_Dc",
        "outputId": "eea7747a-414a-4585-9de0-60f98a97c5a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! I'm here and ready to help. How can I assist you today?\n",
            "Certainly! So far, you greeted me and asked how I was doing. I responded and asked how I could assist you. Then, you requested a summary of our conversation. Would you like to continue or ask about something specific?\n"
          ]
        }
      ],
      "source": [
        "# Memory as Python list\n",
        "chat_history = []\n",
        "\n",
        "def add_to_memory(role: str, content: str):\n",
        "    chat_history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "# Prompt template that includes memory\n",
        "prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"{chat_history}\\nUser: {input}\\nAssistant:\"\n",
        ")\n",
        "\n",
        "# Function to generate response with memory\n",
        "def chat_with_memory(user_input: str):\n",
        "    add_to_memory(\"user\", user_input)\n",
        "    # Flatten memory into string\n",
        "    history_str = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in chat_history])\n",
        "    messages = prompt_template.format_messages(chat_history=history_str, input=user_input)\n",
        "    response = chat.invoke(messages)\n",
        "    add_to_memory(\"assistant\", response.content)\n",
        "    return response.content\n",
        "\n",
        "# Example usage\n",
        "print(chat_with_memory(\"Hello! How are you?\"))\n",
        "print(chat_with_memory(\"Can you summarize what we talked about so far?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2YQSAchhkrWl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YQSAchhkrWl",
        "outputId": "cd37875b-6a1c-435c-c855-2b95215706c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello again! Welcome back. How can I assist you this time?\n"
          ]
        }
      ],
      "source": [
        "#Multi-step memory aware conversation\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "def remember_step(user_input):\n",
        "    add_to_memory(\"user\", user_input)\n",
        "    return user_input\n",
        "\n",
        "def llm_step(user_input):\n",
        "    history_str = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in chat_history])\n",
        "    messages = prompt_template.format_messages(chat_history=history_str, input=user_input)\n",
        "    response = chat.invoke(messages)\n",
        "    add_to_memory(\"assistant\", response.content)\n",
        "    return response.content\n",
        "\n",
        "sequence = RunnableSequence(remember_step, llm_step)\n",
        "\n",
        "# Run sequence\n",
        "output = sequence.invoke(\"Hello again!\")\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9QEam4NckxxP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QEam4NckxxP",
        "outputId": "f94efae3-f6e3-4ed0-9f99-04a6d9b4c839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No problem! You can go ahead and ask your question. If you’ve already asked it earlier, I’ll let you know or provide the answer again. What would you like to know?\n"
          ]
        }
      ],
      "source": [
        "output = sequence.invoke(\"I have a question ,but not sure if I asked you earlier\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ARaWIT8BOdqX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARaWIT8BOdqX",
        "outputId": "8a336be6-6285-4c8f-8818-681ce1047b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The average distance from the Earth to the Moon is about **384,400 kilometers** (approximately **238,855 miles**). This distance can vary slightly because the Moon's orbit around Earth is not a perfect circle—sometimes it's a bit closer (at perigee) and sometimes a bit farther away (at apogee). But 384,400 km is the commonly cited average distance.\n"
          ]
        }
      ],
      "source": [
        "output = sequence.invoke(\"how far is moon in general from earth\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gqJNXmZJOjW0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqJNXmZJOjW0",
        "outputId": "2f4d9602-61f4-4f73-dc45-37c66a3c2c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No worries! Feel free to ask your question about the Moon and Earth. If you’ve already asked it before, I’ll let you know or provide the answer again. What would you like to know?\n"
          ]
        }
      ],
      "source": [
        "output = sequence.invoke(\"I have a question about moon and earth ,but not sure if I asked you earlier\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ykuE8Rs5OoPk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykuE8Rs5OoPk",
        "outputId": "ac520417-1fc4-470a-8b72-995b553685c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, you did ask this question earlier! I answered that the average distance from the Earth to the Moon is about **384,400 kilometers** (or **238,855 miles**). If you have any more questions or need further details, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "output = sequence.invoke(\"how far is moon in general from earth,did i ask you this earlier\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QiOV6midOt69",
      "metadata": {
        "id": "QiOV6midOt69"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "2.6-Langchain, structuring & memory.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
