{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCXRSbESS77U"
      },
      "source": [
        "# RAG Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xBz-KrbtP73"
      },
      "source": [
        "# **Step1: Install and import the dependecies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JH29BgsbRiMu"
      },
      "outputs": [],
      "source": [
        "#!pip install sentence-transformers bitsandbytes\n",
        "from langchain_text_splitters import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    TokenTextSplitter,SentenceTransformersTokenTextSplitter\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2DQ4kvj-bQS7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dratzjpEtUEd"
      },
      "source": [
        "# **Step 2: Load the document**\n",
        "\n",
        "Load the document that will be used as the knowledge source.\n",
        "\n",
        "**Knowledge base**: The text document serves as the underlying knowledge base. Later, when a query is made, relevant parts of this document will be retrieved to augment the LLM's response.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H9OBAJS8sc59"
      },
      "outputs": [],
      "source": [
        "text_loader = TextLoader(\"/content/state_of_union.txt\")\n",
        "text_document = text_loader.load()\n",
        "print(text_document[:100])  # Prints the first 100 characters of the text document\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajrBtwcHtYMh"
      },
      "source": [
        "# **Step 3: Split the document into chunks**\n",
        "\n",
        "Break down the large document into manageable pieces.\n",
        "\n",
        "**Fine-Grained Retrieval**: Smaller chunks allow the retriever to more precisely locate the context relevant to the query, enhancing the generation step with focused context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IX10Ld0bsxgM"
      },
      "outputs": [],
      "source": [
        "doc_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
        "split_texts = doc_splitter.split_documents(text_document)\n",
        "print(len(split_texts))  # Prints the number of chunks the PDF has been split into\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOMVE7TKto9O"
      },
      "source": [
        "# **Step 4: Generate embeddings for each chunk**\n",
        "\n",
        "Convert text chunks into numerical vectors (embeddings) that capture semantic meaning.\n",
        "\n",
        "**Semantic Search**: Embeddings allow the FAISS vector store to perform similarity searches, ensuring that the most relevant context is retrieved for any given query.\n",
        "\n",
        "**Verification**: Printing the length of the embedding vector confirms the transformation was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2UqIhbcsnXX",
        "outputId": "bea43679-2419-4f05-f5d4-90e54c701702"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3099908222.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  hf_embed = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "768\n"
          ]
        }
      ],
      "source": [
        "#model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "#embeddings = HuggingFaceEmbeddings(model_name=model)\n",
        "\n",
        "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "hf_embed = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
        "text = split_texts[0].page_content\n",
        "hf_embed_result = hf_embed.embed_documents([text])\n",
        "print(len(hf_embed_result[0]))  # Prints the length of the first embedded document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8aO836YuRbU"
      },
      "source": [
        "#### If we quickly want to see how the embeddings for the chunks will look like we will do the below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bPcS4HDBs-EU"
      },
      "outputs": [],
      "source": [
        "embedded_chunks = [hf_embed.embed_query(chunk.page_content) for chunk in split_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "op5I6W5CtvTV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_chunks = pd.DataFrame(embedded_chunks)\n",
        "df_chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IgwrSOQu5kk"
      },
      "source": [
        "# **Step 5: Build the FAISS vector store and create a retriever**\n",
        "\n",
        "Build an index (FAISS) for the document embeddings and create a retriever.\n",
        "\n",
        "**Retrieval step**: The retriever is responsible for fetching the most relevant chunks from the document based on the query. These retrieved contexts will later be fed into the generation step to produce an informed answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6pgEZzuucIN",
        "outputId": "2dbe58ea-eafe-4664-ea07-7067fd5a87c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "vectorstore=FAISS.from_documents(split_texts, hf_embed)\n",
        "\n",
        "# It will take the same embedding of the chunks as shown above and and create a vecor database for it which will be temporary, ie non persistent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhADqsOIu-U-"
      },
      "source": [
        "#### Let's see if the retriever works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U8ALpBocuqS2"
      },
      "outputs": [],
      "source": [
        "retriever=vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "D4Qn0wyAd-r5"
      },
      "outputs": [],
      "source": [
        "print(dir(retriever))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "84QbBaUWvBCh",
        "outputId": "184070b1-83c7-4a06-b4d7-712b6ab06e3d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'query = \"What are the key points from the State Of The Union\"\\n# Directly call the retriever as a function\\ndocs = retriever(query)  # NOTE: retriever is callable'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The way the retriever works\n",
        "'''query = \"What are the key points from the State Of The Union\"\n",
        "# Directly call the retriever as a function\n",
        "docs = retriever(query)  # NOTE: retriever is callable'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kjyc-finkILq"
      },
      "outputs": [],
      "source": [
        "#Instead of using retriever, since we are using newer versions\n",
        "query_embedding = hf_embed.embed_query(\"What are the key points from the State Of The Union\")\n",
        "similar_docs = vectorstore.similarity_search_by_vector(query_embedding, k=5)  # top 5 results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aVfo7-kikPz7",
        "outputId": "aa104848-22ea-48d9-faac-341b7c87ccbb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'for doc in similar_docs:\\n    print(doc.page_content)'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for doc in similar_docs:\n",
        "    print(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QGNVIRo3vC-0"
      },
      "outputs": [],
      "source": [
        "query2 = \"How is the United States supporting Ukraine economically and militarily?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hjdBwjRAvFZ3",
        "outputId": "f0adf960-175d-4cfa-b6c5-c6b33c17b8e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'for doc in similar_docs:\\n    print(doc.page_content)'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Query is embedded, FAISS retrieves the top-k most relevant document chunks\n",
        "   These are grounding context, not model memory '''\n",
        "query_embedding = hf_embed.embed_query(query2)\n",
        "similar_docs = vectorstore.similarity_search_by_vector(query_embedding, k=5)  # top 5 results\n",
        "for doc in similar_docs:\n",
        "    print(doc.page_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sZogM2J6d-4"
      },
      "outputs": [],
      "source": [
        "#Note** If using gpt model and AzureOpenAI or AzureChatOpenAI (refer: 'Working_with_AzureOpenAI' folder)\n",
        "import os\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/content/.env\")\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"API_KEY\"),\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    deployment_name=\"gpt-4.1\",\n",
        "    temperature=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPTj3ueqkqbS",
        "outputId": "ecdb3019-cd07-4363-cba8-5eeb73fc066e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='**Key Points from the State of the Union (based on the provided context):**\\n\\n1. **Russian Stock Market and Economy:**  \\n   - The Russian stock market has lost 40% of its value and trading is suspended.\\n   - Russia’s economy is suffering, and President Putin is solely blamed for this situation.\\n\\n2. **Support for Ukraine:**  \\n   - The U.S. and its allies are providing military, economic, and humanitarian assistance to Ukraine.\\n   - Over $1 billion in direct assistance is being given to Ukraine.\\n   - Continued commitment to aid the Ukrainian people as they defend their country and to help ease their suffering.\\n\\n3. **International Coalition:**  \\n   - Support for Ukraine is broad, including all 27 EU members (France, Germany, Italy), the UK, Canada, Japan, Korea, Australia, New Zealand, Switzerland, and others.\\n   - The U.S. is working with 29 other nations to enforce powerful economic sanctions on Russia.\\n\\n4. **Sanctions and Isolation of Russia:**  \\n   - The U.S. and allies are inflicting economic pain on Russia and isolating Putin from the world.\\n   - American diplomacy and resolve are emphasized as critical in this effort.\\n\\n5. **Putin’s Actions and Western Response:**  \\n   - Putin’s attack on Ukraine was premeditated and unprovoked.\\n   - He rejected diplomatic efforts and underestimated the unity and preparedness of the West and NATO.\\n   - The West was ready and responded with coordinated actions.\\n\\n6. **Ukrainian Resistance:**  \\n   - The Ukrainian people, with 30 years of independence, are determined not to let anyone take their country backwards.\\n\\n7. **Impact on Americans and Global Costs:**  \\n   - The President is honest about the global costs of Russia’s invasion, acknowledging its impact around the world.\\n\\n8. **U.S. Military Position:**  \\n   - U.S. forces are not engaged in Ukraine and will not fight Russian forces there.\\n   - U.S. deployments to Europe are to defend NATO allies (Poland, Romania, Latvia, Lithuania, Estonia) in case of further Russian aggression.\\n\\n**Summary:**  \\nThe State of the Union highlights strong U.S. and allied support for Ukraine, severe sanctions against Russia, the isolation of Putin, and the unity and preparedness of NATO. It reassures Americans that U.S. forces will not fight in Ukraine but are deployed to defend NATO allies, and it acknowledges the global impact of the conflict.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 502, 'prompt_tokens': 459, 'total_tokens': 961, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz4oBxtgoAl5nkP1TUA3bN4nmS22w', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd19-01e4-79d1-a77f-beb401e387d9-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 459, 'output_tokens': 502, 'total_tokens': 961, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "#Context construction\n",
        "context = \"\\n\".join([doc.page_content for doc in similar_docs])\n",
        "\n",
        "#To limit tokens for large contexts\n",
        "#context = \"\\n\".join([doc.page_content[:1000] for doc in similar_docs])\n",
        "answer = llm.invoke(f\"Based on the following context, answer the question:\\n\\n{context}\\n\\nQuestion:\\nWhat are the key points from the State Of The Union?\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcUoui50vN3A"
      },
      "source": [
        "# **Step 6: Design a prompt template for the language model**\n",
        "Establish a prompt that instructs the LLM on how to utilize the retrieved context to generate a concise answer.\n",
        "\n",
        "**Guiding Generation**: The prompt template bridges retrieval and generation by ensuring the LLM uses the provided context (from the retriever) to answer the query accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owZKIB3NvGga"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2neeknSivQyL"
      },
      "outputs": [],
      "source": [
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use one sentence and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-P-b5SqvSb0"
      },
      "outputs": [],
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-RJ6awdvV47"
      },
      "outputs": [],
      "source": [
        "output_parser=StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7iSHABuz1zn"
      },
      "outputs": [],
      "source": [
        "def rag_chain(query, vectorstore, llm, prompt, output_parser, k=5):\n",
        "    # Step 1: Retrieve relevant documents\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    similar_docs = vectorstore.similarity_search_by_vector(query_embedding, k=k)\n",
        "\n",
        "    # Step 2: Build context string\n",
        "    context = \"\\n\".join([doc.page_content for doc in similar_docs])\n",
        "\n",
        "    # Step 3: Format the prompt\n",
        "    '''Context + question are injected into the prompt\n",
        "       We are using chat messages, which is correct for GPT\n",
        "       System + human message roles are respected '''\n",
        "    messages = prompt.format_messages(\n",
        "    question=query,\n",
        "    context=context\n",
        "    )\n",
        "\n",
        "    # Step 4: Call LLM\n",
        "    '''GPT-4.1 reasons only over the provided context\n",
        "       The model is not searching FAISS or the web\n",
        "       No fine-tuning is happening — this is pure inference '''\n",
        "    llm_output = llm.invoke(messages)\n",
        "\n",
        "    # Step 5: Parse output\n",
        "    answer = output_parser.parse(llm_output)\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTcQvh0Mz6he",
        "outputId": "a2302b9b-e530-47a4-ed3d-d56edc3fb141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: What are the key points from the State Of The Union?\n",
            "A: content='Key points from the State of the Union include unity across parties, ongoing COVID-19 recovery, prosecuting pandemic fraud, significant deficit reduction, support for education and workers, raising the minimum wage, extending the Child Tax Credit, and focusing economic relief on working Americans.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 555, 'total_tokens': 609, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5Ftz6h1x8Yh5zNnvZX1lKWyXfi5', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd33-3bdc-75e1-8c7a-ff56a8835008-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 555, 'output_tokens': 54, 'total_tokens': 609, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Q: How is the United States supporting Ukraine economically and militarily?\n",
            "A: content='The United States is supporting Ukraine by providing over $1 billion in direct economic assistance, military aid, and humanitarian support, while also enforcing powerful economic sanctions against Russia.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 498, 'total_tokens': 532, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5Fu1tnl4OXU07sIqmyZkxaJ651R', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd33-3f34-7bc0-811b-0ef04d51df40-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 498, 'output_tokens': 34, 'total_tokens': 532, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"What are the key points from the State Of The Union?\",\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    answer = rag_chain(q, vectorstore, llm, prompt, output_parser, k=5)\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx15YT1GNNA3"
      },
      "outputs": [],
      "source": [
        "#Update the RAG function to return sources\n",
        "def rag_chain_with_sources(query, vectorstore, llm, prompt, output_parser, k=5):\n",
        "    # 1. Retrieve documents\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    similar_docs = vectorstore.similarity_search_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    # 2. Build context\n",
        "    context = \"\\n\".join(doc.page_content for doc in similar_docs)\n",
        "\n",
        "    # 3. Format prompt into messages\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=context\n",
        "    )\n",
        "\n",
        "    # 4. Invoke LLM\n",
        "    llm_output = llm.invoke(messages)\n",
        "    answer = output_parser.parse(llm_output)\n",
        "\n",
        "    # 5. Collect sources\n",
        "    sources = []\n",
        "    for doc in similar_docs:\n",
        "        source = doc.metadata.get(\"source\", \"unknown\")\n",
        "        sources.append(source)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    sources = list(dict.fromkeys(sources))\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"sources\": sources,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "6SH97EI_NSb9",
        "outputId": "3733d4a6-31b8-4248-dc21-71249f29079c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:\n",
            "content='The United States is supporting Ukraine by providing over $1 billion in direct economic assistance, military aid, and humanitarian support, while also enforcing powerful economic sanctions against Russia.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 498, 'total_tokens': 532, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5NhvZoX16d4iMAigH2oN4Y1FLIm', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd3a-9db6-7d53-b105-07470b3d3693-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 498, 'output_tokens': 34, 'total_tokens': 532, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Sources:\n",
            "- /content/state_of_union.txt\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nNow we have:\\nGrounded answers\\nTransparent citations\\nAuditable RAG\\n'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = rag_chain_with_sources(\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\",\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    output_parser,\n",
        ")\n",
        "\n",
        "print(\"Answer:\")\n",
        "print(result[\"answer\"])\n",
        "\n",
        "print(\"\\nSources:\")\n",
        "for src in result[\"sources\"]:\n",
        "    print(\"-\", src)\n",
        "'''\n",
        "Now we have:\n",
        "Grounded answers\n",
        "Transparent citations\n",
        "Auditable RAG\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "KIuOcf-yNYlZ",
        "outputId": "d2f1341a-d949-4607-9301-6bd618e45fba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nOur system should:\\n\\nAnswer only when the retrieved context is strong\\nSay “I don’t know based on the provided context” when it isn’t\\n\\nThis is true RAG safety'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Goal\n",
        "'''\n",
        "Our system should:\n",
        "Answer only when the retrieved context is strong\n",
        "Say “I don’t know based on the provided context” when it isn’t\n",
        "This is true RAG safety'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "axOOtJnIPi4I",
        "outputId": "c08c59d9-e659-4691-8807-073341ef634e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'We can use Heuristics  (version-safe):\\n    Number of retrieved chunks\\n    Minimum similarity score\\n    Context length'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We’ll use retrieval signal, not LLM guessing:\n",
        "'''We can use Heuristics  (version-safe):\n",
        "    Number of retrieved chunks\n",
        "    Minimum similarity score\n",
        "    Context length'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68ZDPHWTQuvU"
      },
      "outputs": [],
      "source": [
        "#Define confidence thresholds\n",
        "MAX_SCORE_THRESHOLD = 0.6      # tune this\n",
        "MIN_CONTEXT_LENGTH = 300       # characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ofscj04RAo1"
      },
      "outputs": [],
      "source": [
        "def rag_chain_with_confidence(query, vectorstore, llm, prompt, output_parser, k=5):\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "\n",
        "    results = vectorstore.similarity_search_with_score_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    # Separate docs and scores\n",
        "    docs = [doc for doc, score in results]\n",
        "    scores = [score for doc, score in results]\n",
        "\n",
        "    # Build context\n",
        "    context = \"\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # --- Confidence checks ---\n",
        "    is_confident = True\n",
        "\n",
        "    if len(docs) == 0:\n",
        "        is_confident = False\n",
        "\n",
        "    if min(scores) > MAX_SCORE_THRESHOLD:\n",
        "        is_confident = False\n",
        "\n",
        "    if len(context) < MIN_CONTEXT_LENGTH:\n",
        "        is_confident = False\n",
        "\n",
        "    # If confidence is low → safe answer\n",
        "    if not is_confident:\n",
        "        return {\n",
        "            \"answer\": \"I don't know based on the provided context.\",\n",
        "            \"confidence\": \"low\",\n",
        "            \"sources\": [],\n",
        "        }\n",
        "\n",
        "    # Format prompt\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=context\n",
        "    )\n",
        "\n",
        "    # LLM call\n",
        "    llm_output = llm.invoke(messages)\n",
        "    answer = output_parser.parse(llm_output)\n",
        "\n",
        "    # Sources\n",
        "    sources = list(dict.fromkeys(\n",
        "        doc.metadata.get(\"source\", \"unknown\") for doc in docs\n",
        "    ))\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"confidence\": \"high\",\n",
        "        \"sources\": sources,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbbFeHArRShj",
        "outputId": "72461468-0f47-442e-a592-5cf1bf1a0e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: How is the United States supporting Ukraine economically and militarily?\n",
            "Answer: content='The United States is supporting Ukraine by providing over $1 billion in direct economic assistance, military aid, and humanitarian support, while also enforcing powerful economic sanctions against Russia.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 498, 'total_tokens': 532, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5Z6wrZe16HDbPcwBkQ0OkTHPhDS', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd45-6671-7080-bba3-e3d334c782d5-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 498, 'output_tokens': 34, 'total_tokens': 532, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Confidence: high\n",
            "Sources: ['/content/state_of_union.txt']\n",
            "\n",
            "Q: What is the GDP of Atlantis in 2024?\n",
            "Answer: I don't know based on the provided context.\n",
            "Confidence: low\n",
            "Sources: []\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\",\n",
        "    \"What is the GDP of Atlantis in 2024?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    result = rag_chain_with_confidence(\n",
        "        q, vectorstore, llm, prompt, output_parser\n",
        "    )\n",
        "\n",
        "    print(\"\\nQ:\", q)\n",
        "    print(\"Answer:\", result[\"answer\"])\n",
        "    print(\"Confidence:\", result[\"confidence\"])\n",
        "    print(\"Sources:\", result[\"sources\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfTetClmeQri"
      },
      "source": [
        "## **Without confidence rating:**\n",
        "\n",
        "* LLMs answer even when retrieval is weak\n",
        "* This causes hallucinations\n",
        "\n",
        "  With confidence gating:\n",
        "* Your system behaves honestly\n",
        "* Users trust it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfCr31tKe111"
      },
      "source": [
        "## **Without confidence rating:**\n",
        "\n",
        "  * LLMs answer even when retrieval is weak\n",
        "  * This causes hallucinations\n",
        "\n",
        "    With confidence gating:\n",
        "  * Your system behaves honestly\n",
        "  * Users trust it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEq6bTWHfNWT"
      },
      "source": [
        "## Web search fallback\n",
        "  When confidence is low, we’ll:\n",
        "  * Automatically call web search\n",
        "  * Re-answer using fresh data\n",
        "  \n",
        "  If FAISS retrieval confidence is low, your system should:\n",
        "  Perform a web search\n",
        "  * Build new context from live data\n",
        "  * Ask GPT-4.1 again\n",
        "  * Clearly label the answer as coming from the web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD9Y4_-gSj8b"
      },
      "outputs": [],
      "source": [
        "#User question -->FAISS Retrieval -->Confidence Check\n",
        "#If High Confidence -->Answer from FAISS\n",
        "#If Low Confidence -->Web Search -->GPT --> Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udHfTehaUEB4"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/content/.env\")\n",
        "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\")\n",
        "\n",
        "def web_search(query, num_results=5):\n",
        "    url = \"https://serpapi.com/search.json\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERPAPI_KEY,\n",
        "        \"num\": num_results,\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    results = []\n",
        "    for r in data.get(\"organic_results\", []):\n",
        "        snippet = r.get(\"snippet\")\n",
        "        source = r.get(\"link\")\n",
        "        if snippet:\n",
        "            results.append({\n",
        "                \"content\": snippet,\n",
        "                \"source\": source,\n",
        "            })\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXO-XZCZUL6s"
      },
      "outputs": [],
      "source": [
        "#Build web context\n",
        "def build_web_context(results):\n",
        "    context = []\n",
        "    sources = []\n",
        "\n",
        "    for r in results:\n",
        "        context.append(r[\"content\"])\n",
        "        sources.append(r[\"source\"])\n",
        "\n",
        "    return \"\\n\".join(context), list(dict.fromkeys(sources))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UYHLatcUPCi"
      },
      "outputs": [],
      "source": [
        "def rag_chain_structured(\n",
        "    query,\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    output_parser,\n",
        "    k=5,\n",
        "):\n",
        "    # --- FAISS retrieval ---\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    results = vectorstore.similarity_search_with_score_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    docs = [doc for doc, score in results]\n",
        "    scores = [score for doc, score in results]\n",
        "\n",
        "    context = \"\\n\".join(f\"{doc.page_content}\" for doc in docs)\n",
        "\n",
        "    # Confidence check\n",
        "    confident = len(docs) > 0 and min(scores) <= 0.6 and len(context) >= 300\n",
        "\n",
        "    if not confident:\n",
        "        # Optional: fallback to web search\n",
        "        web_results = web_search(query)\n",
        "        if web_results:\n",
        "            context = \"\\n\".join(r['content'] for r in web_results)\n",
        "            sources = [{\n",
        "                \"source\": r['source'],  # Changed from r['url'] to r['source']\n",
        "                \"score\": None,\n",
        "                \"text\": r['content']\n",
        "            } for r in web_results]\n",
        "            source_type = \"web\"\n",
        "            confidence = \"medium\"\n",
        "        else:\n",
        "            return {\n",
        "                \"answer\": \"I don't know based on available information.\",\n",
        "                \"confidence\": \"low\",\n",
        "                \"source_type\": \"none\",\n",
        "                \"sources\": [],\n",
        "            }\n",
        "    else:\n",
        "        sources = [\n",
        "            {\n",
        "                \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
        "                \"score\": score,\n",
        "                \"text\": doc.page_content\n",
        "            }\n",
        "            for doc, score in results\n",
        "        ]\n",
        "        source_type = \"vectorstore\"\n",
        "        confidence = \"high\"\n",
        "\n",
        "    # --- Prompt + LLM ---\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=context\n",
        "    )\n",
        "\n",
        "    llm_output = llm.invoke(messages)\n",
        "    answer = output_parser.parse(llm_output)\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"confidence\": confidence,\n",
        "        \"source_type\": source_type,\n",
        "        \"sources\": sources,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwmpTKExUWtg",
        "outputId": "8862d56a-5022-4f01-80c6-c664132a1dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: How is the United States supporting Ukraine economically and militarily?\n",
            "Answer: content='The United States is supporting Ukraine by providing over $1 billion in direct economic assistance, military aid, and humanitarian support, while also enforcing powerful economic sanctions against Russia.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 498, 'total_tokens': 532, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5rZdZN8RxRfsngUPiniD7gnWxjs', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd56-de21-75f1-ab1e-c30d2757fdeb-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 498, 'output_tokens': 34, 'total_tokens': 532, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Confidence: high\n",
            "Source Type: vectorstore\n",
            "Sources:\n",
            "- /content/state_of_union.txt\n",
            "\n",
            "Q: What happened in the latest Apple WWDC?\n",
            "Answer: content='At the latest Apple WWDC, Apple announced its broadest design update ever, introduced more advanced Apple Intelligence features, and unveiled exciting new capabilities across all its platforms.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 186, 'total_tokens': 220, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5rebPw78ytZntbPKrJA8m9G2sUP', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd56-f2f2-78d2-a550-1247262a1fc3-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 186, 'output_tokens': 34, 'total_tokens': 220, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Confidence: medium\n",
            "Source Type: web\n",
            "Sources:\n",
            "- https://developer.apple.com/wwdc25/\n",
            "- https://www.apple.com/apple-events/\n",
            "- https://www.wired.com/live/apple-wwdc-2025-live-blog/\n",
            "- https://www.reddit.com/r/iOSProgramming/comments/1l3jpce/what_was_the_biggest_thing_dropped_on_a_wwdc/\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\",\n",
        "    \"What happened in the latest Apple WWDC?\"]\n",
        "\n",
        "for q in questions:\n",
        "    result = rag_chain_with_web_fallback(\n",
        "        q, vectorstore, llm, prompt, output_parser\n",
        "    )\n",
        "\n",
        "    print(\"\\nQ:\", q)\n",
        "    print(\"Answer:\", result[\"answer\"])\n",
        "    print(\"Confidence:\", result[\"confidence\"])\n",
        "    print(\"Source Type:\", result[\"source_type\"])\n",
        "    print(\"Sources:\")\n",
        "    for s in result[\"sources\"]:\n",
        "        print(\"-\", s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibBd8sLPf7je"
      },
      "source": [
        "## Streaming responses (UX+ production readiness)\n",
        "   Stream GPT tokens live\n",
        "   * token streaming so answers appear progressively  instead of waiting for the full response.\n",
        "   * Show partial answers while generating\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "dN7x0UdkWAPQ",
        "outputId": "6cfca11a-e2d2-44b2-9cc0-bb0f15ddeb0d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nIn new LangChain versions:\\n\\nStreaming is handled via callbacks\\nChat models emit tokens via on_llm_new_token\\nNo model.eval() concept exists for GPT models\\n(that’s only for local PyTorch models)\\n'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "In new LangChain versions:\n",
        "\n",
        "Streaming is handled via callbacks\n",
        "Chat models emit tokens via on_llm_new_token\n",
        "No model.eval() concept exists for GPT models\n",
        "(that’s only for local PyTorch models)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LX7CkmmTWWlQ"
      },
      "outputs": [],
      "source": [
        "#Create a streaming callback handler\n",
        "from langchain_core.callbacks import BaseCallbackHandler\n",
        "\n",
        "class StreamingStdOutCallbackHandler(BaseCallbackHandler):\n",
        "    def on_llm_new_token(self, token: str, **kwargs):\n",
        "        print(token, end=\"\", flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNsMRVA1Wdbz"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "streaming_llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"API_KEY\"),\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    deployment_name=\"gpt-4.1\",\n",
        "    temperature=0,\n",
        "    streaming=True,\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        ")\n",
        "#Note-callbacks must be passed at LLM creation time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGxBgwzKWhd3"
      },
      "outputs": [],
      "source": [
        "#Update RAG to support streaming\n",
        "def rag_chain_streaming(\n",
        "    query,\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    output_parser,\n",
        "    k=5,\n",
        "):\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    results = vectorstore.similarity_search_with_score_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    docs = [doc for doc, score in results]\n",
        "    scores = [score for doc, score in results]\n",
        "\n",
        "    context = \"\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    confident = (\n",
        "        len(docs) > 0\n",
        "        and min(scores) <= 0.6\n",
        "        and len(context) >= 300\n",
        "    )\n",
        "\n",
        "    if confident:\n",
        "        print(\"\\nAnswer (FAISS):\\n\")\n",
        "\n",
        "        messages = prompt.format_messages(\n",
        "            question=query,\n",
        "            context=context\n",
        "        )\n",
        "\n",
        "        llm.invoke(messages)  # streams automatically\n",
        "        print(\"\\n\")\n",
        "\n",
        "        return\n",
        "\n",
        "    # ---- Web fallback ----\n",
        "    print(\"\\nAnswer (Web Search):\\n\")\n",
        "\n",
        "    web_results = web_search(query)\n",
        "    web_context, _ = build_web_context(web_results)\n",
        "\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=web_context\n",
        "    )\n",
        "\n",
        "    llm.invoke(messages)\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmzqVPT4WzWW",
        "outputId": "acc93810-ac30-42c3-835d-efc8e013e359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Q: What are the key points from the State Of The Union?\n",
            "\n",
            "Answer (Web Search):\n",
            "\n",
            "Key points from the State of the Union include a focus on the health of the economy and immigration.\n",
            "\n",
            "\n",
            "==============================\n",
            "Q: How is the United States supporting Ukraine economically and militarily?\n",
            "\n",
            "Answer (FAISS):\n",
            "\n",
            "The United States is supporting Ukraine by providing over $1 billion in direct economic assistance, military aid, and humanitarian support, while also enforcing powerful economic sanctions against Russia.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Run it now\n",
        "questions = [\n",
        "    \"What are the key points from the State Of The Union?\",\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Q:\", q)\n",
        "    rag_chain_streaming(\n",
        "        q,\n",
        "        vectorstore,\n",
        "        streaming_llm,\n",
        "        prompt,\n",
        "        output_parser,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "OwWcSrudXBKd",
        "outputId": "85535477-6748-4359-8e21-ea76c9734062"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Right now your system:\\n\\n  -- Retrieves documents\\n  -- Answers correctly\\n  -- Streams responses\\n\\n  But it does not explain where the answer came from.\\n\\n  In Next Step we will:\\n  -- Track document metadata\\n  -- Inject sources into the prompt\\n  -- Return answer + citations\\n  -- Work for FAISS and web fallback\\n  '"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Source Attribution & Citations (FAISS + Web)\n",
        "'''Right now your system:\n",
        "\n",
        "  -- Retrieves documents\n",
        "  -- Answers correctly\n",
        "  -- Streams responses\n",
        "\n",
        "  But it does not explain where the answer came from.\n",
        "\n",
        "  In Next Step we will:\n",
        "  -- Track document metadata\n",
        "  -- Inject sources into the prompt\n",
        "  -- Return answer + citations\n",
        "  -- Work for FAISS and web fallback\n",
        "  '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "FzIrSe-AXe9h",
        "outputId": "eebedb9a-c10c-4519-f3d7-33da46a2a0cd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nA citation is not a URL necessarily.\\n  It can be:\\n    File name\\n    Document ID\\n    Page number\\n    Chunk index\\n    Web URL\\nYour FAISS docs already support this via Document.metadata\\n'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "A citation is not a URL necessarily.\n",
        "  It can be:\n",
        "    File name\n",
        "    Document ID\n",
        "    Page number\n",
        "    Chunk index\n",
        "    Web URL\n",
        "Your FAISS docs already support this via Document.metadata\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vugj2QpdXrJM"
      },
      "outputs": [],
      "source": [
        "#Ensure metadata exists\n",
        "\n",
        "for i, doc in enumerate(similar_docs):\n",
        "    doc.metadata[\"source\"] = \"State of the Union Address\"\n",
        "    doc.metadata[\"chunk_id\"] = i\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gt2JJmyZGIN"
      },
      "outputs": [],
      "source": [
        "#Modify context construction to include source tags\n",
        "def build_context_with_sources(docs):\n",
        "    context_chunks = []\n",
        "\n",
        "    for i, doc in enumerate(docs):\n",
        "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
        "        chunk_id = doc.metadata.get(\"chunk_id\", i)\n",
        "\n",
        "        context_chunks.append(\n",
        "            f\"[Source {i+1}: {source}, chunk {chunk_id}]\\n{doc.page_content}\"\n",
        "        )\n",
        "\n",
        "    return \"\\n\\n\".join(context_chunks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJhLn9ZqZXtM"
      },
      "outputs": [],
      "source": [
        "#Update prompt to Require citations\n",
        "template = \"\"\"\n",
        "You are an assistant for question-answering tasks.\n",
        "\n",
        "Use ONLY the provided context to answer.\n",
        "Cite sources using the format [Source X].\n",
        "\n",
        "If the answer is not contained in the context, say you don't know.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer (with citations):\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Enaoe45dZhIG"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7MgXIe9Zumy"
      },
      "outputs": [],
      "source": [
        "#Update RAG Chain\n",
        "def rag_chain_with_citations(\n",
        "    query,\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    k=5,\n",
        "):\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    results = vectorstore.similarity_search_with_score_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    docs = [doc for doc, score in results]\n",
        "    scores = [score for doc, score in results]\n",
        "\n",
        "    context = build_context_with_sources(docs)\n",
        "\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=context\n",
        "    )\n",
        "\n",
        "    print(\"\\n Answer:\\n\")\n",
        "    llm.invoke(messages)\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gNVw9NAZ0fs"
      },
      "outputs": [],
      "source": [
        "def build_web_context(web_results):\n",
        "    chunks = []\n",
        "\n",
        "    for i, r in enumerate(web_results):\n",
        "        chunks.append(\n",
        "            f\"[Source {i+1}: {r['source']}]\\n{r['content']}\" # Changed from r['url'] to r['source']\n",
        "        )\n",
        "\n",
        "    return \"\\n\\n\".join(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q1esdx_aTJG",
        "outputId": "54c85316-9a10-46a8-db27-7a18fe7690e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'source': '/content/state_of_union.txt'}\n"
          ]
        }
      ],
      "source": [
        "#Testing\n",
        "doc = vectorstore.similarity_search(\"test\")[0]\n",
        "print(doc.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1cuZ52Napnn",
        "outputId": "9d20fc24-408c-43bf-b0a3-15054541e9ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Document 1 ---\n",
            "Score: 0.51034504\n",
            "Source: State of the Union Address\n",
            "The Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame. \n",
            "\n",
            "Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistan\n",
            "\n",
            "--- Document 2 ---\n",
            "Score: 0.6614249\n",
            "Source: State of the Union Address\n",
            "Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \n",
            "\n",
            "We are inflicting pain on Russia and supporting the people of Ukraine. Putin is now\n",
            "\n",
            "--- Document 3 ---\n",
            "Score: 0.7009914\n",
            "Source: State of the Union Address\n",
            "And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n",
            "\n",
            "To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has cos\n"
          ]
        }
      ],
      "source": [
        "#Test retrieval with scores\n",
        "query = \"How is the United States supporting Ukraine?\"\n",
        "query_embedding = hf_embed.embed_query(query)\n",
        "\n",
        "results = vectorstore.similarity_search_with_score_by_vector(\n",
        "    query_embedding, k=3\n",
        ")\n",
        "\n",
        "for i, (doc, score) in enumerate(results):\n",
        "    print(f\"\\n--- Document {i+1} ---\")\n",
        "    print(\"Score:\", score)\n",
        "    print(\"Source:\", doc.metadata.get(\"source\"))\n",
        "    print(doc.page_content[:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9VVdRNLau29",
        "outputId": "443dda50-0912-4495-9193-fdf792f4b9c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Source 1: State of the Union Address, chunk 0]\n",
            "The Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame. \n",
            "\n",
            "Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. \n",
            "\n",
            "We are giving more than $1 Billion in direct assistance to Ukraine. \n",
            "\n",
            "And we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.\n",
            "\n",
            "[Source 2: State of the Union Address, chunk 1]\n",
            "Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \n",
            "\n",
            "We are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \n",
            "\n",
            "Together with our allies –we are right now enforcing powerful economic sanctions.\n",
            "\n",
            "[Source 3: State of the Union Address, chunk 3]\n",
            "And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n",
            "\n",
            "To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world.\n"
          ]
        }
      ],
      "source": [
        "#Test context construction with source tags\n",
        "context = build_context_with_sources([doc for doc, _ in results])\n",
        "print(context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eNchEsva7am",
        "outputId": "1d6f65db-e916-4e35-d471-52edc579669e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HumanMessage\n",
            "\n",
            "You are an assistant for question-answering tasks.\n",
            "\n",
            "Use ONLY the provided context to answer.\n",
            "Cite sources using the format [Source X].\n",
            "\n",
            "If the answer is not contained in the context, say you don't know.\n",
            "\n",
            "Question: How is the United States supporting Ukraine?\n",
            "\n",
            "Context:\n",
            "[Source 1: State of the Union \n"
          ]
        }
      ],
      "source": [
        "#Test prompt formatting\n",
        "messages = prompt.format_messages(\n",
        "    question=query,\n",
        "    context=context\n",
        ")\n",
        "\n",
        "for m in messages:\n",
        "    print(type(m).__name__)\n",
        "    print(m.content[:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Dz2w5MTbGH_",
        "outputId": "2c03d9ae-9903-44c9-cfd7-9442a36d4b3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The United States is supporting Ukraine by providing military assistance, economic assistance, and humanitarian assistance. Specifically, the U.S. is giving more than $1 billion in direct assistance to Ukraine. Additionally, the U.S., together with its allies, is enforcing powerful economic sanctions on Russia to inflict pain on the Russian economy and support the people of Ukraine [Source 1][Source 2].\n"
          ]
        }
      ],
      "source": [
        "#Test LLM Citation behaviour\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMlvtFCUbP6b",
        "outputId": "531c20c7-b5c9-4dbc-b1e0-bb905a89f95a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Answer:\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Test Full RAG\n",
        "result = rag_chain_with_citations(\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\",\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    k=5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "zA55NmV_bitk",
        "outputId": "ad09dac1-8644-47c4-c2c8-d8bcf7fe7638"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n-- Return JSON instead of free text\\n-- Include retrieval scores for each chunk\\n-- Optionally, highlight which sentences were used in the answer\\n-- Compatible with both FAISS and web fallback\\n'"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Here we add structured JSON output, ranking sources, and highlighting exact sentences.\n",
        "'''\n",
        "-- Return JSON instead of free text\n",
        "-- Include retrieval scores for each chunk\n",
        "-- Optionally, highlight which sentences were used in the answer\n",
        "-- Compatible with both FAISS and web fallback\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9aAqbOSbw7q"
      },
      "outputs": [],
      "source": [
        "def rag_chain_structured(\n",
        "    query,\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    output_parser,\n",
        "    k=5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Hybrid RAG pipeline with structured JSON output, source citations, and web fallback.\n",
        "\n",
        "    Args:\n",
        "        query (str): User question\n",
        "        vectorstore: FAISS or similar retriever\n",
        "        llm: AzureChatOpenAI LLM\n",
        "        prompt: ChatPromptTemplate\n",
        "        output_parser: StrOutputParser or similar\n",
        "        k (int): Number of top documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            \"answer\": str,\n",
        "            \"confidence\": \"high\"|\"medium\"|\"low\",\n",
        "            \"source_type\": \"vectorstore\"|\"web\"|\"none\",\n",
        "            \"sources\": list of dicts {\n",
        "                \"source\": str,\n",
        "                \"score\": float|None,\n",
        "                \"text\": str\n",
        "            }\n",
        "        }\n",
        "    \"\"\"\n",
        "    # --- Step 1: FAISS retrieval ---\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    results = vectorstore.similarity_search_with_score_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    docs = [doc for doc, score in results]\n",
        "    scores = [score for doc, score in results]\n",
        "\n",
        "    context = \"\\n\".join(f\"{doc.page_content}\" for doc in docs)\n",
        "\n",
        "    # --- Step 2: Confidence check ---\n",
        "    confident = len(docs) > 0 and min(scores) <= 0.6 and len(context) >= 300\n",
        "\n",
        "    if confident:\n",
        "        sources = [\n",
        "            {\n",
        "                \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
        "                \"score\": score,\n",
        "                \"text\": doc.page_content\n",
        "            }\n",
        "            for doc, score in results\n",
        "        ]\n",
        "        source_type = \"vectorstore\"\n",
        "        confidence = \"high\"\n",
        "    else:\n",
        "        # --- Step 3: Web fallback ---\n",
        "        web_results = web_search(query)\n",
        "        if web_results:\n",
        "            context = \"\\n\".join(r.get('content', '') for r in web_results)\n",
        "            sources = [\n",
        "                {\n",
        "                    \"source\": r.get(\"link\") or r.get(\"url\") or \"unknown\",\n",
        "                    \"score\": None,\n",
        "                    \"text\": r.get(\"content\", \"\")\n",
        "                }\n",
        "                for r in web_results\n",
        "            ]\n",
        "            source_type = \"web\"\n",
        "            confidence = \"medium\"\n",
        "        else:\n",
        "            # No info available\n",
        "            return {\n",
        "                \"answer\": \"I don't know based on available information.\",\n",
        "                \"confidence\": \"low\",\n",
        "                \"source_type\": \"none\",\n",
        "                \"sources\": [],\n",
        "            }\n",
        "\n",
        "    # --- Step 4: Prompt formatting and LLM call ---\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=context\n",
        "    )\n",
        "\n",
        "    llm_output = llm.invoke(messages)\n",
        "    answer = output_parser.parse(llm_output)\n",
        "\n",
        "    # --- Step 5: Return structured JSON ---\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"confidence\": confidence,\n",
        "        \"source_type\": source_type,\n",
        "        \"sources\": sources,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slOPP038b3eC",
        "outputId": "b31dbf25-95ac-4c67-c3c6-823d6d45fc2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Q: What are the key points from the State Of The Union?\n",
            "Answer: content=\"The key points from the State of the Union, ahead of President Joe Biden's third address, are that Americans are focused on the health of the economy and immigration [Source 1].\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 177, 'total_tokens': 215, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz6P40pQO9Yk37hGxqsya0E8viODZ', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd76-90ff-70d2-9ad5-11437d678d8f-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 177, 'output_tokens': 38, 'total_tokens': 215, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Confidence: medium\n",
            "Source Type: web\n",
            "Sources:\n",
            "- unknown (score: None )\n",
            "- unknown (score: None )\n",
            "- unknown (score: None )\n",
            "- unknown (score: None )\n",
            "\n",
            "==============================\n",
            "Q: How is the United States supporting Ukraine economically and militarily?\n",
            "Answer: content='The United States is supporting Ukraine both economically and militarily by providing more than $1 billion in direct assistance, which includes military, economic, and humanitarian aid. Additionally, the U.S. is working with allies to enforce powerful economic sanctions against Russia to further support Ukraine. However, American forces are not engaged in conflict within Ukraine; instead, U.S. military deployments are focused on defending NATO allies in Europe [Source X].' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 500, 'total_tokens': 586, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz6P5UY27ug5653wzv8eSK42rdVdQ', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd76-9424-72a2-a3c8-13b956081c5d-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 500, 'output_tokens': 86, 'total_tokens': 586, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Confidence: high\n",
            "Source Type: vectorstore\n",
            "Sources:\n",
            "- State of the Union Address (score: 0.5097524 )\n",
            "- State of the Union Address (score: 0.7064922 )\n",
            "- State of the Union Address (score: 0.7256527 )\n",
            "- State of the Union Address (score: 0.74181306 )\n",
            "- State of the Union Address (score: 0.79181457 )\n"
          ]
        }
      ],
      "source": [
        "#Finally test your production grade RAG chain\n",
        "questions = [\n",
        "    \"What are the key points from the State Of The Union?\",\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    result = rag_chain_structured(q, vectorstore, llm, prompt, output_parser)\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Q:\", q)\n",
        "    print(\"Answer:\", result[\"answer\"])\n",
        "    print(\"Confidence:\", result[\"confidence\"])\n",
        "    print(\"Source Type:\", result[\"source_type\"])\n",
        "    print(\"Sources:\")\n",
        "    for s in result[\"sources\"]:\n",
        "        print(\"-\", s[\"source\"], \"(score:\", s[\"score\"], \")\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAO5i5tWWt6C"
      },
      "source": [
        "# **Step 7: Load and configure a quantized language model**\n",
        "\n",
        "Load a quantized version of a large language model (Falcon3-1B-Base) for efficient and cost-effective text generation.\n",
        "\n",
        "**Generation Step**: This model is responsible for generating the final answer. It takes the prompt (which includes the retrieved context) and produces a response, completing the RAG pipeline.\n",
        "\n",
        "**Efficiency**: 4-bit quantization reduces resource usage while maintaining performance, crucial for deploying RAG systems in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "AW13wgYNwU6W",
        "outputId": "20a9248e-8ad0-48c9-b476-fb21403b4892"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nimport torch\\n\\nMODEL_NAME = \"tiiuae/Falcon3-1B-Base\"\\n\\n# Configure 4-bit quantization\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.float16,\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_quant_type=\"nf4\"\\n)\\n\\n# Load model\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    MODEL_NAME,\\n    quantization_config=bnb_config,\\n    device_map=\"auto\"\\n)\\n\\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\nmodel.eval()'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Using transformers\n",
        "'''\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"tiiuae/Falcon3-1B-Base\"\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model.eval()'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g62BmgR3uHa6"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        ")\n",
        "\n",
        "prompt2 = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(\n",
        "        content=\"You are a knowledgeable assistant. Answer based on the retrieved context.\"\n",
        "    ),\n",
        "    (\"human\", \"{context}\\nQuestion: {question}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "bO7R4qXcx4K-",
        "outputId": "7e246813-5406-419c-b75a-38275d13e0a8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'model.eval()\\ngeneration_config = model.generation_config\\n# Set temperature to 0 for deterministic responses\\ngeneration_config.temperature = 0.8\\n# Set number of returned sequences to 1\\ngeneration_config.num_return_sequences = 1\\n# Set maximum new tokens per response\\ngeneration_config.max_new_tokens = 256\\n# Disable token caching\\ngeneration_config.use_cache = False\\n# Set repetition penalty for more diverse responses\\ngeneration_config.repetition_penalty = 1.7\\n# Enable sampling for temperature to take effect\\ngeneration_config.do_sample = True\\n# Define pad and EOS token IDs\\ngeneration_config.pad_token_id = tokenizer.eos_token_id\\ngeneration_config.eos_token_id = tokenizer.eos_token_id'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#with model.eval()-->you are telling a locally loaded neural network to:\n",
        "'''\n",
        "Disable dropout\n",
        "Disable training-only layers\n",
        "Switch to inference mode\n",
        "\n",
        "This is necessary because you own the model weights and execution'''\n",
        "\n",
        "'''model.eval()\n",
        "generation_config = model.generation_config\n",
        "# Set temperature to 0 for deterministic responses\n",
        "generation_config.temperature = 0.8\n",
        "# Set number of returned sequences to 1\n",
        "generation_config.num_return_sequences = 1\n",
        "# Set maximum new tokens per response\n",
        "generation_config.max_new_tokens = 256\n",
        "# Disable token caching\n",
        "generation_config.use_cache = False\n",
        "# Set repetition penalty for more diverse responses\n",
        "generation_config.repetition_penalty = 1.7\n",
        "# Enable sampling for temperature to take effect\n",
        "generation_config.do_sample = True\n",
        "# Define pad and EOS token IDs\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlrwNgcxyEpt"
      },
      "outputs": [],
      "source": [
        "'''from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        "    pipeline,\n",
        ")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu6m0iRnXPCp"
      },
      "source": [
        "# **Step 8: Set up the generation pipeline and chain the components**\n",
        "\n",
        "Build an end-to-end pipeline that seamlessly connects document retrieval with text generation.\n",
        "\n",
        "**Integration**: The chain uses the retriever to fetch context, applies the prompt template to integrate the query with the retrieved context, and then passes the final prompt to the LLM for answer generation.\n",
        "\n",
        "**Pipeline composition**: Using the pipe operator (|), the components are elegantly chained together to perform a complete RAG operation in one go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7mblPjGx4N2",
        "outputId": "141dc17f-d06d-457a-bd45-7d89136cc9b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "'''from langchain.llms import HuggingFacePipeline # Import HuggingFacePipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Create the HuggingFacePipeline object\n",
        "llm_pipeline = HuggingFacePipeline(pipeline=pipe)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZaWVCCQvXi5"
      },
      "outputs": [],
      "source": [
        "'''rag_chain = (\n",
        "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm_pipeline\n",
        "    | output_parser\n",
        ")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR0AosXgXis0"
      },
      "source": [
        "# **Step 9: Invoke the pipeline with a query**\n",
        "\n",
        "Execute the entire RAG pipeline with a sample query.\n",
        "\n",
        "**Final output**: The pipeline retrieves relevant chunks from the document, forms a context-rich prompt, and the LLM generates a concise answer based on that context.\n",
        "\n",
        "**End-to-end flow**: This step demonstrates the full cycle of RAG—retrieval and augmented generation—in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ptSN7VvZfv",
        "outputId": "344536ff-8a37-4f69-a0d8-012441753db7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/voc/work/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "'''result = rag_chain.invoke(\"How is the United States supporting Ukraine economically and militarily?\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "rRvnTlYEzQHY",
        "outputId": "53129755-583b-42ae-f09d-b49c7a51ff8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In order to provide financial or military resources directly towards helping those affected by conflict situations such as war crimes investigations can also involve funding humanitarian organizations working within these areas which may include medical teams assisting victims during conflicts; this would fall under international law regarding human rights protection measures when dealing specifically about how funds should ideally go if there exists any form of violence occurring between different groups living together peacefully but still having disagreements over territory/resources etc., so it might make sense depending upon specific circumstances whether certain types of donations made via official channels provided through government agencies responsible overseeing security forces operating near borders where clashes often occur due lack thereof proper communication systems being established beforehand among both sides involved making sure everyone understands exactly why money needs going out instead simply throwing cash into pockets hoping someone else takes care off whatever problem arises later down road without actually knowing anything concrete behind situation causing strife initially leading up until point reached here today!'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''result'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5_Ht02VX-W5"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "This RAG (Retrieval-augmented generation) pipeline exemplifies how to combine retrieval-based methods with generative AI to produce informed, context-driven answers. By following these high-level steps—setting up the environment, loading and splitting the document, generating embeddings, building a FAISS vector store, and creating a retriever—you establish a robust foundation for pinpointing the most relevant pieces of information. Integrating a prompt template ensures that the language model is guided to leverage this retrieved context effectively. Finally, by employing a quantized language model in an end-to-end chain, the system efficiently generates concise and accurate responses. Overall, this approach not only enhances the model’s output by grounding it in factual context but also streamlines the process, making it scalable and adaptable to various domains and applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQlkpX4iYCwy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "2.9-RAG_Implementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 [3.10]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
