{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Implementation"
      ],
      "metadata": {
        "id": "PCXRSbESS77U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xBz-KrbtP73"
      },
      "source": [
        "# **Step1: Install and import the dependecies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH29BgsbRiMu"
      },
      "outputs": [],
      "source": [
        "#!pip install sentence-transformers bitsandbytes\n",
        "from langchain_text_splitters import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    TokenTextSplitter,SentenceTransformersTokenTextSplitter\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DQ4kvj-bQS7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dratzjpEtUEd"
      },
      "source": [
        "# **Step 2: Load the document**\n",
        "\n",
        "Load the document that will be used as the knowledge source.\n",
        "\n",
        "**Knowledge base**: The text document serves as the underlying knowledge base. Later, when a query is made, relevant parts of this document will be retrieved to augment the LLM's response.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9OBAJS8sc59",
        "outputId": "1cf0db58-8cda-4567-a646-478d7ab3cd26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': '/content/state_of_union.txt'}, page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. \\n\\nIn this struggle as President Zelenskyy said in his speech to the European Parliament Light will win over darkness. The Ukrainian Ambassador to the United States is here tonight. \\n\\nLet each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \\n\\nPlease rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. \\n\\nThroughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   \\n\\nThey keep moving.   \\n\\nAnd the costs and the threats to America and the world keep rising.   \\n\\nThat’s why the NATO Alliance was created to secure peace and stability in Europe after World War 2. \\n\\nThe United States is a member along with 29 other nations. \\n\\nIt matters. American diplomacy matters. American resolve matters. \\n\\nPutin’s latest attack on Ukraine was premeditated and unprovoked. \\n\\nHe rejected repeated efforts at diplomacy. \\n\\nHe thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \\n\\nWe prepared extensively and carefully. \\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \\n\\nWe countered Russia’s lies with truth.   \\n\\nAnd now that he has acted the free world is holding him accountable. \\n\\nAlong with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \\n\\nWe are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \\n\\nTogether with our allies –we are right now enforcing powerful economic sanctions. \\n\\nWe are cutting off Russia’s largest banks from the international financial system.  \\n\\nPreventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion war fund worthless.   \\n\\nWe are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.  \\n\\nTonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. \\n\\nThe U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.  \\n\\nWe are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. \\n\\nAnd tonight I am announcing that we will join our allies in closing off American air space to all Russian flights – further isolating Russia – and adding an additional squeeze –on their economy. The Ruble has lost 30% of its value. \\n\\nThe Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame. \\n\\nTogether with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. \\n\\nWe are giving more than $1 Billion in direct assistance to Ukraine. \\n\\nAnd we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.  \\n\\nLet me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine.  \\n\\nOur forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies – in the event that Putin decides to keep moving west.  \\n\\nFor that purpose we’ve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. \\n\\nAs I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power.  \\n\\nAnd we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them.  \\n\\nPutin has unleashed violence and chaos.  But while he may make gains on the battlefield – he will pay a continuing high price over the long run. \\n\\nAnd a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay. \\n\\nWhen the history of this era is written Putin’s war on Ukraine will have left Russia weaker and the rest of the world stronger. \\n\\nWhile it shouldn’t have taken something so terrible for people around the world to see what’s at stake now everyone sees it clearly. \\n\\nWe see the unity among leaders of nations and a more unified Europe a more unified West. And we see unity among the people who are gathering in cities in large crowds around the world even in Russia to demonstrate their support for Ukraine.  \\n\\nIn the battle between democracy and autocracy, democracies are rising to the moment, and the world is clearly choosing the side of peace and security. \\n\\nThis is a real test. It’s going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people. \\n\\nTo our fellow Ukrainian Americans who forge a deep bond that connects our two nations we stand with you. \\n\\nPutin may circle Kyiv with tanks, but he will never gain the hearts and souls of the Ukrainian people. \\n\\nHe will never extinguish their love of freedom. He will never weaken the resolve of the free world. \\n\\nWe meet tonight in an America that has lived through two of the hardest years this nation has ever faced. \\n\\nThe pandemic has been punishing. \\n\\nAnd so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. \\n\\nI understand. \\n\\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. \\n\\nThat’s why one of the first things I did as President was fight to pass the American Rescue Plan.  \\n\\nBecause people were hurting. We needed to act, and we did. \\n\\nFew pieces of legislation have done more in a critical moment in our history to lift us out of crisis. \\n\\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \\n\\nAnd as my Dad used to say, it gave people a little breathing room. \\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people—and left no one behind. \\n\\nAnd it worked. It created jobs. Lots of jobs. \\n\\nIn fact—our economy created over 6.5 Million new jobs just last year, more jobs created in one year  \\nthan ever before in the history of America. \\n\\nOur economy grew at a rate of 5.7% last year, the strongest growth in nearly 40 years, the first step in bringing fundamental change to an economy that hasn’t worked for the working people of this nation for too long.  \\n\\nFor the past 40 years we were told that if we gave tax breaks to those at the very top, the benefits would trickle down to everyone else. \\n\\nBut that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. \\n\\nVice President Harris and I ran for office with a new economic vision for America. \\n\\nInvest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up  \\nand the middle out, not from the top down.  \\n\\nBecause we know that when the middle class grows, the poor have a ladder up and the wealthy do very well. \\n\\nAmerica used to have the best roads, bridges, and airports on Earth. \\n\\nNow our infrastructure is ranked 13th in the world. \\n\\nWe won’t be able to compete for the jobs of the 21st Century if we don’t fix that. \\n\\nThat’s why it was so important to pass the Bipartisan Infrastructure Law—the most sweeping investment to rebuild America in history. \\n\\nThis was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen. \\n\\nWe’re done talking about infrastructure weeks. \\n\\nWe’re going to have an infrastructure decade. \\n\\nIt is going to transform America and put us on a path to win the economic competition of the 21st Century that we face with the rest of the world—particularly with China.  \\n\\nAs I’ve told Xi Jinping, it is never a good bet to bet against the American people. \\n\\nWe’ll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America. \\n\\nAnd we’ll do it all to withstand the devastating effects of the climate crisis and promote environmental justice. \\n\\nWe’ll build a national network of 500,000 electric vehicle charging stations, begin to replace poisonous lead pipes—so every child—and every American—has clean water to drink at home and at school, provide affordable high-speed internet for every American—urban, suburban, rural, and tribal communities. \\n\\n4,000 projects have already been announced. \\n\\nAnd tonight, I’m announcing that this year we will start fixing over 65,000 miles of highway and 1,500 bridges in disrepair. \\n\\nWhen we use taxpayer dollars to rebuild America – we are going to Buy American: buy American products to support American jobs. \\n\\nThe federal government spends about $600 Billion a year to keep the country safe and secure. \\n\\nThere’s been a law on the books for almost a century \\nto make sure taxpayers’ dollars support American jobs and businesses. \\n\\nEvery Administration says they’ll do it, but we are actually doing it. \\n\\nWe will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America. \\n\\nBut to compete for the best jobs of the future, we also need to level the playing field with China and other competitors. \\n\\nThat’s why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing. \\n\\nLet me give you one example of why it’s so important to pass it. \\n\\nIf you travel 20 miles east of Columbus, Ohio, you’ll find 1,000 empty acres of land. \\n\\nIt won’t look like much, but if you stop and look closely, you’ll see a Field of dreams, the ground on which America’s future will be built. \\n\\nThis is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor mega site. \\n\\nUp to eight state-of-the-art factories in one place. 10,000 new good-paying jobs. \\n\\nSome of the most sophisticated manufacturing in the world to make computer chips the size of a fingertip that power the world and our everyday lives. \\n\\nSmartphones. The Internet. Technology we have yet to invent. \\n\\nBut that’s just the beginning. \\n\\nIntel’s CEO, Pat Gelsinger, who is here tonight, told me they are ready to increase their investment from  \\n$20 billion to $100 billion. \\n\\nThat would be one of the biggest investments in manufacturing in American history. \\n\\nAnd all they’re waiting for is for you to pass this bill. \\n\\nSo let’s not wait any longer. Send it to my desk. I’ll sign it.  \\n\\nAnd we will really take off. \\n\\nAnd Intel is not alone. \\n\\nThere’s something happening in America. \\n\\nJust look around and you’ll see an amazing story. \\n\\nThe rebirth of the pride that comes from stamping products Made In America. The revitalization of American manufacturing.   \\n\\nCompanies are choosing to build new factories here, when just a few years ago, they would have built them overseas. \\n\\nThat’s what is happening. Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. \\n\\nGM is making the largest investment in its history—$7 billion to build electric vehicles, creating 4,000 jobs in Michigan. \\n\\nAll told, we created 369,000 new manufacturing jobs in America just last year. \\n\\nPowered by people I’ve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who’s here with us tonight. \\n\\nAs Ohio Senator Sherrod Brown says, It’s time to bury the label Rust Belt.\\n\\nIt’s time. \\n\\nBut with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.  \\n\\nInflation is robbing them of the gains they might otherwise feel. \\n\\nI get it. That’s why my top priority is getting prices under control. \\n\\nLook, our economy roared back faster than most predicted, but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories. \\n\\nThe pandemic also disrupted global supply chains. \\n\\nWhen factories close, it takes longer to make goods and get them from the warehouse to the store, and prices go up. \\n\\nLook at cars. \\n\\nLast year, there weren’t enough semiconductors to make all the cars that people wanted to buy. \\n\\nAnd guess what, prices of automobiles went up. \\n\\nSo—we have a choice. \\n\\nOne way to fight inflation is to drive down wages and make Americans poorer.  \\n\\nI have a better plan to fight inflation. \\n\\nLower your costs, not your wages. \\n\\nMake more cars and semiconductors in America. \\n\\nMore infrastructure and innovation in America. \\n\\nMore goods moving faster and cheaper in America. \\n\\nMore jobs where you can earn a good living in America. \\n\\nAnd instead of relying on foreign supply chains, let’s make it in America. \\n\\nEconomists call it increasing the productive capacity of our economy.\\n\\nI call it building a better America. \\n\\nMy plan to fight inflation will lower your costs and lower the deficit. \\n\\n17 Nobel laureates in economics say my plan will ease long-term inflationary pressures. Top business leaders and most Americans support my plan. And here’s the plan: \\n\\nFirst – cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.  \\n\\nHe and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.  \\n\\nBut drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshua’s mom. \\n\\nImagine what it’s like to look at your child who needs insulin and have no idea how you’re going to pay for it.  \\n\\nWhat it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be. \\n\\nJoshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.  \\n\\nFor Joshua, and for the 200,000 other young people with Type 1 diabetes, let’s cap the cost of insulin at $35 a month so everyone can afford it.  \\n\\nDrug companies will still do very well. And while we’re at it let Medicare negotiate lower prices for prescription drugs, like the VA already does. \\n\\nLook, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let’s close the coverage gap and make those savings permanent. \\n\\nSecond – cut energy costs for families an average of $500 a year by combatting climate change.  \\n\\nLet’s provide investments and tax credits to weatherize your homes and businesses to be energy efficient and you get a tax credit; double America’s clean energy production in solar, wind, and so much more;  lower the price of electric vehicles, saving you another $80 a month because you’ll never have to pay at the gas pump again. \\n\\nThird – cut the cost of child care. Many families pay up to $14,000 a year for child care per child.  \\n\\nMiddle-class and working families shouldn’t have to pay more than 7% of their income for care of young children.  \\n\\nMy plan will cut the cost in half for most families and help parents, including millions of women, who left the workforce during the pandemic because they couldn’t afford child care, to be able to get back to work. \\n\\nMy plan doesn’t stop there. It also includes home and long-term care. More affordable housing. And Pre-K for every 3- and 4-year-old.  \\n\\nAll of these will lower costs. \\n\\nAnd under my plan, nobody earning less than $400,000 a year will pay an additional penny in new taxes. Nobody.  \\n\\nThe one thing all Americans agree on is that the tax system is not fair. We have to fix it.  \\n\\nI’m not looking to punish anyone. But let’s make sure corporations and the wealthiest Americans start paying their fair share. \\n\\nJust last year, 55 Fortune 500 corporations earned $40 billion in profits and paid zero dollars in federal income tax.  \\n\\nThat’s simply not fair. That’s why I’ve proposed a 15% minimum tax rate for corporations. \\n\\nWe got more than 130 countries to agree on a global minimum tax rate so companies can’t get out of paying their taxes at home by shipping jobs and factories overseas. \\n\\nThat’s why I’ve proposed closing loopholes so the very wealthy don’t pay a lower tax rate than a teacher or a firefighter.  \\n\\nSo that’s my plan. It will grow the economy and lower costs for families. \\n\\nSo what are we waiting for? Let’s get this done. And while you’re at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.  \\n\\nMy plan will not only lower costs to give families a fair shot, it will lower the deficit. \\n\\nThe previous Administration not only ballooned the deficit with tax cuts for the very wealthy and corporations, it undermined the watchdogs whose job was to keep pandemic relief funds from being wasted. \\n\\nBut in my administration, the watchdogs have been welcomed back. \\n\\nWe’re going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.  \\n\\nAnd tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \\n\\nBy the end of this year, the deficit will be down to less than half what it was before I took office.  \\n\\nThe only president ever to cut the deficit by more than one trillion dollars in a single year. \\n\\nLowering your costs also means demanding more competition. \\n\\nI’m a capitalist, but capitalism without competition isn’t capitalism. \\n\\nIt’s exploitation—and it drives up prices. \\n\\nWhen corporations don’t have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under. \\n\\nWe see it happening with ocean carriers moving goods in and out of America. \\n\\nDuring the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits. \\n\\nTonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\n\\nThat ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\n\\nWe’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\n\\nLet’s pass the Paycheck Fairness Act and paid leave.  \\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\n\\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges. \\n\\nAnd let’s pass the PRO Act when a majority of workers want to form a union—they shouldn’t be stopped.  \\n\\nWhen we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven’t done in a long time: build a better America. \\n\\nFor more than two years, COVID-19 has impacted every decision in our lives and the life of the nation. \\n\\nAnd I know you’re tired, frustrated, and exhausted. \\n\\nBut I also know this. \\n\\nBecause of the progress we’ve made, because of your resilience and the tools we have, tonight I can say  \\nwe are moving forward safely, back to more normal routines.  \\n\\nWe’ve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \\n\\nJust a few days ago, the Centers for Disease Control and Prevention—the CDC—issued new mask guidelines. \\n\\nUnder these new guidelines, most Americans in most of the country can now be mask free.   \\n\\nAnd based on the projections, more of the country will reach that point across the next couple of weeks. \\n\\nThanks to the progress we have made this past year, COVID-19 need no longer control our lives.  \\n\\nI know some are talking about living with COVID-19. Tonight – I say that we will never just accept living with COVID-19. \\n\\n')]\n"
          ]
        }
      ],
      "source": [
        "text_loader = TextLoader(\"/content/state_of_union.txt\")\n",
        "text_document = text_loader.load()\n",
        "print(text_document[:100])  # Prints the first 100 characters of the text document\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajrBtwcHtYMh"
      },
      "source": [
        "# **Step 3: Split the document into chunks**\n",
        "\n",
        "Break down the large document into manageable pieces.\n",
        "\n",
        "**Fine-Grained Retrieval**: Smaller chunks allow the retriever to more precisely locate the context relevant to the query, enhancing the generation step with focused context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX10Ld0bsxgM",
        "outputId": "9fa7431b-d7d4-4cf4-dd5b-6e1ee4c988f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53\n"
          ]
        }
      ],
      "source": [
        "doc_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
        "split_texts = doc_splitter.split_documents(text_document)\n",
        "print(len(split_texts))  # Prints the number of chunks the PDF has been split into\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOMVE7TKto9O"
      },
      "source": [
        "# **Step 4: Generate embeddings for each chunk**\n",
        "\n",
        "Convert text chunks into numerical vectors (embeddings) that capture semantic meaning.\n",
        "\n",
        "**Semantic Search**: Embeddings allow the FAISS vector store to perform similarity searches, ensuring that the most relevant context is retrieved for any given query.\n",
        "\n",
        "**Verification**: Printing the length of the embedding vector confirms the transformation was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2UqIhbcsnXX",
        "outputId": "aad64979-97c3-410d-ab2a-243a28d26686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3099908222.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  hf_embed = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n"
          ]
        }
      ],
      "source": [
        "#model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "#embeddings = HuggingFaceEmbeddings(model_name=model)\n",
        "\n",
        "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "hf_embed = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
        "text = split_texts[0].page_content\n",
        "hf_embed_result = hf_embed.embed_documents([text])\n",
        "print(len(hf_embed_result[0]))  # Prints the length of the first embedded document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8aO836YuRbU"
      },
      "source": [
        "#### If we quickly want to see how the embeddings for the chunks will look like we will do the below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPcS4HDBs-EU"
      },
      "outputs": [],
      "source": [
        "embedded_chunks = [hf_embed.embed_query(chunk.page_content) for chunk in split_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op5I6W5CtvTV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_chunks = pd.DataFrame(embedded_chunks)\n",
        "df_chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IgwrSOQu5kk"
      },
      "source": [
        "# **Step 5: Build the FAISS vector store and create a retriever**\n",
        "\n",
        "Build an index (FAISS) for the document embeddings and create a retriever.\n",
        "\n",
        "**Retrieval step**: The retriever is responsible for fetching the most relevant chunks from the document based on the query. These retrieved contexts will later be fed into the generation step to produce an informed answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6pgEZzuucIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7f6597-0003-42df-b954-6fc08d5a958b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "vectorstore=FAISS.from_documents(split_texts, hf_embed)\n",
        "\n",
        "# It will take the same embedding of the chunks as shown above and and create a vecor database for it which will be temporary, ie non persistent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhADqsOIu-U-"
      },
      "source": [
        "#### Let's see if the retriever works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8ALpBocuqS2"
      },
      "outputs": [],
      "source": [
        "retriever=vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(retriever))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Qn0wyAd-r5",
        "outputId": "74799a6a-dfb6-4dc1-ed36-9b196142175d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "84QbBaUWvBCh",
        "outputId": "5ac52eff-715f-4290-f4aa-dca769874b62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'query = \"What are the key points from the State Of The Union\"\\n# Directly call the retriever as a function\\ndocs = retriever(query)  # NOTE: retriever is callable'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# The way the retriever works\n",
        "'''query = \"What are the key points from the State Of The Union\"\n",
        "# Directly call the retriever as a function\n",
        "docs = retriever(query)  # NOTE: retriever is callable'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instead of using retriever, since we are using newer versions\n",
        "query_embedding = hf_embed.embed_query(\"What are the key points from the State Of The Union\")\n",
        "similar_docs = vectorstore.similarity_search_by_vector(query_embedding, k=5)  # top 5 results"
      ],
      "metadata": {
        "id": "kjyc-finkILq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in similar_docs:\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVfo7-kikPz7",
        "outputId": "7a6067e0-eae4-49a1-d8d8-35638eb2ec5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges. \n",
            "\n",
            "And let’s pass the PRO Act when a majority of workers want to form a union—they shouldn’t be stopped.  \n",
            "\n",
            "When we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven’t done in a long time: build a better America.\n",
            "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
            "\n",
            "Last year COVID-19 kept us apart. This year we are finally together again. \n",
            "\n",
            "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
            "\n",
            "With a duty to one another to the American people to the Constitution. \n",
            "\n",
            "And with an unwavering resolve that freedom will always triumph over tyranny.\n",
            "But in my administration, the watchdogs have been welcomed back. \n",
            "\n",
            "We’re going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.  \n",
            "\n",
            "And tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \n",
            "\n",
            "By the end of this year, the deficit will be down to less than half what it was before I took office.  \n",
            "\n",
            "The only president ever to cut the deficit by more than one trillion dollars in a single year.\n",
            "We’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \n",
            "\n",
            "Let’s pass the Paycheck Fairness Act and paid leave.  \n",
            "\n",
            "Raise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty.\n",
            "It fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \n",
            "\n",
            "Helped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \n",
            "\n",
            "And as my Dad used to say, it gave people a little breathing room. \n",
            "\n",
            "And unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people—and left no one behind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGNVIRo3vC-0"
      },
      "outputs": [],
      "source": [
        "query2 = \"How is the United States supporting Ukraine economically and militarily?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjdBwjRAvFZ3",
        "outputId": "566dfd13-03bf-4c74-8e65-244a85224f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame. \n",
            "\n",
            "Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. \n",
            "\n",
            "We are giving more than $1 Billion in direct assistance to Ukraine. \n",
            "\n",
            "And we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.\n",
            "Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \n",
            "\n",
            "We are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \n",
            "\n",
            "Together with our allies –we are right now enforcing powerful economic sanctions.\n",
            "The United States is a member along with 29 other nations. \n",
            "\n",
            "It matters. American diplomacy matters. American resolve matters. \n",
            "\n",
            "Putin’s latest attack on Ukraine was premeditated and unprovoked. \n",
            "\n",
            "He rejected repeated efforts at diplomacy. \n",
            "\n",
            "He thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \n",
            "\n",
            "We prepared extensively and carefully.\n",
            "And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n",
            "\n",
            "To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world.\n",
            "Let me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine.  \n",
            "\n",
            "Our forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies – in the event that Putin decides to keep moving west.  \n",
            "\n",
            "For that purpose we’ve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia.\n"
          ]
        }
      ],
      "source": [
        "'''Query is embedded, FAISS retrieves the top-k most relevant document chunks\n",
        "   These are grounding context, not model memory '''\n",
        "query_embedding = hf_embed.embed_query(query2)\n",
        "similar_docs = vectorstore.similarity_search_by_vector(query_embedding, k=5)  # top 5 results\n",
        "for doc in similar_docs:\n",
        "    print(doc.page_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/content/.env\")\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"API_KEY\"),\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    deployment_name=\"gpt-4.1\",\n",
        "    temperature=0,\n",
        ")"
      ],
      "metadata": {
        "id": "9sZogM2J6d-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Context construction\n",
        "context = \"\\n\".join([doc.page_content for doc in similar_docs])\n",
        "\n",
        "#To limit tokens for large contexts\n",
        "#context = \"\\n\".join([doc.page_content[:1000] for doc in similar_docs])\n",
        "answer = llm.invoke(f\"Based on the following context, answer the question:\\n\\n{context}\\n\\nQuestion:\\nWhat are the key points from the State Of The Union?\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "qPTj3ueqkqbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecdb3019-cd07-4363-cba8-5eeb73fc066e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='**Key Points from the State of the Union (based on the provided context):**\\n\\n1. **Russian Stock Market and Economy:**  \\n   - The Russian stock market has lost 40% of its value and trading is suspended.\\n   - Russia’s economy is suffering, and President Putin is solely blamed for this situation.\\n\\n2. **Support for Ukraine:**  \\n   - The U.S. and its allies are providing military, economic, and humanitarian assistance to Ukraine.\\n   - Over $1 billion in direct assistance is being given to Ukraine.\\n   - Continued commitment to aid the Ukrainian people as they defend their country and to help ease their suffering.\\n\\n3. **International Coalition:**  \\n   - Support for Ukraine is broad, including all 27 EU members (France, Germany, Italy), the UK, Canada, Japan, Korea, Australia, New Zealand, Switzerland, and others.\\n   - The U.S. is working with 29 other nations to enforce powerful economic sanctions on Russia.\\n\\n4. **Sanctions and Isolation of Russia:**  \\n   - The U.S. and allies are inflicting economic pain on Russia and isolating Putin from the world.\\n   - American diplomacy and resolve are emphasized as critical in this effort.\\n\\n5. **Putin’s Actions and Western Response:**  \\n   - Putin’s attack on Ukraine was premeditated and unprovoked.\\n   - He rejected diplomatic efforts and underestimated the unity and preparedness of the West and NATO.\\n   - The West was ready and responded with coordinated actions.\\n\\n6. **Ukrainian Resistance:**  \\n   - The Ukrainian people, with 30 years of independence, are determined not to let anyone take their country backwards.\\n\\n7. **Impact on Americans and Global Costs:**  \\n   - The President is honest about the global costs of Russia’s invasion, acknowledging its impact around the world.\\n\\n8. **U.S. Military Position:**  \\n   - U.S. forces are not engaged in Ukraine and will not fight Russian forces there.\\n   - U.S. deployments to Europe are to defend NATO allies (Poland, Romania, Latvia, Lithuania, Estonia) in case of further Russian aggression.\\n\\n**Summary:**  \\nThe State of the Union highlights strong U.S. and allied support for Ukraine, severe sanctions against Russia, the isolation of Putin, and the unity and preparedness of NATO. It reassures Americans that U.S. forces will not fight in Ukraine but are deployed to defend NATO allies, and it acknowledges the global impact of the conflict.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 502, 'prompt_tokens': 459, 'total_tokens': 961, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz4oBxtgoAl5nkP1TUA3bN4nmS22w', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd19-01e4-79d1-a77f-beb401e387d9-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 459, 'output_tokens': 502, 'total_tokens': 961, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcUoui50vN3A"
      },
      "source": [
        "# **Step 6: Design a prompt template for the language model**\n",
        "Establish a prompt that instructs the LLM on how to utilize the retrieved context to generate a concise answer.\n",
        "\n",
        "**Guiding Generation**: The prompt template bridges retrieval and generation by ensuring the LLM uses the provided context (from the retriever) to answer the query accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owZKIB3NvGga"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2neeknSivQyL"
      },
      "outputs": [],
      "source": [
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use one sentence and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-P-b5SqvSb0"
      },
      "outputs": [],
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-RJ6awdvV47"
      },
      "outputs": [],
      "source": [
        "output_parser=StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_chain(query, vectorstore, llm, prompt, output_parser, k=5):\n",
        "    # Step 1: Retrieve relevant documents\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    similar_docs = vectorstore.similarity_search_by_vector(query_embedding, k=k)\n",
        "\n",
        "    # Step 2: Build context string\n",
        "    context = \"\\n\".join([doc.page_content for doc in similar_docs])\n",
        "\n",
        "    # Step 3: Format the prompt\n",
        "    '''Context + question are injected into the prompt\n",
        "       We are using chat messages, which is correct for GPT\n",
        "       System + human message roles are respected '''\n",
        "    messages = prompt.format_messages(\n",
        "    question=query,\n",
        "    context=context\n",
        "    )\n",
        "\n",
        "    # Step 4: Call LLM\n",
        "    '''GPT-4.1 reasons only over the provided context\n",
        "       The model is not searching FAISS or the web\n",
        "       No fine-tuning is happening — this is pure inference '''\n",
        "    llm_output = llm.invoke(messages)\n",
        "\n",
        "    # Step 5: Parse output\n",
        "    answer = output_parser.parse(llm_output)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "N7iSHABuz1zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What are the key points from the State Of The Union?\",\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    answer = rag_chain(q, vectorstore, llm, prompt, output_parser, k=5)\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTcQvh0Mz6he",
        "outputId": "a2302b9b-e530-47a4-ed3d-d56edc3fb141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What are the key points from the State Of The Union?\n",
            "A: content='Key points from the State of the Union include unity across parties, ongoing COVID-19 recovery, prosecuting pandemic fraud, significant deficit reduction, support for education and workers, raising the minimum wage, extending the Child Tax Credit, and focusing economic relief on working Americans.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 555, 'total_tokens': 609, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5Ftz6h1x8Yh5zNnvZX1lKWyXfi5', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd33-3bdc-75e1-8c7a-ff56a8835008-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 555, 'output_tokens': 54, 'total_tokens': 609, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Q: How is the United States supporting Ukraine economically and militarily?\n",
            "A: content='The United States is supporting Ukraine by providing over $1 billion in direct economic assistance, military aid, and humanitarian support, while also enforcing powerful economic sanctions against Russia.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 498, 'total_tokens': 532, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5Fu1tnl4OXU07sIqmyZkxaJ651R', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd33-3f34-7bc0-811b-0ef04d51df40-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 498, 'output_tokens': 34, 'total_tokens': 532, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Update the RAG function to return sources\n",
        "def rag_chain_with_sources(query, vectorstore, llm, prompt, output_parser, k=5):\n",
        "    # 1. Retrieve documents\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    similar_docs = vectorstore.similarity_search_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    # 2. Build context\n",
        "    context = \"\\n\".join(doc.page_content for doc in similar_docs)\n",
        "\n",
        "    # 3. Format prompt into messages\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=context\n",
        "    )\n",
        "\n",
        "    # 4. Invoke LLM\n",
        "    llm_output = llm.invoke(messages)\n",
        "    answer = output_parser.parse(llm_output)\n",
        "\n",
        "    # 5. Collect sources\n",
        "    sources = []\n",
        "    for doc in similar_docs:\n",
        "        source = doc.metadata.get(\"source\", \"unknown\")\n",
        "        sources.append(source)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    sources = list(dict.fromkeys(sources))\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"sources\": sources,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Yx15YT1GNNA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_chain_with_sources(\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\",\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    output_parser,\n",
        ")\n",
        "\n",
        "print(\"Answer:\")\n",
        "print(result[\"answer\"])\n",
        "\n",
        "print(\"\\nSources:\")\n",
        "for src in result[\"sources\"]:\n",
        "    print(\"-\", src)\n",
        "'''\n",
        "Now we have:\n",
        "Grounded answers\n",
        "Transparent citations\n",
        "Auditable RAG\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "6SH97EI_NSb9",
        "outputId": "3733d4a6-31b8-4248-dc21-71249f29079c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "content='The United States is supporting Ukraine by providing over $1 billion in direct economic assistance, military aid, and humanitarian support, while also enforcing powerful economic sanctions against Russia.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 498, 'total_tokens': 532, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5NhvZoX16d4iMAigH2oN4Y1FLIm', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd3a-9db6-7d53-b105-07470b3d3693-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 498, 'output_tokens': 34, 'total_tokens': 532, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Sources:\n",
            "- /content/state_of_union.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNow we have:\\nGrounded answers\\nTransparent citations\\nAuditable RAG\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Goal\n",
        "'''\n",
        "Our system should:\n",
        "Answer only when the retrieved context is strong\n",
        "Say “I don’t know based on the provided context” when it isn’t\n",
        "This is true RAG safety'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "KIuOcf-yNYlZ",
        "outputId": "d2f1341a-d949-4607-9301-6bd618e45fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOur system should:\\n\\nAnswer only when the retrieved context is strong\\nSay “I don’t know based on the provided context” when it isn’t\\n\\nThis is true RAG safety'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We’ll use retrieval signal, not LLM guessing:\n",
        "'''We can use Heuristics  (version-safe):\n",
        "    Number of retrieved chunks\n",
        "    Minimum similarity score\n",
        "    Context length'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "axOOtJnIPi4I",
        "outputId": "c08c59d9-e659-4691-8807-073341ef634e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We can use Heuristics  (version-safe):\\n    Number of retrieved chunks\\n    Minimum similarity score\\n    Context length'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define confidence thresholds\n",
        "MAX_SCORE_THRESHOLD = 0.6      # tune this\n",
        "MIN_CONTEXT_LENGTH = 300       # characters"
      ],
      "metadata": {
        "id": "68ZDPHWTQuvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_chain_with_confidence(query, vectorstore, llm, prompt, output_parser, k=5):\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "\n",
        "    results = vectorstore.similarity_search_with_score_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    # Separate docs and scores\n",
        "    docs = [doc for doc, score in results]\n",
        "    scores = [score for doc, score in results]\n",
        "\n",
        "    # Build context\n",
        "    context = \"\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # --- Confidence checks ---\n",
        "    is_confident = True\n",
        "\n",
        "    if len(docs) == 0:\n",
        "        is_confident = False\n",
        "\n",
        "    if min(scores) > MAX_SCORE_THRESHOLD:\n",
        "        is_confident = False\n",
        "\n",
        "    if len(context) < MIN_CONTEXT_LENGTH:\n",
        "        is_confident = False\n",
        "\n",
        "    # If confidence is low → safe answer\n",
        "    if not is_confident:\n",
        "        return {\n",
        "            \"answer\": \"I don't know based on the provided context.\",\n",
        "            \"confidence\": \"low\",\n",
        "            \"sources\": [],\n",
        "        }\n",
        "\n",
        "    # Format prompt\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=context\n",
        "    )\n",
        "\n",
        "    # LLM call\n",
        "    llm_output = llm.invoke(messages)\n",
        "    answer = output_parser.parse(llm_output)\n",
        "\n",
        "    # Sources\n",
        "    sources = list(dict.fromkeys(\n",
        "        doc.metadata.get(\"source\", \"unknown\") for doc in docs\n",
        "    ))\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"confidence\": \"high\",\n",
        "        \"sources\": sources,\n",
        "    }"
      ],
      "metadata": {
        "id": "6ofscj04RAo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\",\n",
        "    \"What is the GDP of Atlantis in 2024?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    result = rag_chain_with_confidence(\n",
        "        q, vectorstore, llm, prompt, output_parser\n",
        "    )\n",
        "\n",
        "    print(\"\\nQ:\", q)\n",
        "    print(\"Answer:\", result[\"answer\"])\n",
        "    print(\"Confidence:\", result[\"confidence\"])\n",
        "    print(\"Sources:\", result[\"sources\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbbFeHArRShj",
        "outputId": "72461468-0f47-442e-a592-5cf1bf1a0e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: How is the United States supporting Ukraine economically and militarily?\n",
            "Answer: content='The United States is supporting Ukraine by providing over $1 billion in direct economic assistance, military aid, and humanitarian support, while also enforcing powerful economic sanctions against Russia.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 498, 'total_tokens': 532, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5Z6wrZe16HDbPcwBkQ0OkTHPhDS', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd45-6671-7080-bba3-e3d334c782d5-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 498, 'output_tokens': 34, 'total_tokens': 532, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Confidence: high\n",
            "Sources: ['/content/state_of_union.txt']\n",
            "\n",
            "Q: What is the GDP of Atlantis in 2024?\n",
            "Answer: I don't know based on the provided context.\n",
            "Confidence: low\n",
            "Sources: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Without confidence rating:**\n",
        "\n",
        "* LLMs answer even when retrieval is weak\n",
        "* This causes hallucinations\n",
        "\n",
        "  With confidence gating:\n",
        "* Your system behaves honestly\n",
        "* Users trust it\n"
      ],
      "metadata": {
        "id": "VfTetClmeQri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Without confidence rating:**\n",
        "\n",
        "  * LLMs answer even when retrieval is weak\n",
        "  * This causes hallucinations\n",
        "\n",
        "    With confidence gating:\n",
        "  * Your system behaves honestly\n",
        "  * Users trust it\n"
      ],
      "metadata": {
        "id": "lfCr31tKe111"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web search fallback\n",
        "  When confidence is low, we’ll:\n",
        "  * Automatically call web search\n",
        "  * Re-answer using fresh data\n",
        "  \n",
        "  If FAISS retrieval confidence is low, your system should:\n",
        "  Perform a web search\n",
        "  * Build new context from live data\n",
        "  * Ask GPT-4.1 again\n",
        "  * Clearly label the answer as coming from the web"
      ],
      "metadata": {
        "id": "OEq6bTWHfNWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#User question -->FAISS Retrieval -->Confidence Check\n",
        "#If High Confidence -->Answer from FAISS\n",
        "#If Low Confidence -->Web Search -->GPT --> Answer"
      ],
      "metadata": {
        "id": "SD9Y4_-gSj8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/content/.env\")\n",
        "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\")\n",
        "\n",
        "def web_search(query, num_results=5):\n",
        "    url = \"https://serpapi.com/search.json\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERPAPI_KEY,\n",
        "        \"num\": num_results,\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    results = []\n",
        "    for r in data.get(\"organic_results\", []):\n",
        "        snippet = r.get(\"snippet\")\n",
        "        source = r.get(\"link\")\n",
        "        if snippet:\n",
        "            results.append({\n",
        "                \"content\": snippet,\n",
        "                \"source\": source,\n",
        "            })\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "udHfTehaUEB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build web context\n",
        "def build_web_context(results):\n",
        "    context = []\n",
        "    sources = []\n",
        "\n",
        "    for r in results:\n",
        "        context.append(r[\"content\"])\n",
        "        sources.append(r[\"source\"])\n",
        "\n",
        "    return \"\\n\".join(context), list(dict.fromkeys(sources))\n"
      ],
      "metadata": {
        "id": "KXO-XZCZUL6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_chain_structured(\n",
        "    query,\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    output_parser,\n",
        "    k=5,\n",
        "):\n",
        "    # --- FAISS retrieval ---\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    results = vectorstore.similarity_search_with_score_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    docs = [doc for doc, score in results]\n",
        "    scores = [score for doc, score in results]\n",
        "\n",
        "    context = \"\\n\".join(f\"{doc.page_content}\" for doc in docs)\n",
        "\n",
        "    # Confidence check\n",
        "    confident = len(docs) > 0 and min(scores) <= 0.6 and len(context) >= 300\n",
        "\n",
        "    if not confident:\n",
        "        # Optional: fallback to web search\n",
        "        web_results = web_search(query)\n",
        "        if web_results:\n",
        "            context = \"\\n\".join(r['content'] for r in web_results)\n",
        "            sources = [{\n",
        "                \"source\": r['source'],  # Changed from r['url'] to r['source']\n",
        "                \"score\": None,\n",
        "                \"text\": r['content']\n",
        "            } for r in web_results]\n",
        "            source_type = \"web\"\n",
        "            confidence = \"medium\"\n",
        "        else:\n",
        "            return {\n",
        "                \"answer\": \"I don't know based on available information.\",\n",
        "                \"confidence\": \"low\",\n",
        "                \"source_type\": \"none\",\n",
        "                \"sources\": [],\n",
        "            }\n",
        "    else:\n",
        "        sources = [\n",
        "            {\n",
        "                \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
        "                \"score\": score,\n",
        "                \"text\": doc.page_content\n",
        "            }\n",
        "            for doc, score in results\n",
        "        ]\n",
        "        source_type = \"vectorstore\"\n",
        "        confidence = \"high\"\n",
        "\n",
        "    # --- Prompt + LLM ---\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=context\n",
        "    )\n",
        "\n",
        "    llm_output = llm.invoke(messages)\n",
        "    answer = output_parser.parse(llm_output)\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"confidence\": confidence,\n",
        "        \"source_type\": source_type,\n",
        "        \"sources\": sources,\n",
        "    }"
      ],
      "metadata": {
        "id": "-UYHLatcUPCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\",\n",
        "    \"What happened in the latest Apple WWDC?\"]\n",
        "\n",
        "for q in questions:\n",
        "    result = rag_chain_with_web_fallback(\n",
        "        q, vectorstore, llm, prompt, output_parser\n",
        "    )\n",
        "\n",
        "    print(\"\\nQ:\", q)\n",
        "    print(\"Answer:\", result[\"answer\"])\n",
        "    print(\"Confidence:\", result[\"confidence\"])\n",
        "    print(\"Source Type:\", result[\"source_type\"])\n",
        "    print(\"Sources:\")\n",
        "    for s in result[\"sources\"]:\n",
        "        print(\"-\", s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwmpTKExUWtg",
        "outputId": "8862d56a-5022-4f01-80c6-c664132a1dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: How is the United States supporting Ukraine economically and militarily?\n",
            "Answer: content='The United States is supporting Ukraine by providing over $1 billion in direct economic assistance, military aid, and humanitarian support, while also enforcing powerful economic sanctions against Russia.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 498, 'total_tokens': 532, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5rZdZN8RxRfsngUPiniD7gnWxjs', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd56-de21-75f1-ab1e-c30d2757fdeb-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 498, 'output_tokens': 34, 'total_tokens': 532, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Confidence: high\n",
            "Source Type: vectorstore\n",
            "Sources:\n",
            "- /content/state_of_union.txt\n",
            "\n",
            "Q: What happened in the latest Apple WWDC?\n",
            "Answer: content='At the latest Apple WWDC, Apple announced its broadest design update ever, introduced more advanced Apple Intelligence features, and unveiled exciting new capabilities across all its platforms.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 186, 'total_tokens': 220, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz5rebPw78ytZntbPKrJA8m9G2sUP', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd56-f2f2-78d2-a550-1247262a1fc3-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 186, 'output_tokens': 34, 'total_tokens': 220, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Confidence: medium\n",
            "Source Type: web\n",
            "Sources:\n",
            "- https://developer.apple.com/wwdc25/\n",
            "- https://www.apple.com/apple-events/\n",
            "- https://www.wired.com/live/apple-wwdc-2025-live-blog/\n",
            "- https://www.reddit.com/r/iOSProgramming/comments/1l3jpce/what_was_the_biggest_thing_dropped_on_a_wwdc/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming responses (UX+ production readiness)\n",
        "   Stream GPT tokens live\n",
        "   * token streaming so answers appear progressively  instead of waiting for the full response.\n",
        "   * Show partial answers while generating\n"
      ],
      "metadata": {
        "id": "ibBd8sLPf7je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "In new LangChain versions:\n",
        "\n",
        "Streaming is handled via callbacks\n",
        "Chat models emit tokens via on_llm_new_token\n",
        "No model.eval() concept exists for GPT models\n",
        "(that’s only for local PyTorch models)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "dN7x0UdkWAPQ",
        "outputId": "6cfca11a-e2d2-44b2-9cc0-bb0f15ddeb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIn new LangChain versions:\\n\\nStreaming is handled via callbacks\\nChat models emit tokens via on_llm_new_token\\nNo model.eval() concept exists for GPT models\\n(that’s only for local PyTorch models)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a streaming callback handler\n",
        "from langchain_core.callbacks import BaseCallbackHandler\n",
        "\n",
        "class StreamingStdOutCallbackHandler(BaseCallbackHandler):\n",
        "    def on_llm_new_token(self, token: str, **kwargs):\n",
        "        print(token, end=\"\", flush=True)\n"
      ],
      "metadata": {
        "id": "LX7CkmmTWWlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "\n",
        "streaming_llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"API_KEY\"),\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    deployment_name=\"gpt-4.1\",\n",
        "    temperature=0,\n",
        "    streaming=True,\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        ")\n",
        "#Note-callbacks must be passed at LLM creation time"
      ],
      "metadata": {
        "id": "CNsMRVA1Wdbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Update RAG to support streaming\n",
        "def rag_chain_streaming(\n",
        "    query,\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    output_parser,\n",
        "    k=5,\n",
        "):\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    results = vectorstore.similarity_search_with_score_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    docs = [doc for doc, score in results]\n",
        "    scores = [score for doc, score in results]\n",
        "\n",
        "    context = \"\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    confident = (\n",
        "        len(docs) > 0\n",
        "        and min(scores) <= 0.6\n",
        "        and len(context) >= 300\n",
        "    )\n",
        "\n",
        "    if confident:\n",
        "        print(\"\\nAnswer (FAISS):\\n\")\n",
        "\n",
        "        messages = prompt.format_messages(\n",
        "            question=query,\n",
        "            context=context\n",
        "        )\n",
        "\n",
        "        llm.invoke(messages)  # streams automatically\n",
        "        print(\"\\n\")\n",
        "\n",
        "        return\n",
        "\n",
        "    # ---- Web fallback ----\n",
        "    print(\"\\nAnswer (Web Search):\\n\")\n",
        "\n",
        "    web_results = web_search(query)\n",
        "    web_context, _ = build_web_context(web_results)\n",
        "\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=web_context\n",
        "    )\n",
        "\n",
        "    llm.invoke(messages)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "hGxBgwzKWhd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run it now\n",
        "questions = [\n",
        "    \"What are the key points from the State Of The Union?\",\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Q:\", q)\n",
        "    rag_chain_streaming(\n",
        "        q,\n",
        "        vectorstore,\n",
        "        streaming_llm,\n",
        "        prompt,\n",
        "        output_parser,\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmzqVPT4WzWW",
        "outputId": "acc93810-ac30-42c3-835d-efc8e013e359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Q: What are the key points from the State Of The Union?\n",
            "\n",
            "Answer (Web Search):\n",
            "\n",
            "Key points from the State of the Union include a focus on the health of the economy and immigration.\n",
            "\n",
            "\n",
            "==============================\n",
            "Q: How is the United States supporting Ukraine economically and militarily?\n",
            "\n",
            "Answer (FAISS):\n",
            "\n",
            "The United States is supporting Ukraine by providing over $1 billion in direct economic assistance, military aid, and humanitarian support, while also enforcing powerful economic sanctions against Russia.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Source Attribution & Citations (FAISS + Web)\n",
        "'''Right now your system:\n",
        "\n",
        "  -- Retrieves documents\n",
        "  -- Answers correctly\n",
        "  -- Streams responses\n",
        "\n",
        "  But it does not explain where the answer came from.\n",
        "\n",
        "  In Next Step we will:\n",
        "  -- Track document metadata\n",
        "  -- Inject sources into the prompt\n",
        "  -- Return answer + citations\n",
        "  -- Work for FAISS and web fallback\n",
        "  '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "OwWcSrudXBKd",
        "outputId": "85535477-6748-4359-8e21-ea76c9734062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Right now your system:\\n\\n  -- Retrieves documents\\n  -- Answers correctly\\n  -- Streams responses\\n\\n  But it does not explain where the answer came from.\\n\\n  In Next Step we will:\\n  -- Track document metadata\\n  -- Inject sources into the prompt\\n  -- Return answer + citations\\n  -- Work for FAISS and web fallback\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A citation is not a URL necessarily.\n",
        "  It can be:\n",
        "    File name\n",
        "    Document ID\n",
        "    Page number\n",
        "    Chunk index\n",
        "    Web URL\n",
        "Your FAISS docs already support this via Document.metadata\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "FzIrSe-AXe9h",
        "outputId": "eebedb9a-c10c-4519-f3d7-33da46a2a0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nA citation is not a URL necessarily.\\n  It can be:\\n    File name\\n    Document ID\\n    Page number\\n    Chunk index\\n    Web URL\\nYour FAISS docs already support this via Document.metadata\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensure metadata exists\n",
        "\n",
        "for i, doc in enumerate(similar_docs):\n",
        "    doc.metadata[\"source\"] = \"State of the Union Address\"\n",
        "    doc.metadata[\"chunk_id\"] = i\n"
      ],
      "metadata": {
        "id": "vugj2QpdXrJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modify context construction to include source tags\n",
        "def build_context_with_sources(docs):\n",
        "    context_chunks = []\n",
        "\n",
        "    for i, doc in enumerate(docs):\n",
        "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
        "        chunk_id = doc.metadata.get(\"chunk_id\", i)\n",
        "\n",
        "        context_chunks.append(\n",
        "            f\"[Source {i+1}: {source}, chunk {chunk_id}]\\n{doc.page_content}\"\n",
        "        )\n",
        "\n",
        "    return \"\\n\\n\".join(context_chunks)\n"
      ],
      "metadata": {
        "id": "_gt2JJmyZGIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Update prompt to Require citations\n",
        "template = \"\"\"\n",
        "You are an assistant for question-answering tasks.\n",
        "\n",
        "Use ONLY the provided context to answer.\n",
        "Cite sources using the format [Source X].\n",
        "\n",
        "If the answer is not contained in the context, say you don't know.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer (with citations):\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "MJhLn9ZqZXtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "Enaoe45dZhIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Update RAG Chain\n",
        "def rag_chain_with_citations(\n",
        "    query,\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    k=5,\n",
        "):\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    results = vectorstore.similarity_search_with_score_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    docs = [doc for doc, score in results]\n",
        "    scores = [score for doc, score in results]\n",
        "\n",
        "    context = build_context_with_sources(docs)\n",
        "\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=context\n",
        "    )\n",
        "\n",
        "    print(\"\\n Answer:\\n\")\n",
        "    llm.invoke(messages)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "O7MgXIe9Zumy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_web_context(web_results):\n",
        "    chunks = []\n",
        "\n",
        "    for i, r in enumerate(web_results):\n",
        "        chunks.append(\n",
        "            f\"[Source {i+1}: {r['source']}]\\n{r['content']}\" # Changed from r['url'] to r['source']\n",
        "        )\n",
        "\n",
        "    return \"\\n\\n\".join(chunks)"
      ],
      "metadata": {
        "id": "2gNVw9NAZ0fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "doc = vectorstore.similarity_search(\"test\")[0]\n",
        "print(doc.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q1esdx_aTJG",
        "outputId": "54c85316-9a10-46a8-db27-7a18fe7690e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': '/content/state_of_union.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test retrieval with scores\n",
        "query = \"How is the United States supporting Ukraine?\"\n",
        "query_embedding = hf_embed.embed_query(query)\n",
        "\n",
        "results = vectorstore.similarity_search_with_score_by_vector(\n",
        "    query_embedding, k=3\n",
        ")\n",
        "\n",
        "for i, (doc, score) in enumerate(results):\n",
        "    print(f\"\\n--- Document {i+1} ---\")\n",
        "    print(\"Score:\", score)\n",
        "    print(\"Source:\", doc.metadata.get(\"source\"))\n",
        "    print(doc.page_content[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1cuZ52Napnn",
        "outputId": "9d20fc24-408c-43bf-b0a3-15054541e9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Document 1 ---\n",
            "Score: 0.51034504\n",
            "Source: State of the Union Address\n",
            "The Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame. \n",
            "\n",
            "Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistan\n",
            "\n",
            "--- Document 2 ---\n",
            "Score: 0.6614249\n",
            "Source: State of the Union Address\n",
            "Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \n",
            "\n",
            "We are inflicting pain on Russia and supporting the people of Ukraine. Putin is now\n",
            "\n",
            "--- Document 3 ---\n",
            "Score: 0.7009914\n",
            "Source: State of the Union Address\n",
            "And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n",
            "\n",
            "To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has cos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test context construction with source tags\n",
        "context = build_context_with_sources([doc for doc, _ in results])\n",
        "print(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9VVdRNLau29",
        "outputId": "443dda50-0912-4495-9193-fdf792f4b9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Source 1: State of the Union Address, chunk 0]\n",
            "The Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame. \n",
            "\n",
            "Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. \n",
            "\n",
            "We are giving more than $1 Billion in direct assistance to Ukraine. \n",
            "\n",
            "And we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.\n",
            "\n",
            "[Source 2: State of the Union Address, chunk 1]\n",
            "Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \n",
            "\n",
            "We are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \n",
            "\n",
            "Together with our allies –we are right now enforcing powerful economic sanctions.\n",
            "\n",
            "[Source 3: State of the Union Address, chunk 3]\n",
            "And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n",
            "\n",
            "To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test prompt formatting\n",
        "messages = prompt.format_messages(\n",
        "    question=query,\n",
        "    context=context\n",
        ")\n",
        "\n",
        "for m in messages:\n",
        "    print(type(m).__name__)\n",
        "    print(m.content[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eNchEsva7am",
        "outputId": "1d6f65db-e916-4e35-d471-52edc579669e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HumanMessage\n",
            "\n",
            "You are an assistant for question-answering tasks.\n",
            "\n",
            "Use ONLY the provided context to answer.\n",
            "Cite sources using the format [Source X].\n",
            "\n",
            "If the answer is not contained in the context, say you don't know.\n",
            "\n",
            "Question: How is the United States supporting Ukraine?\n",
            "\n",
            "Context:\n",
            "[Source 1: State of the Union \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test LLM Citation behaviour\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Dz2w5MTbGH_",
        "outputId": "2c03d9ae-9903-44c9-cfd7-9442a36d4b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The United States is supporting Ukraine by providing military assistance, economic assistance, and humanitarian assistance. Specifically, the U.S. is giving more than $1 billion in direct assistance to Ukraine. Additionally, the U.S., together with its allies, is enforcing powerful economic sanctions on Russia to inflict pain on the Russian economy and support the people of Ukraine [Source 1][Source 2].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Full RAG\n",
        "result = rag_chain_with_citations(\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\",\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    k=5\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMlvtFCUbP6b",
        "outputId": "531c20c7-b5c9-4dbc-b1e0-bb905a89f95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Answer:\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we add structured JSON output, ranking sources, and highlighting exact sentences.\n",
        "'''\n",
        "-- Return JSON instead of free text\n",
        "-- Include retrieval scores for each chunk\n",
        "-- Optionally, highlight which sentences were used in the answer\n",
        "-- Compatible with both FAISS and web fallback\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "zA55NmV_bitk",
        "outputId": "ad09dac1-8644-47c4-c2c8-d8bcf7fe7638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n-- Return JSON instead of free text\\n-- Include retrieval scores for each chunk\\n-- Optionally, highlight which sentences were used in the answer\\n-- Compatible with both FAISS and web fallback\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_chain_structured(\n",
        "    query,\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt,\n",
        "    output_parser,\n",
        "    k=5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Hybrid RAG pipeline with structured JSON output, source citations, and web fallback.\n",
        "\n",
        "    Args:\n",
        "        query (str): User question\n",
        "        vectorstore: FAISS or similar retriever\n",
        "        llm: AzureChatOpenAI LLM\n",
        "        prompt: ChatPromptTemplate\n",
        "        output_parser: StrOutputParser or similar\n",
        "        k (int): Number of top documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            \"answer\": str,\n",
        "            \"confidence\": \"high\"|\"medium\"|\"low\",\n",
        "            \"source_type\": \"vectorstore\"|\"web\"|\"none\",\n",
        "            \"sources\": list of dicts {\n",
        "                \"source\": str,\n",
        "                \"score\": float|None,\n",
        "                \"text\": str\n",
        "            }\n",
        "        }\n",
        "    \"\"\"\n",
        "    # --- Step 1: FAISS retrieval ---\n",
        "    query_embedding = hf_embed.embed_query(query)\n",
        "    results = vectorstore.similarity_search_with_score_by_vector(\n",
        "        query_embedding, k=k\n",
        "    )\n",
        "\n",
        "    docs = [doc for doc, score in results]\n",
        "    scores = [score for doc, score in results]\n",
        "\n",
        "    context = \"\\n\".join(f\"{doc.page_content}\" for doc in docs)\n",
        "\n",
        "    # --- Step 2: Confidence check ---\n",
        "    confident = len(docs) > 0 and min(scores) <= 0.6 and len(context) >= 300\n",
        "\n",
        "    if confident:\n",
        "        sources = [\n",
        "            {\n",
        "                \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
        "                \"score\": score,\n",
        "                \"text\": doc.page_content\n",
        "            }\n",
        "            for doc, score in results\n",
        "        ]\n",
        "        source_type = \"vectorstore\"\n",
        "        confidence = \"high\"\n",
        "    else:\n",
        "        # --- Step 3: Web fallback ---\n",
        "        web_results = web_search(query)\n",
        "        if web_results:\n",
        "            context = \"\\n\".join(r.get('content', '') for r in web_results)\n",
        "            sources = [\n",
        "                {\n",
        "                    \"source\": r.get(\"link\") or r.get(\"url\") or \"unknown\",\n",
        "                    \"score\": None,\n",
        "                    \"text\": r.get(\"content\", \"\")\n",
        "                }\n",
        "                for r in web_results\n",
        "            ]\n",
        "            source_type = \"web\"\n",
        "            confidence = \"medium\"\n",
        "        else:\n",
        "            # No info available\n",
        "            return {\n",
        "                \"answer\": \"I don't know based on available information.\",\n",
        "                \"confidence\": \"low\",\n",
        "                \"source_type\": \"none\",\n",
        "                \"sources\": [],\n",
        "            }\n",
        "\n",
        "    # --- Step 4: Prompt formatting and LLM call ---\n",
        "    messages = prompt.format_messages(\n",
        "        question=query,\n",
        "        context=context\n",
        "    )\n",
        "\n",
        "    llm_output = llm.invoke(messages)\n",
        "    answer = output_parser.parse(llm_output)\n",
        "\n",
        "    # --- Step 5: Return structured JSON ---\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"confidence\": confidence,\n",
        "        \"source_type\": source_type,\n",
        "        \"sources\": sources,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "C9aAqbOSbw7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finally test your production grade RAG chain\n",
        "questions = [\n",
        "    \"What are the key points from the State Of The Union?\",\n",
        "    \"How is the United States supporting Ukraine economically and militarily?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    result = rag_chain_structured(q, vectorstore, llm, prompt, output_parser)\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Q:\", q)\n",
        "    print(\"Answer:\", result[\"answer\"])\n",
        "    print(\"Confidence:\", result[\"confidence\"])\n",
        "    print(\"Source Type:\", result[\"source_type\"])\n",
        "    print(\"Sources:\")\n",
        "    for s in result[\"sources\"]:\n",
        "        print(\"-\", s[\"source\"], \"(score:\", s[\"score\"], \")\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slOPP038b3eC",
        "outputId": "b31dbf25-95ac-4c67-c3c6-823d6d45fc2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Q: What are the key points from the State Of The Union?\n",
            "Answer: content=\"The key points from the State of the Union, ahead of President Joe Biden's third address, are that Americans are focused on the health of the economy and immigration [Source 1].\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 177, 'total_tokens': 215, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz6P40pQO9Yk37hGxqsya0E8viODZ', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd76-90ff-70d2-9ad5-11437d678d8f-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 177, 'output_tokens': 38, 'total_tokens': 215, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Confidence: medium\n",
            "Source Type: web\n",
            "Sources:\n",
            "- unknown (score: None )\n",
            "- unknown (score: None )\n",
            "- unknown (score: None )\n",
            "- unknown (score: None )\n",
            "\n",
            "==============================\n",
            "Q: How is the United States supporting Ukraine economically and militarily?\n",
            "Answer: content='The United States is supporting Ukraine both economically and militarily by providing more than $1 billion in direct assistance, which includes military, economic, and humanitarian aid. Additionally, the U.S. is working with allies to enforce powerful economic sanctions against Russia to further support Ukraine. However, American forces are not engaged in conflict within Ukraine; instead, U.S. military deployments are focused on defending NATO allies in Europe [Source X].' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 500, 'total_tokens': 586, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_f99638a8d7', 'id': 'chatcmpl-Cz6P5UY27ug5653wzv8eSK42rdVdQ', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--019bcd76-9424-72a2-a3c8-13b956081c5d-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 500, 'output_tokens': 86, 'total_tokens': 586, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Confidence: high\n",
            "Source Type: vectorstore\n",
            "Sources:\n",
            "- State of the Union Address (score: 0.5097524 )\n",
            "- State of the Union Address (score: 0.7064922 )\n",
            "- State of the Union Address (score: 0.7256527 )\n",
            "- State of the Union Address (score: 0.74181306 )\n",
            "- State of the Union Address (score: 0.79181457 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 7: Load and configure a quantized language model**\n",
        "\n",
        "Load a quantized version of a large language model (Falcon3-1B-Base) for efficient and cost-effective text generation.\n",
        "\n",
        "**Generation Step**: This model is responsible for generating the final answer. It takes the prompt (which includes the retrieved context) and produces a response, completing the RAG pipeline.\n",
        "\n",
        "**Efficiency**: 4-bit quantization reduces resource usage while maintaining performance, crucial for deploying RAG systems in production."
      ],
      "metadata": {
        "id": "GAO5i5tWWt6C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "AW13wgYNwU6W",
        "outputId": "20a9248e-8ad0-48c9-b476-fb21403b4892"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nimport torch\\n\\nMODEL_NAME = \"tiiuae/Falcon3-1B-Base\"\\n\\n# Configure 4-bit quantization\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.float16,\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_quant_type=\"nf4\"\\n)\\n\\n# Load model\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    MODEL_NAME,\\n    quantization_config=bnb_config,\\n    device_map=\"auto\"\\n)\\n\\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\nmodel.eval()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "#Using transformers\n",
        "'''\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"tiiuae/Falcon3-1B-Base\"\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model.eval()'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        ")\n",
        "\n",
        "prompt2 = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(\n",
        "        content=\"You are a knowledgeable assistant. Answer based on the retrieved context.\"\n",
        "    ),\n",
        "    (\"human\", \"{context}\\nQuestion: {question}\")\n",
        "])"
      ],
      "metadata": {
        "id": "g62BmgR3uHa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO7R4qXcx4K-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "7e246813-5406-419c-b75a-38275d13e0a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model.eval()\\ngeneration_config = model.generation_config\\n# Set temperature to 0 for deterministic responses\\ngeneration_config.temperature = 0.8\\n# Set number of returned sequences to 1\\ngeneration_config.num_return_sequences = 1\\n# Set maximum new tokens per response\\ngeneration_config.max_new_tokens = 256\\n# Disable token caching\\ngeneration_config.use_cache = False\\n# Set repetition penalty for more diverse responses\\ngeneration_config.repetition_penalty = 1.7\\n# Enable sampling for temperature to take effect\\ngeneration_config.do_sample = True\\n# Define pad and EOS token IDs\\ngeneration_config.pad_token_id = tokenizer.eos_token_id\\ngeneration_config.eos_token_id = tokenizer.eos_token_id'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "#with model.eval()-->you are telling a locally loaded neural network to:\n",
        "'''\n",
        "Disable dropout\n",
        "Disable training-only layers\n",
        "Switch to inference mode\n",
        "\n",
        "This is necessary because you own the model weights and execution'''\n",
        "\n",
        "'''model.eval()\n",
        "generation_config = model.generation_config\n",
        "# Set temperature to 0 for deterministic responses\n",
        "generation_config.temperature = 0.8\n",
        "# Set number of returned sequences to 1\n",
        "generation_config.num_return_sequences = 1\n",
        "# Set maximum new tokens per response\n",
        "generation_config.max_new_tokens = 256\n",
        "# Disable token caching\n",
        "generation_config.use_cache = False\n",
        "# Set repetition penalty for more diverse responses\n",
        "generation_config.repetition_penalty = 1.7\n",
        "# Enable sampling for temperature to take effect\n",
        "generation_config.do_sample = True\n",
        "# Define pad and EOS token IDs\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlrwNgcxyEpt"
      },
      "outputs": [],
      "source": [
        "'''from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        "    pipeline,\n",
        ")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 8: Set up the generation pipeline and chain the components**\n",
        "\n",
        "Build an end-to-end pipeline that seamlessly connects document retrieval with text generation.\n",
        "\n",
        "**Integration**: The chain uses the retriever to fetch context, applies the prompt template to integrate the query with the retrieved context, and then passes the final prompt to the LLM for answer generation.\n",
        "\n",
        "**Pipeline composition**: Using the pipe operator (|), the components are elegantly chained together to perform a complete RAG operation in one go."
      ],
      "metadata": {
        "id": "Zu6m0iRnXPCp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7mblPjGx4N2",
        "outputId": "141dc17f-d06d-457a-bd45-7d89136cc9b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "'''from langchain.llms import HuggingFacePipeline # Import HuggingFacePipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Create the HuggingFacePipeline object\n",
        "llm_pipeline = HuggingFacePipeline(pipeline=pipe)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZaWVCCQvXi5"
      },
      "outputs": [],
      "source": [
        "'''rag_chain = (\n",
        "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm_pipeline\n",
        "    | output_parser\n",
        ")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 9: Invoke the pipeline with a query**\n",
        "\n",
        "Execute the entire RAG pipeline with a sample query.\n",
        "\n",
        "**Final output**: The pipeline retrieves relevant chunks from the document, forms a context-rich prompt, and the LLM generates a concise answer based on that context.\n",
        "\n",
        "**End-to-end flow**: This step demonstrates the full cycle of RAG—retrieval and augmented generation—in action."
      ],
      "metadata": {
        "id": "YR0AosXgXis0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ptSN7VvZfv",
        "outputId": "344536ff-8a37-4f69-a0d8-012441753db7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/voc/work/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "'''result = rag_chain.invoke(\"How is the United States supporting Ukraine economically and militarily?\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "rRvnTlYEzQHY",
        "outputId": "53129755-583b-42ae-f09d-b49c7a51ff8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In order to provide financial or military resources directly towards helping those affected by conflict situations such as war crimes investigations can also involve funding humanitarian organizations working within these areas which may include medical teams assisting victims during conflicts; this would fall under international law regarding human rights protection measures when dealing specifically about how funds should ideally go if there exists any form of violence occurring between different groups living together peacefully but still having disagreements over territory/resources etc., so it might make sense depending upon specific circumstances whether certain types of donations made via official channels provided through government agencies responsible overseeing security forces operating near borders where clashes often occur due lack thereof proper communication systems being established beforehand among both sides involved making sure everyone understands exactly why money needs going out instead simply throwing cash into pockets hoping someone else takes care off whatever problem arises later down road without actually knowing anything concrete behind situation causing strife initially leading up until point reached here today!'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''result'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "This RAG (Retrieval-augmented generation) pipeline exemplifies how to combine retrieval-based methods with generative AI to produce informed, context-driven answers. By following these high-level steps—setting up the environment, loading and splitting the document, generating embeddings, building a FAISS vector store, and creating a retriever—you establish a robust foundation for pinpointing the most relevant pieces of information. Integrating a prompt template ensures that the language model is guided to leverage this retrieved context effectively. Finally, by employing a quantized language model in an end-to-end chain, the system efficiently generates concise and accurate responses. Overall, this approach not only enhances the model’s output by grounding it in factual context but also streamlines the process, making it scalable and adaptable to various domains and applications."
      ],
      "metadata": {
        "id": "D5_Ht02VX-W5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cQlkpX4iYCwy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "2.9-RAG_Implementation.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3 [3.10]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}