{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VKcw2iCvHrTvGgwTG83zHiy4",
      "metadata": {
        "id": "VKcw2iCvHrTvGgwTG83zHiy4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# Initialize client once\n",
        "client = AzureOpenAI(\n",
        "    api_key=\"MyKey1\",\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    azure_endpoint=\"https://ak7si-mi0k5d0p-eastus2.cognitiveservices.azure.com/\"\n",
        ")"
      ],
      "metadata": {
        "id": "FjyUcVbgy-Rk"
      },
      "id": "FjyUcVbgy-Rk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WiV_xPDgJJ5_",
      "metadata": {
        "id": "WiV_xPDgJJ5_"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, deployment_name=\"gpt-4.1-nano\"):\n",
        "    \"\"\"\n",
        "    Get a chat completion from Azure OpenAI.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): User input prompt.\n",
        "        deployment_name (str): The deployment name you gave your model in Azure portal.\n",
        "\n",
        "    Returns:\n",
        "        dict: Full response object, or error dict.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=deployment_name,    # <-- This is the \"deployment name\" not the raw model name\n",
        "            messages=messages,\n",
        "            temperature=0.1,\n",
        "            top_p=0.8,\n",
        "            max_tokens=512\n",
        "        )\n",
        "\n",
        "        return response.model_dump()  # Return the full response as dict\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Your Prompts__\n",
        "#Provide a series of prompts that guide the model through a chain of thought.\n",
        "#Call the __get_completion__ to get a response from the AI model.\n",
        "#Print both the prompt and the AI-generated response."
      ],
      "metadata": {
        "id": "ZlY6258sVC5p"
      },
      "id": "ZlY6258sVC5p",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nl0JIGN8JhZt",
      "metadata": {
        "id": "nl0JIGN8JhZt"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Imagine you are a detective trying to solve a mystery.\",\n",
        "    \"You arrive at the crime scene and start looking for clues.\",\n",
        "    \"You find a strange object at the crime scene. What is it?\",\n",
        "    \"How does this object relate to the crime?\",\n",
        "    \"Who do you think is the suspect and why?\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pb9YT-O4JwXE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb9YT-O4JwXE",
        "outputId": "b2a84502-1796-43dd-d36d-5cc117db3489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Imagine you are a detective trying to solve a mystery.\n",
            "Response: {'id': 'chatcmpl-CmO2fX1XNdBJzaLIhjePAUDL5IuXe', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Absolutely! Let's dive into the mystery together. \\n\\n**Scenario:**  \\nA valuable diamond necklace has gone missing from the city museum during a high-profile exhibition. The security footage shows no signs of forced entry, and the staff claims to have seen nothing suspicious. The only clues are a faint footprint near the display case and a strange, partially torn piece of fabric found on the floor.\\n\\n**As the detective, I would start by asking myself:**  \\n- Who had access to the exhibit during the time of the theft?  \\n- Are there any witnesses or security logs that might reveal suspicious activity?  \\n- Could the footprint belong to someone known or an outsider?  \\n- What does the torn fabric suggest? Was there a struggle or an attempt to hide something?\\n\\n**Next steps:**  \\n1. Interview staff and visitors around the time of the theft.  \\n2. Examine the footprint—measure it, analyze its size, and see if it matches any known shoes in the staff or visitor records.  \\n3. Collect and analyze the torn fabric for fibers or DNA evidence.  \\n4. Review security footage from adjacent areas or earlier times for suspicious behavior.\\n\\n**Your turn:**  \\nWould you like to suggest a suspect, a theory, or perhaps help analyze one of the clues?\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765648197, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 255, 'prompt_tokens': 18, 'total_tokens': 273, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n",
            "\n",
            "Prompt: You arrive at the crime scene and start looking for clues.\n",
            "Response: {'id': 'chatcmpl-CmO2gDKtmfr8Kgow8Ua8DWIps3gUS', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'As I arrive at the crime scene, I begin by securing the area to ensure that no evidence is disturbed. I observe the surroundings carefully, noting any immediate signs of disturbance or unusual activity. I look for physical clues such as fingerprints, footprints, bloodstains, or any objects out of place. I also check for surveillance cameras or witnesses nearby. Documenting everything meticulously, I take photographs and detailed notes to preserve the scene for further analysis. My goal is to gather as much relevant evidence as possible to help piece together what happened.', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'low'}}}], 'created': 1765648198, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 108, 'prompt_tokens': 19, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'low'}}}]}\n",
            "\n",
            "Prompt: You find a strange object at the crime scene. What is it?\n",
            "Response: {'id': 'chatcmpl-CmO2h2z2yDFRakdkcfxoExUt3FGKU', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Could you please provide more details about the scene or the object? That way, I can help identify or analyze it more accurately.', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765648199, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 27, 'prompt_tokens': 21, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'low'}}}]}\n",
            "\n",
            "Prompt: How does this object relate to the crime?\n",
            "Response: {'id': 'chatcmpl-CmO2hyTiHUkFbgc9yigH5xtWesk0s', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Could you please provide more context or specify which object you're referring to? That way, I can give you a more accurate and helpful response.\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765648199, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 29, 'prompt_tokens': 16, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n",
            "\n",
            "Prompt: Who do you think is the suspect and why?\n",
            "Response: {'id': 'chatcmpl-CmO2iVcvGxBLmUxxULSPgq1QEnw1c', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Could you please provide more context or details about the situation or case you're referring to? That way, I can better assist you in analyzing the suspect.\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765648200, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 31, 'prompt_tokens': 17, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for prompt in prompts:\n",
        "    response = get_completion(prompt)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1FC--falKi_7",
      "metadata": {
        "id": "1FC--falKi_7"
      },
      "outputs": [],
      "source": [
        "#Another example\n",
        "prompt = \"\"\"\n",
        "\n",
        "let's analyze the sentiment of the review step by step\n",
        "\n",
        "1. Identify the Positive aspect of the review and give a score from 10\n",
        "2. Identify the Negative aspect of the review and give a score from 10\n",
        "3. Weight the positive and negative aspect to determine the overall sentiment.\n",
        "4. provide the final sentiment classification with justification for scores used in above steps.\n",
        "\n",
        "Review: \"The product is very well designed product\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nLdOToLGLnm4",
      "metadata": {
        "id": "nLdOToLGLnm4"
      },
      "outputs": [],
      "source": [
        "response = get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bmkmNpZiLqry",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmkmNpZiLqry",
        "outputId": "96ba6944-9e0a-4130-ec26-7812d62c7dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-CmL0O8sUJSd8zMf8WieTlKecaky5A', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Let\\'s analyze the review step by step:\\n\\n**1. Identify the Positive aspect of the review and give a score from 10**\\n\\n- The review states: \"The product is very well designed product.\"\\n- The positive aspect is the design quality, which is described as \"very well designed.\"\\n- This indicates a strong positive sentiment toward the design.\\n\\n**Score:** 8/10\\n\\n**2. Identify the Negative aspect of the review and give a score from 10**\\n\\n- The review does not mention any negative aspects.\\n- There are no complaints or criticisms present.\\n\\n**Score:** 0/10\\n\\n**3. Weight the positive and negative aspects to determine overall sentiment**\\n\\n- Since the positive aspect is strong (8/10) and negative is absent (0/10), the overall sentiment leans positive.\\n- The absence of negatives suggests the review is primarily positive.\\n\\n**4. Final sentiment classification with justification**\\n\\n- **Sentiment:** Positive\\n- **Justification:** The review highlights a strong positive aspect (design quality) with a high score, and no negative points are mentioned. Therefore, the overall sentiment is clearly positive, supported by the high positive score and zero negative score.\\n\\n**Final conclusion:** The review is predominantly positive, emphasizing the well-designed nature of the product.', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765636524, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 260, 'prompt_tokens': 95, 'total_tokens': 355, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fBui_PtqLtBZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBui_PtqLtBZ",
        "outputId": "085aa27a-e830-409b-f3f8-7e73c3c6b3a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-CmO4ulUw8qSScxc2Ow88RqNG7P7sL',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': 'Let\\'s analyze the review step by step:\\n\\n**1. Identify the Positive aspect of the review and give a score from 10**  \\n- The review states: \"The product is very well designed product.\"  \\n- The positive aspect is the design quality.  \\n- Since the statement is explicitly positive about the design, I would assign a high positive score.  \\n**Score: 8/10**\\n\\n**2. Identify the Negative aspect of the review and give a score from 10**  \\n- There is no mention of any negative aspect in the review.  \\n- Therefore, negative sentiment is minimal or absent.  \\n**Score: 1/10** (indicating very low negativity)\\n\\n**3. Weight the positive and negative aspects to determine overall sentiment**  \\n- Positive score: 8  \\n- Negative score: 1  \\n- Since the positive aspect is significantly stronger, the overall sentiment leans positive.\\n\\n**4. Final sentiment classification with justification**  \\n- The review is predominantly positive, emphasizing the well-designed nature of the product.  \\n- The absence of negative comments supports a positive overall sentiment.  \\n- **Final sentiment: Positive**  \\n- **Justification:** The high positive score (8/10) and minimal negative score (1/10) indicate a favorable opinion, leading to an overall positive sentiment.',\n",
              "    'refusal': None,\n",
              "    'role': 'assistant',\n",
              "    'annotations': [],\n",
              "    'audio': None,\n",
              "    'function_call': None,\n",
              "    'tool_calls': None},\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'protected_material_code': {'filtered': False, 'detected': False},\n",
              "    'protected_material_text': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}],\n",
              " 'created': 1765648336,\n",
              " 'model': 'gpt-4.1-nano-2025-04-14',\n",
              " 'object': 'chat.completion',\n",
              " 'service_tier': None,\n",
              " 'system_fingerprint': 'fp_03e44fcc34',\n",
              " 'usage': {'completion_tokens': 268,\n",
              "  'prompt_tokens': 95,\n",
              "  'total_tokens': 363,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
              " 'prompt_filter_results': [{'prompt_index': 0,\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'jailbreak': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bCHlhhSUL4nF",
      "metadata": {
        "id": "bCHlhhSUL4nF"
      },
      "outputs": [],
      "source": [
        "#Another example\n",
        "prompt = \"\"\"\n",
        "\n",
        "let's sort the values of the list step by step\n",
        "\n",
        "1. Start with the unsorted list.\n",
        "2. Compare each elements and find the smallest value.\n",
        "3. Place the samllest value in the first position.\n",
        "4. Repeat the process for all the remaming elements.\n",
        "5. provide the sorted list.\n",
        "\n",
        "Sort the list : [3,1,4,6,5,9,2]\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rwY12Ul0MMhs",
      "metadata": {
        "id": "rwY12Ul0MMhs"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "\n",
        "\"You have 12 identical-looking balls, but one is either heavier or lighter.\n",
        "You have a balance scale and can only use it three times.\n",
        "Explain step-by-step how you can find the odd ball and determine whether it is heavier or lighter.\n",
        "Think through the problem carefully and explain your reasoning in detail before giving the final answer.\"\n",
        "\n",
        "\"\"\"\n",
        "response = get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IGFmVJiWM8-C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGFmVJiWM8-C",
        "outputId": "df35bf4e-408b-4c1e-a2f4-a337db60fae2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-C8dhKgYZQK5tMQncPza4Z92uUhSSs',\n",
              " 'choices': [{'finish_reason': 'length',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': \"To solve the problem of identifying the odd ball among 12 identical-looking balls, where one ball is either heavier or lighter, we can use a systematic approach with a balance scale. The key is to divide the balls into groups and use the results of the weighings to narrow down the possibilities. Here’s a step-by-step breakdown of the solution:\\n\\n### Step 1: Initial Grouping\\n1. **Divide the 12 balls into three groups of 4 balls each**: Let's label the balls as A1, A2, A3, A4 (Group A), B1, B2, B3, B4 (Group B), and C1, C2, C3, C4 (Group C).\\n\\n### Step 2: First Weighing\\n2. **Weigh Group A against Group B**:\\n   - **Case 1**: If the scales balance, then the odd ball is in Group C (C1, C2, C3, C4).\\n   - **Case 2**: If Group A is heavier, then the odd ball is either in Group A (and heavier) or in Group B (and lighter).\\n   - **Case 3**: If Group B is heavier, then the odd ball is either in Group B (and heavier) or in Group A (and lighter).\\n\\n### Step 3: Second Weighing\\nNow, we will analyze each case separately.\\n\\n#### Case 1: A = B (Odd ball is in Group C)\\n3. **Weigh C1, C2 against C3, C4**:\\n   - If they balance, then the odd ball is not among C1, C2, C3, or C4, which is impossible since we know one is odd. So, this case will not occur.\\n   - If C1, C2 is heavier, then one of C1 or C2 is heavier, or one of C3 or C4 is lighter.\\n   - If C3, C4 is heavier, then one of C3 or C4 is heavier, or one of C1 or C2 is lighter.\\n\\n#### Case 2: A > B (Odd ball is in A or B)\\n4. **Weigh A1, A2, B1 against A3, A4, B2**:\\n   - If they balance, then the odd ball is either B3 or B4 (and lighter) or A1, A2 (and heavier).\\n   - If\",\n",
              "    'refusal': None,\n",
              "    'role': 'assistant',\n",
              "    'annotations': [],\n",
              "    'audio': None,\n",
              "    'function_call': None,\n",
              "    'tool_calls': None},\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'protected_material_code': {'filtered': False, 'detected': False},\n",
              "    'protected_material_text': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}],\n",
              " 'created': 1756175258,\n",
              " 'model': 'gpt-4o-mini-2024-07-18',\n",
              " 'object': 'chat.completion',\n",
              " 'service_tier': None,\n",
              " 'system_fingerprint': 'fp_efad92c60b',\n",
              " 'usage': {'completion_tokens': 512,\n",
              "  'prompt_tokens': 74,\n",
              "  'total_tokens': 586,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
              " 'prompt_filter_results': [{'prompt_index': 0,\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'jailbreak': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "__o766f5NCpZ",
      "metadata": {
        "id": "__o766f5NCpZ"
      },
      "outputs": [],
      "source": [
        "#Another example\n",
        "prompt = \"\"\"\n",
        "Let's consider which is heavier: 1000 feathers or a 30-pound weight.\n",
        "I'll think through this in a few different ways and then decide which answer seems most consistent.\n",
        "\n",
        "1. First line of reasoning: A single feather is very light, almost weightless.\n",
        "So, 1000 feathers might still be quite light, possibly lighter than a 30-pound weight.\n",
        "\n",
        "2. Second line of reasoning: 1000 is a large number, and when you add up the weight of so many feathers,\n",
        "it could be quite heavy. Maybe it's heavier than a 30-pound weight.\n",
        "\n",
        "3. Third line of reasoning: The average weight of a feather is very small. Even 1000 feathers would not add up to 30 pounds.\n",
        "\n",
        "Considering these reasonings, the most consistent answer is: & the reason to choose the answer is :\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a0I6UQ_NvQI",
      "metadata": {
        "id": "6a0I6UQ_NvQI"
      },
      "outputs": [],
      "source": [
        "response = get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lq4_40kdNz9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq4_40kdNz9f",
        "outputId": "61519a36-87c3-402e-85cf-f8a38098c1f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-CmOAEs365wJaM1Dr9Jlpdmqlu9BOh',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': \"Considering all three lines of reasoning:\\n\\n1. The first suggests that a single feather is extremely light, so 1000 feathers might still be quite light—likely less than 30 pounds.\\n2. The second emphasizes that 1000 feathers could accumulate to a significant weight, possibly exceeding 30 pounds.\\n3. The third indicates that even 1000 feathers probably don't reach 30 pounds, given their tiny individual weights.\\n\\nMost evidence points toward the idea that 1000 feathers are still relatively light and unlikely to weigh more than a 30-pound weight. Therefore, the most consistent answer is:\\n\\n**1000 feathers are lighter than a 30-pound weight.**\\n\\nThe reason to choose this answer is because, based on the typical weight of a feather, even a thousand of them would not come close to reaching 30 pounds.\",\n",
              "    'refusal': None,\n",
              "    'role': 'assistant',\n",
              "    'annotations': [],\n",
              "    'audio': None,\n",
              "    'function_call': None,\n",
              "    'tool_calls': None},\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'protected_material_code': {'filtered': False, 'detected': False},\n",
              "    'protected_material_text': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}],\n",
              " 'created': 1765648666,\n",
              " 'model': 'gpt-4.1-nano-2025-04-14',\n",
              " 'object': 'chat.completion',\n",
              " 'service_tier': None,\n",
              " 'system_fingerprint': 'fp_03e44fcc34',\n",
              " 'usage': {'completion_tokens': 168,\n",
              "  'prompt_tokens': 176,\n",
              "  'total_tokens': 344,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
              " 'prompt_filter_results': [{'prompt_index': 0,\n",
              "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
              "    'jailbreak': {'filtered': False, 'detected': False},\n",
              "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
              "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
              "    'violence': {'filtered': False, 'severity': 'safe'}}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To work with Langchain, Install langchain related dependencies"
      ],
      "metadata": {
        "id": "d1M365W83qas"
      },
      "id": "d1M365W83qas",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lwHLLhIDN1Rp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwHLLhIDN1Rp",
        "outputId": "a05dcc04-ca16-420d-9ef6-9165b523751d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (1.1.3)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.4.42)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (2.11.10)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.32.5)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai langchain_core langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade \"langchain>=0.3.29\" \"langchain-core>=1.0.0\" langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH-Er3wK4ENB",
        "outputId": "6a097425-b1dd-4e65-d357-69d0bfd339b1"
      },
      "id": "oH-Er3wK4ENB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain>=0.3.29 in /usr/local/lib/python3.12/dist-packages (1.1.3)\n",
            "Requirement already satisfied: langchain-core>=1.0.0 in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.3)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.3.29) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.3.29) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (0.4.42)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0) (0.12.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=1.0.0) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.3.29) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.3.29) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.3.29) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.3.29) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3.29) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3.29) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3.29) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain>=0.3.29) (1.12.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureOpenAI"
      ],
      "metadata": {
        "id": "qMcbe_eQ4LfW"
      },
      "id": "qMcbe_eQ4LfW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize client once\n",
        "#client_lc = AzureOpenAI(\n",
        "#    api_key=\"MyKey1\",\n",
        "#    api_version=\"2024-12-01-preview\",\n",
        "#    azure_endpoint=\"https://ak7si-mi0k5d0p-eastus2.cognitiveservices.azure.com/\",\n",
        "#    deployment_name=\"gpt-4o-mini\", # chat completions would work with this model\n",
        "#    temperature=0.5,\n",
        "#    top_p=0.8,\n",
        "#    max_tokens=512\n",
        "#)"
      ],
      "metadata": {
        "id": "pjtgqh4r4hU_"
      },
      "id": "pjtgqh4r4hU_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "# Initialize client once\n",
        "client_lc = AzureChatOpenAI(\n",
        "    api_key=\"MyKey1\",\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    azure_endpoint=\"https://ak7si-mi0k5d0p-eastus2.cognitiveservices.azure.com/\",\n",
        "    deployment_name=\"gpt-5-chat\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "MbWwtnel4scU"
      },
      "id": "MbWwtnel4scU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_lc(prompt):\n",
        "    try:\n",
        "        response = client_lc.invoke(prompt)\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}"
      ],
      "metadata": {
        "id": "a_Ip0ZzV47rU"
      },
      "id": "a_Ip0ZzV47rU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain human intelligence with respect to cognitive ability\"\n",
        "response = get_completion_lc(prompt)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSxnw6wi5RIp",
        "outputId": "31b6e624-2398-4bf9-bc75-7b40a806b0f8"
      },
      "id": "oSxnw6wi5RIp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human intelligence, in the context of **cognitive ability**, refers to the set of mental capacities that enable people to acquire knowledge, reason, solve problems, and adapt to new situations. It encompasses a broad range of processes that allow individuals to think, learn, and apply what they know effectively.\n",
            "\n",
            "Here’s a breakdown of how cognitive abilities contribute to human intelligence:\n",
            "\n",
            "1. **Perception and Attention**  \n",
            "   These are the foundational processes that allow humans to notice and interpret information from the environment. Attention determines what information is processed deeply, influencing learning and memory formation.\n",
            "\n",
            "2. **Memory**  \n",
            "   Memory—both short-term (working memory) and long-term—is crucial for storing and retrieving information. Working memory supports complex reasoning and problem-solving, while long-term memory allows for the accumulation of knowledge and experiences.\n",
            "\n",
            "3. **Learning**  \n",
            "   Learning is the process of acquiring new information or skills. It involves forming associations, recognizing patterns, and adjusting behavior based on feedback. Effective learning depends on attention, memory, and motivation.\n",
            "\n",
            "4. **Reasoning and Problem-Solving**  \n",
            "   These abilities allow humans to manipulate information, make inferences, and generate solutions. Reasoning can be deductive (drawing logical conclusions from general principles) or inductive (forming generalizations from specific examples).\n",
            "\n",
            "5. **Language and Communication**  \n",
            "   Language is a powerful cognitive tool that enables abstract thought, complex communication, and cultural transmission. It allows people to represent ideas symbolically and share knowledge efficiently.\n",
            "\n",
            "6. **Executive Functions**  \n",
            "   These include planning, decision-making, cognitive flexibility, and inhibitory control. Executive functions help regulate behavior and thought, allowing individuals to pursue goals and adapt to changing circumstances.\n",
            "\n",
            "7. **Metacognition**  \n",
            "   Metacognition is “thinking about thinking”—the ability to monitor and control one’s cognitive processes. It enables self-awareness of understanding, strategy selection, and error correction.\n",
            "\n",
            "Together, these cognitive abilities form the basis of **general intelligence (often referred to as “g” in psychology)**, which reflects an individual’s overall capacity to process information and apply mental skills across diverse tasks.\n",
            "\n",
            "In summary, human intelligence—viewed through cognitive ability—is not just about knowledge or skill in a single domain, but the flexible and integrated use of multiple mental processes that enable understanding, learning, and adaptation in complex environments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(langchain.__version__)"
      ],
      "metadata": {
        "id": "mUuyJkle5a72"
      },
      "id": "mUuyJkle5a72",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "UZfL6bjWSoJp"
      },
      "id": "UZfL6bjWSoJp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z-JFzudWwLiE",
      "metadata": {
        "id": "z-JFzudWwLiE"
      },
      "outputs": [],
      "source": [
        "\n",
        "templatee = \" Please write a {length} review,of the book {book_title}. \"\n",
        "\n",
        "input_variabless = [ \"length\", \"book_title\" ]\n",
        "\n",
        "# Creating the prompt template\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=input_variabless,\n",
        "    template=templatee\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A19x7WxYwVUp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A19x7WxYwVUp",
        "outputId": "d1f25eaf-74ce-4534-ad00-402b55571529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Please write a short review,of the book  House Of Dragon. \n",
            "AI Response:\n",
            "{'id': 'chatcmpl-C9JOsuKfGicZpmxcCxIL1WRojsS5v', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': '\"House of the Dragon\" is a captivating exploration of the Targaryen dynasty, set in the rich and intricate world created by George R.R. Martin. This prequel delves into the complex relationships, political intrigue, and the fiery legacy of the Targaryens, showcasing their rise and the internal conflicts that ultimately lead to their downfall. The narrative is filled with vivid characters, each with their own ambitions and flaws, making it a compelling read for fans of epic fantasy. The book masterfully balances action and character development, immersing readers in a world of dragons, betrayal, and power struggles. Overall, \"House of the Dragon\" is a thrilling addition to the lore of Westeros, offering both depth and excitement for those eager to explore the history behind the beloved series.', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1756335562, 'model': 'gpt-4o-mini-2024-07-18', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_efad92c60b', 'usage': {'completion_tokens': 160, 'prompt_tokens': 22, 'total_tokens': 182, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ],
      "source": [
        "formatted_prompt = prompt.format(length = \"short\", book_title = \" House Of Dragon\")\n",
        "\n",
        "print(formatted_prompt)\n",
        "\n",
        "response = get_completion(formatted_prompt)\n",
        "print(\"AI Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5QWV2NU7wdZ5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QWV2NU7wdZ5",
        "outputId": "84f79134-abde-4ee5-de82-fdcf4f1020de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Give me an interesting fact about space exploration\n",
            "AI Response:\n",
            "{'id': 'chatcmpl-C9JPAnynjNu8N5M0Oa6oQFfZS65KP', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"One interesting fact about space exploration is that the Voyager 1 spacecraft, launched by NASA in 1977, is the farthest human-made object from Earth. As of now, it is over 14 billion miles away and has entered interstellar space, providing valuable data about the outer solar system and the environment beyond it. Voyager 1 carries a Golden Record, which contains sounds and images representing the diversity of life and culture on Earth, intended as a message to any extraterrestrial life that might encounter it. This makes it not only a scientific mission but also a symbolic gesture of humanity's desire to connect with the cosmos.\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1756335580, 'model': 'gpt-4o-mini-2024-07-18', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_efad92c60b', 'usage': {'completion_tokens': 126, 'prompt_tokens': 15, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ],
      "source": [
        "jinja2_template = \"Give me an {{ adjective }} fact about {{ topic }}\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(jinja2_template, template_format = \"jinja2\" )\n",
        "\n",
        "user_question = prompt.format(adjective=\"interesting\", topic=\"space exploration\")\n",
        "\n",
        "print(user_question)\n",
        "\n",
        "response = get_completion(user_question)\n",
        "print(\"AI Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "HimcVeX8wiBr",
      "metadata": {
        "id": "HimcVeX8wiBr"
      },
      "outputs": [],
      "source": [
        "#Generate multiple lines of reasoning to answer the question.\n",
        "#Ask the AI to evaluate these reasonings and determine the most consistent answer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Let's consider which is heavier: 1000 feathers or a 30-pound weight.\n",
        "I'll think through this in a few different ways and then decide which answer seems most consistent.\n",
        "\n",
        "1. First line of reasoning: A single feather is very light, almost weightless.\n",
        "So, 1000 feathers might still be quite light, possibly lighter than a 30-pound weight.\n",
        "\n",
        "2. Second line of reasoning: 1000 is a large number, and when you add up the weight of so many feathers,\n",
        "it could be quite heavy. Maybe it's heavier than a 30-pound weight.\n",
        "\n",
        "3. Third line of reasoning: The average weight of a feather is very small. Even 1000 feathers would not add up to 30 pounds.\n",
        "\n",
        "Considering these reasonings, the most consistent answer is:\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_kNat2gWZAN",
        "outputId": "5d92cfa4-25f0-4b09-9866-33608e1b186f"
      },
      "id": "I_kNat2gWZAN",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-CmQImEgUoOXMoytlZrgoXX68r4Su3', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Given the reasoning you've outlined:\\n\\n- The first reasoning suggests that a single feather is very light, so 1000 feathers might still be lighter than 30 pounds.\\n- The second reasoning considers that 1000 feathers could add up to a significant weight, potentially exceeding 30 pounds.\\n- The third reasoning emphasizes that individual feathers are very light, so even 1000 of them probably wouldn't reach 30 pounds.\\n\\nIn reality, the weight of 1000 feathers depends on the size and type of the feathers, but generally, feathers are quite light. Typically, a single feather weighs only a few grams at most. For example, if one feather weighs about 0.5 grams, then 1000 feathers weigh approximately 500 grams, which is about 1.1 pounds—much less than 30 pounds.\\n\\n**Therefore, the most consistent answer is:**  \\n**1000 feathers are lighter than a 30-pound weight.**\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765656884, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 190, 'prompt_tokens': 167, 'total_tokens': 357, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "\n",
        "A farmer has 17 sheeps, all but 9 run away. How many are left?\n",
        "\n",
        "1. All but 9 ran away --> 9 are left\n",
        "\n",
        "2. \"All but 9\" means 9 stayed --> 9 are left\n",
        "\n",
        "3. Subtracting 17 - 9 --> 8 are left\n",
        "\n",
        "Considering these reasonings, the most consistent answer is:\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWKH3oh9WfDy",
        "outputId": "9b389b25-3635-40c9-972f-5d041e1d5b84"
      },
      "id": "hWKH3oh9WfDy",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-CmQJAq9hGoRT47kJv4CIUVbHk6gNn', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Let\\'s analyze the reasoning carefully:\\n\\n- The phrase \"all but 9 run away\" means that 9 sheep did **not** run away.\\n- Therefore, the number of sheep remaining is **9**.\\n\\nThe other interpretations:\\n\\n- \"All but 9\" does **not** mean subtracting 9 from 17; it simply indicates that 9 sheep are left.\\n- The statement \"17 - 9 = 8\" is incorrect in this context because it misinterprets the phrase.\\n\\n**Most consistent answer:**\\n\\n**9 sheep are left.**', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765656908, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 115, 'prompt_tokens': 87, 'total_tokens': 202, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "I will solve the following math problem in several different ways and check if I arrive at the same answer each time.\n",
        "\n",
        "Problem: There were 15 apples and you took away 4, how many apples do you have?\n",
        "\n",
        "1. First approach: 15 apples - 4 apples = 11 apples, which is incorrect\n",
        "\n",
        "2. Second approach: If i take away 4 apples, then i have 4 apples with me, which is correct\n",
        "\n",
        "3. Third approach: Taking away 4 apples means i have 4 apples, which is correct\n",
        "\n",
        "4. Fourth approach : Subtracting 4 from 15 will give me 11 apples, which is incorrect.\n",
        "\n",
        "Let's see which approach gives the most consistent result.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AbQyfG5Wku-",
        "outputId": "78587bc5-c8ef-410c-840d-23fec1e0313e"
      },
      "id": "7AbQyfG5Wku-",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-CmQJPPVcjaVNTRx79zfqvCIYf0dbz', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Let\\'s analyze each approach carefully:\\n\\n1. **First approach:**  \\n\"15 apples - 4 apples = 11 apples\"  \\nThis calculation correctly finds the number of apples remaining **after** taking away 4.  \\n**However, the question asks:** *\"how many apples do you have?\"*  \\nIf you took away 4 apples, **you now have those 4 apples** with you.  \\n**So, this approach is incorrect** in answering the question, because it focuses on what\\'s left, not what you have.\\n\\n2. **Second approach:**  \\n\"If I take away 4 apples, then I have 4 apples with me.\"  \\n**This is correct** because when you take away 4 apples, **you now possess those 4 apples**.\\n\\n3. **Third approach:**  \\n\"Taking away 4 apples means I have 4 apples.\"  \\n**This is also correct** for the same reason as the second approach.\\n\\n4. **Fourth approach:**  \\n\"Subtracting 4 from 15 will give me 11 apples,\" which is **incorrect** in context.  \\n**Again, this is about what\\'s left, not what you have.**  \\nSo, this approach is incorrect for answering the question.\\n\\n---\\n\\n### **Summary:**\\n\\n- The **correct answer** to \"how many apples do you have?\" is **4**, because you took away 4 apples and now possess them.\\n- Approaches **2 and 3** correctly identify that you have 4 apples.\\n- Approaches **1 and 4** focus on the remaining apples (11), which **do not answer** the question asked.\\n\\n**Conclusion:**  \\nThe most consistent and correct approaches are **2 and 3**, both indicating you have **4 apples**.', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765656923, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 360, 'prompt_tokens': 154, 'total_tokens': 514, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Provide a series of prompts that guide the model through a tree of thought.\n",
        "#Call __get_completion__ to get a response from the AI model.\n",
        "#Print both the prompt and the AI-generated response."
      ],
      "metadata": {
        "id": "EAtrIkhEWod4"
      },
      "id": "EAtrIkhEWod4",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Solve the problem: A farmer has 100 meters of fencing and wants to enclose the maximum area for his rectangular field. What should the dimensions be?\n",
        "\n",
        "Let's think about this in a few ways:\n",
        "\n",
        "1. If the field is a square, each side would be 100 / 4 = 25 meters. The area would be 25 * 25 = 625 square meters.\n",
        "\n",
        "2. What if the field is not a square? Let's try a 4:1 ratio. The lengths would be 40 and 10 meters. The area would be 40 * 10 = 400 square meters.\n",
        "\n",
        "3. Are there any other ratios that might give a larger area than a square or a 4:1 rectangle?\n",
        "\n",
        "Considering these options and reason out on own and then output the best dimensions for the maximum area :\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"AI Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWU5Oqs4W3SX",
        "outputId": "692a4270-56b1-4ece-a01c-0416c835158e"
      },
      "id": "vWU5Oqs4W3SX",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Response:\n",
            "{'id': 'chatcmpl-CmQKghqtZxtskqzSJ5lKMvS90yekL', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Let's analyze the problem systematically.\\n\\nGiven:\\n- Total fencing (perimeter) \\\\( P = 100 \\\\) meters\\n- The field is rectangular with length \\\\( L \\\\) and width \\\\( W \\\\)\\n- Perimeter constraint: \\\\( 2L + 2W = 100 \\\\) or simplified as \\\\( L + W = 50 \\\\)\\n\\n**Objective:**\\nMaximize the area \\\\( A = L \\\\times W \\\\)\\n\\n---\\n\\n### Step 1: Express area in terms of one variable\\n\\nFrom the perimeter constraint:\\n\\\\[\\nW = 50 - L\\n\\\\]\\n\\nSo, the area:\\n\\\\[\\nA(L) = L \\\\times (50 - L) = 50L - L^2\\n\\\\]\\n\\n---\\n\\n### Step 2: Find the value of \\\\( L \\\\) that maximizes \\\\( A(L) \\\\)\\n\\nThis is a quadratic function:\\n\\\\[\\nA(L) = -L^2 + 50L\\n\\\\]\\n\\nThe parabola opens downward (since coefficient of \\\\( L^2 \\\\) is negative), so the maximum occurs at the vertex:\\n\\n\\\\[\\nL_{max} = -\\\\frac{b}{2a} = -\\\\frac{50}{2 \\\\times (-1)} = \\\\frac{50}{2} = 25\\n\\\\]\\n\\nCorrespondingly:\\n\\\\[\\nW = 50 - L = 50 - 25 = 25\\n\\\\]\\n\\n---\\n\\n### **Conclusion:**\\n\\nThe maximum area is achieved when the rectangle is a **square** with sides of **25 meters**.\\n\\n**Maximum area:**\\n\\\\[\\nA_{max} = 25 \\\\times 25 = 625 \\\\text{ square meters}\\n\\\\]\\n\\n---\\n\\n### **Final answer:**\\n\\n**The farmer should build a square with sides of 25 meters to enclose the maximum area of 625 square meters.**\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765657002, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 376, 'prompt_tokens': 175, 'total_tokens': 551, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Let's analyze the legal case by considering multiple precedents and possible outcomes.\n",
        "\n",
        "1. Precedent 1: A similar case where the plaintiff won.\n",
        "    - Branch A: The court found that the defendant was negligent.\n",
        "        - Sub-branch A1: The plaintiff was awarded damages due to clear evidence of negligence.\n",
        "        - Sub-branch A2: The court ruled in favor of the plaintiff due to the defendant's breach of duty.\n",
        "\n",
        "2. Precedent 2: A similar case where the defendant won.\n",
        "    - Branch B: The court found no negligence on the defendant's part.\n",
        "        - Sub-branch B1: The plaintiff failed to provide sufficient evidence.\n",
        "        - Sub-branch B2: The court ruled that the plaintiff assumed the risk.\n",
        "\n",
        "3. Precedent 3: A case with a mixed outcome.\n",
        "    - Branch C: The court found both parties partially at fault.\n",
        "        - Sub-branch C1: Damages were reduced based on the plaintiff's contributory negligence.\n",
        "        - Sub-branch C2: The court ruled that both parties shared liability, resulting in a split decision.\n",
        "\n",
        "4. Based on the facts of the current case, consider the most likely outcome.\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGSuVsyHW7u9",
        "outputId": "a90e1a69-3133-4416-c7bd-89ea53b1ad75"
      },
      "id": "rGSuVsyHW7u9",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-CmQL7u732UhOdTTkvqaE2f3YP8QAw', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"To analyze the most likely outcome of the current case, let's compare its facts with the provided precedents and their branches.\\n\\n**Step 1: Identify key facts of the current case**  \\n(Assuming typical factors such as evidence strength, negligence, breach of duty, contributory negligence, or shared liability—please specify if you have particular facts.)\\n\\n**Step 2: Match with relevant precedents**\\n\\n- **If the current case closely resembles Precedent 1 (plaintiff wins):**  \\n  - *Branch A (defendant negligent)*  \\n    - *Sub-branch A1:* Clear evidence of negligence → Likely damages awarded.  \\n    - *Sub-branch A2:* Breach of duty established → Likely in favor of plaintiff.\\n\\n- **If it resembles Precedent 2 (defendant wins):**  \\n  - *Branch B (no negligence)*  \\n    - *Sub-branch B1:* Insufficient evidence → Court rules in favor of defendant.  \\n    - *Sub-branch B2:* Plaintiff assumed risk → Defense prevails.\\n\\n- **If it resembles Precedent 3 (mixed outcome):**  \\n  - *Branch C (both parties at fault)*  \\n    - *Sub-branch C1:* Damages reduced due to contributory negligence.  \\n    - *Sub-branch C2:* Shared liability → Split decision.\\n\\n**Step 3: Consider the most probable outcome based on facts**  \\n- If the evidence suggests clear negligence and breach of duty, the most likely outcome aligns with **Precedent 1**, favoring the plaintiff.  \\n- If evidence is weak or the defendant can demonstrate lack of negligence or assumption of risk, the outcome may favor the defendant, similar to **Precedent 2**.  \\n- If both parties are partially at fault, the case may resemble **Precedent 3**, leading to damages being reduced or shared liability.\\n\\n**Conclusion:**  \\nWithout specific facts, the most probable outcome depends on the strength of evidence regarding negligence and liability. If the current case has strong evidence of defendant negligence, the most likely outcome is that the court finds in favor of the plaintiff, awarding damages. Conversely, if evidence is weak or defenses are strong, the defendant may prevail. If fault is shared, expect a reduced damages award or a split liability decision.\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765657029, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 478, 'prompt_tokens': 253, 'total_tokens': 731, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "\n",
        "\"You have 12 identical-looking balls, but one is either heavier or lighter.\n",
        "You have a balance scale and can only use it three times.\n",
        "Think step-by-step, and at each step, explore multiple possible ways to proceed.\n",
        "Evaluate which options are promising and continue expanding those paths.\n",
        "Discard any unhelpful paths. After exploring the reasoning tree, provide the best solution.\"\n",
        "\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"AI Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyV0mns1XCfN",
        "outputId": "4ab18730-2b26-4884-dd23-54bc88431f7c"
      },
      "id": "fyV0mns1XCfN",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Response:\n",
            "{'id': 'chatcmpl-CmQLQac8lNUQ7hJaTKN3etDi5E8uw', 'choices': [{'finish_reason': 'length', 'index': 0, 'logprobs': None, 'message': {'content': \"Let's carefully analyze the problem:\\n\\n**Problem Restatement:**\\n- 12 identical-looking balls.\\n- Exactly one ball is either heavier or lighter than the rest.\\n- You have a balance scale.\\n- You can only use the scale **3 times**.\\n- Goal: Identify the **odd ball** and determine whether it is **heavier or lighter**.\\n\\n---\\n\\n### Step 1: Understand the constraints and possibilities\\n\\n- Total possibilities:\\n  - 12 balls.\\n  - Each could be heavier or lighter.\\n  - Total scenarios: 12 balls × 2 states (heavier or lighter) = **24 possibilities**.\\n\\n- Each weighing can have three outcomes:\\n  - Left side heavier.\\n  - Right side heavier.\\n  - Balance (equal).\\n\\n- With 3 weighings, the maximum number of distinguishable outcomes:\\n  - \\\\(3^3 = 27\\\\).\\n\\n- Since 27 > 24, it is theoretically possible to identify the odd ball and whether it is heavier or lighter within 3 weighings.\\n\\n---\\n\\n### Step 2: Strategy overview\\n\\nThe general approach:\\n- Divide the balls into groups to maximize information gain.\\n- Use the first weighing to narrow down the possibilities.\\n- Use subsequent weighings to pinpoint the odd ball and its nature.\\n\\n---\\n\\n### Step 3: Step-by-step reasoning\\n\\n#### **First Weighing:**\\n\\n**Option A:** Divide into three groups of 4 balls each:\\n- Group 1: Balls 1-4\\n- Group 2: Balls 5-8\\n- Group 3: Balls 9-12\\n\\n**Weigh:** Group 1 vs. Group 2\\n\\n**Possible outcomes:**\\n\\n1. **Balance:** The odd ball is in Group 3 (balls 9-12).  \\n2. **Left side heavier:** The odd ball is in Group 1, and it is heavier, or in Group 2, and it is lighter.  \\n3. **Right side heavier:** The odd ball is in Group 1, and it is lighter, or in Group 2, and it is heavier.\\n\\n---\\n\\n#### **Second Weighing:**\\n\\nBased on the first outcome:\\n\\n---\\n\\n### Scenario 1: First weighing balanced (Group 3 contains the odd ball)\\n\\n- Now, focus on balls 9-12.\\n- We know the odd ball is among these 4.\\n- To determine which one and whether heavier or lighter, weigh 1 vs. 2 (from balls 9-12).\\n\\n**We\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765657048, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 512, 'prompt_tokens': 83, 'total_tokens': 595, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Let's plan a weekend trip by considering multiple options.\n",
        "\n",
        "1. Option 1: Go to the mountains.\n",
        "    - Branch A: If the weather is good in the mountains.\n",
        "        - Sub-branch A1: You can go hiking.\n",
        "        - Sub-branch A2: You can visit a nearby lake.\n",
        "    - Branch B: If the weather is bad in the mountains.\n",
        "        - Sub-branch B1: You will stay in a cabin and relax.\n",
        "        - Sub-branch B2: You can explore local museums.\n",
        "\n",
        "2. Option 2: Go to the beach.\n",
        "    - Branch C: If the weather is sunny at the beach.\n",
        "        - Sub-branch C1: You can swim in the ocean.\n",
        "        - Sub-branch C2: You can sunbathe and play beach volleyball.\n",
        "    - Branch D: If the weather is cloudy or rainy at the beach.\n",
        "        - Sub-branch D1: You will visit indoor attractions like an aquarium.\n",
        "        - Sub-branch D2: You can go shopping in beachside stores.\n",
        "\n",
        "3. Considering all these factors, decide the best option for your weekend trip.\n",
        "\n",
        "What should i do for my trip to Goa in month of august\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUx1V3UcXHGH",
        "outputId": "fc03ddc0-5105-400d-db5f-2eacf31c463d"
      },
      "id": "BUx1V3UcXHGH",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-CmQLhTFkwV3yIO99u9Qy1xBLoeegM', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Planning a trip to Goa in August involves considering the weather conditions, as August is typically the monsoon season with heavy rains and high humidity. Here's a breakdown based on the options you provided:\\n\\n**Option 1: Mountains**  \\n- Not applicable, since Goa is a coastal region and doesn't have mountainous terrain suitable for this option.\\n\\n**Option 2: Beach**  \\n- **Weather in August:** Usually rainy, cloudy, and humid. Sunny days are less frequent, and outdoor activities like swimming or sunbathing may be limited.\\n\\n**Recommended Choice:**  \\n- **Indoor activities and exploring local culture** are better suited during the monsoon season.  \\n- You can visit indoor attractions such as museums, art galleries, or spice plantations.  \\n- Enjoy local cuisine at cozy cafes or restaurants.  \\n- Consider shopping at indoor markets or exploring historical sites that are less affected by rain.\\n\\n**Additional Tips:**  \\n- Check the weather forecast before planning outdoor activities.  \\n- Embrace the monsoon ambiance—lush greenery, fewer crowds, and scenic waterfalls.  \\n- Be prepared with rain gear and waterproof clothing.\\n\\n**Conclusion:**  \\nGiven the typical weather in August, the best option for your Goa trip is to focus on indoor attractions, cultural experiences, and enjoying the natural beauty that the monsoon season enhances. Outdoor beach activities might be limited, but the overall experience can still be memorable with the right planning.\", 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'created': 1765657065, 'model': 'gpt-4.1-nano-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_03e44fcc34', 'usage': {'completion_tokens': 283, 'prompt_tokens': 254, 'total_tokens': 537, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oclHM6TLXLMv"
      },
      "id": "oclHM6TLXLMv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Working_with_prompts.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}